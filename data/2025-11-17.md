<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.RO](#cs.RO) [Total: 22]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Mathematical Framework for AI Singularity: Conditions, Bounds, and Control of Recursive Improvement](https://arxiv.org/abs/2511.10668)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.CV

TL;DR: AI能力增长是无限的还是有限的，取决于资源投入和部署策略。本文提出了一个分析框架，将能力增长与资源扩张和部署政策联系起来。该框架考虑了物理和信息论的限制，例如电力、带宽和内存，这些都为瞬时改进设定了上限。通过内生增长模型，将资本与计算、数据和能源联系起来，区分了超线性（能力无界增长）和亚临界（能力有界增长）两种状态。该模型可以根据可观测的指标（如设施电力、IO带宽、训练吞吐量、基准损失和支出）来判断AI能力的增长是无界的还是有界的。研究结果为AI能力的增长提供了可证伪的测试方法和可部署的安全控制措施，例如功率上限、吞吐量限制和评估门限。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决AI能力是否会无界增长的问题，并提出可测试的条件来判断这种情况的可能性，以及可行的控制措施。

Method: 本文开发了一个分析框架，用于研究递归自我改进，并将能力增长与资源扩张和部署政策联系起来。该框架考虑了物理和信息论的限制，并建立了区分超线性增长和亚临界增长的临界边界。研究还推导了决策规则，以可观测的系列指标来判断AI能力的增长是无界的还是有界的。

Result: 该框架能够根据可观测的指标（如设施电力、IO带宽、训练吞吐量、基准损失和支出）来判断AI能力的增长是无界的还是有界的，并提供了相应的安全控制措施，如功率上限、吞吐量限制和评估门限。

Conclusion: 本文提出的分析框架将AI能力增长的猜测转变为可测试的条件和可部署的控制措施，能够证明或排除AI奇点（无界增长）的可能性。

Abstract: AI systems improve by drawing on more compute, data, energy, and better training methods. This paper asks a precise, testable version of the "runaway growth" question: under what measurable conditions could capability escalate without bound in finite time, and under what conditions can that be ruled out? We develop an analytic framework for recursive self-improvement that links capability growth to resource build-out and deployment policies. Physical and information-theoretic limits from power, bandwidth, and memory define a service envelope that caps instantaneous improvement. An endogenous growth model couples capital to compute, data, and energy and defines a critical boundary separating superlinear from subcritical regimes. We derive decision rules that map observable series (facility power, IO bandwidth, training throughput, benchmark losses, and spending) into yes/no certificates for runaway versus nonsingular behavior. The framework yields falsifiable tests based on how fast improvement accelerates relative to its current level, and it provides safety controls that are directly implementable in practice, such as power caps, throughput throttling, and evaluation gates. Analytical case studies cover capped-power, saturating-data, and investment-amplified settings, illustrating when the envelope binds and when it does not. The approach is simulation-free and grounded in measurements engineers already collect. Limitations include dependence on the chosen capability metric and on regularity diagnostics; future work will address stochastic dynamics, multi-agent competition, and abrupt architectural shifts. Overall, the results replace speculation with testable conditions and deployable controls for certifying or precluding an AI singularity.

</details>


### [2] [Fast Data Attribution for Text-to-Image Models](https://arxiv.org/abs/2511.10721)
*Sheng-Yu Wang,Aaron Hertzmann,Alexei A Efros,Richard Zhang,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本研究提出了一种可扩展高效的数据归因方法，通过将慢速归因方法蒸馏到特征嵌入空间，并结合高效的索引和搜索方法，从而无需运行昂贵的归因算法即可快速检索到有影响力的训练图像。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型的数据归因方法计算成本高昂，不适用于实际应用。

Method: 将慢速的、基于卸载的归因方法蒸馏到特征嵌入空间，结合高效的索引和搜索方法。

Result: 在MSCOCO和LAION数据集上，该方法在几秒钟内即可找到高影响力图像，速度比现有方法快2,500倍至400,000倍，性能相当或更优。

Conclusion: 该方法是在真实世界模型（如Stable Diffusion）上大规模应用数据归因方法的有意义一步，实现了可扩展且高效的数据归因。

Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.

</details>


### [3] [Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow](https://arxiv.org/abs/2511.10766)
*Pooja P Jain,Pietro Mascagni,Giuseppe Massimiani,Nabani Banik,Marta Goglia,Lorenzo Arboit,Britty Baby,Andrea Balla,Ludovica Baldari,Gianfranco Silecchia,Claudio Fiorillo,CompSurg Colorectal Experts Group,Sergio Alfieri,Salvador Morales-Conde,Deborah S Keller,Luigi Boni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 本研究开发并验证了一个名为ColoWorkflow的视频评估工具，用于分析微创结直肠手术的工作流程，以期减少手术差异、优化培训和提高手术表现。


<details>
  <summary>Details</summary>
Motivation: 微创结直肠手术存在程序差异大、学习曲线陡峭和并发症影响结局等问题，视频评估（VBA）可提供数据驱动的见解来解决这些问题，但现有工具难以标准化和实施。

Method: 通过Delphi法就可推广的工作流程描述达成共识，并基于此开发了ColoWorkflow工具。使用该工具对来自五个中心、包含腹腔镜和机器人结直肠手术的多中心视频数据集进行评估，并检验其适用性和评分者间信度。

Result: 研究达成了10个操作无关阶段和34个操作特异性步骤的共识。ColoWorkflow被应用于54个结直肠手术视频，显示了广泛的适用性。评分者间信度中等，阶段的Cohen

Conclusion: ColoWorkflow是首个基于共识、经过验证的、用于全面分析微创结直肠手术工作流程的VBA工具。它提供了一个可重复的视频评估框架，能够实现跨机构的基准比较，支持人工智能驱动的工作流程识别，并可能促进培训标准化、加速能力获得和推动数据驱动的手术质量改进。

Abstract: Minimally invasive colorectal surgery is characterized by procedural variability, a difficult learning curve, and complications that impact quality and outcomes. Video-based assessment (VBA) offers an opportunity to generate data-driven insights to reduce variability, optimize training, and improve surgical performance. However, existing tools for workflow analysis remain difficult to standardize and implement. This study aims to develop and validate a VBA tool for workflow analysis across minimally invasive colorectal procedures. A Delphi process was conducted to achieve consensus on generalizable workflow descriptors. The resulting framework informed the development of a new VBA tool, ColoWorkflow. Independent raters then applied ColoWorkflow to a multicentre video dataset of laparoscopic and robotic colorectal surgery (CRS). Applicability and inter-rater reliability were evaluated. Consensus was achieved for 10 procedure-agnostic phases and 34 procedure-specific steps describing CRS workflows. ColoWorkflow was developed and applied to 54 colorectal operative videos (left and right hemicolectomies, sigmoid and rectosigmoid resections, and total proctocolectomies) from five centres. The tool demonstrated broad applicability, with all but one label utilized. Inter-rater reliability was moderate, with mean Cohen's K of 0.71 for phases and 0.66 for steps. Most discrepancies arose at phase transitions and step boundary definitions. ColoWorkflow is the first consensus-based, validated VBA tool for comprehensive workflow analysis in minimally invasive CRS. It establishes a reproducible framework for video-based performance assessment, enabling benchmarking across institutions and supporting the development of artificial intelligence-driven workflow recognition. Its adoption may standardize training, accelerate competency acquisition, and advance data-informed surgical quality improvement.

</details>


### [4] [Accuracy-Preserving CNN Pruning Method under Limited Data Availability](https://arxiv.org/abs/2511.10861)
*Daisuke Yasui,Toshitaka Matsuki,Hiroshi Sato*

Main category: cs.CV

TL;DR: CNN模型压缩技术的研究，提出了一种新的基于LRP的剪枝方法，以提高剪枝率和模型准确性，特别是在数据有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LRP的剪枝方法虽然有潜力，但在精度下降方面仍存在不足，限制了其实际应用。

Method: 提出了一种新的剪枝方法，利用LRP技术，在无需微调的情况下实现高剪枝率并保持模型准确性，特别是在数据量有限的情况下。

Result: 该方法在剪枝的同时能够更好地保持模型准确性，并且在数据量有限的情况下优于现有方法。

Conclusion: 提出了一种新的基于LRP的剪枝方法，在保持模型准确性的前提下实现了更高的剪枝率，特别适用于数据量有限的场景。

Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.

</details>


### [5] [Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling](https://arxiv.org/abs/2511.10866)
*Seoik Jung,Taekyung Song,Yangro Lee,Sungjun Lee*

Main category: cs.CV

TL;DR: 该研究提出一种短期滑动学习框架，利用LLM自动为1-2秒的短视频片段打标签，以实现对CCTV监控视频的实时暴力检测。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理需要精细时间分辨率的快速暴力事件时存在局限性，需要更有效的实时暴力检测方法。

Method: 将视频分割成1-2秒的短片段，并利用大型语言模型（LLM）进行自动字幕标注，构建细粒度数据集。该方法充分利用每个短片段的所有帧来保持时间连续性，从而精确识别快速的暴力事件。

Result: 在RWF-2000数据集上达到95.25%的准确率，在UCF-Crime数据集（处理长视频）上性能提升至83.25%，证明了其良好的泛化能力和在智能监控系统中的实时应用潜力。

Conclusion: 提出的短期滑动学习框架结合LLM自动标注，能够有效实现CCTV视频的实时暴力检测，并在准确性和泛化能力方面优于传统方法。

Abstract: This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.

</details>


### [6] [DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting](https://arxiv.org/abs/2511.10894)
*Luciano Araujo Dourado Filho,Almir Moreira da Silva Neto,Anthony Miyaguchi,Rodrigo Pereira David,Rodrigo Tripodi Calumby,Lukáš Picek*

Main category: cs.CV

TL;DR: 利用预训练的ViT和概率头进行降雨量预测，在Weather4cast基准测试中CRPS为3.5102，比3D-UNET有效性提高约26%。


<details>
  <summary>Details</summary>
Motivation: 提出一种具有竞争力和计算效率的方法来进行概率性降雨量预测。

Method: 将视频投影仪（V-JEPA Vision Transformer）与轻量级概率头相结合，并连接到预训练的卫星视觉编码器（DINOv3-SAT493M），将编码器令牌映射到4小时累积降雨量的离散经验累积分布函数（eCDF）。投影仪-头部通过连续排序概率得分（CRPS）进行端到端优化。

Result: 所提出的方法在Weather4cast 2025基准测试中取得了3.5102的CRPS，相较于最佳的3D-UNET，有效性提升了约26%。

Conclusion: 该方法是一种有前景的降雨量预测方法，在效率和性能上都优于3D-UNET基线。

Abstract: This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3\text{-}SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Continuous Ranked Probability Score (CRPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102 (CRPS), which represents $\approx$26\% in effectiveness gain against the best 3D-UNET.

</details>


### [7] [PhaseWin Search Framework Enable Efficient Object-Level Interpretation](https://arxiv.org/abs/2511.10914)
*Zihan Gu,Ruoyu Chen,Junchi Zhang,Yue Hu,Hua Zhang,Xiaochun Cao*

Main category: cs.CV

TL;DR: PhaseWin是一种新的相位窗口搜索算法，能够以近乎线性的复杂度和高保真度实现对象级区域归因，解决了现有方法效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于子模子集选择的归因方法虽然保真度高，但效率低下，限制了其在实际应用中的部署。

Method: PhaseWin采用相位窗口搜索算法，通过自适应剪枝、窗口化细粒度选择和动态监督机制，取代了传统的二次成本贪婪选择，实现了从粗到精的搜索，近似贪婪行为并显著减少模型评估次数。

Result: PhaseWin的计算成本仅为贪婪方法的20%，但能达到其95%以上的归因保真度。在对象检测和视觉基础任务上，它与Grounding DINO和Florence-2结合使用时，表现优于其他归因基线。

Conclusion: PhaseWin在可扩展、高保真度的对象级多模态模型归因方面树立了新的技术标杆。

Abstract: Attribution is essential for interpreting object-level foundation models. Recent methods based on submodular subset selection have achieved high faithfulness, but their efficiency limitations hinder practical deployment in real-world scenarios. To address this, we propose PhaseWin, a novel phase-window search algorithm that enables faithful region attribution with near-linear complexity. PhaseWin replaces traditional quadratic-cost greedy selection with a phased coarse-to-fine search, combining adaptive pruning, windowed fine-grained selection, and dynamic supervision mechanisms to closely approximate greedy behavior while dramatically reducing model evaluations. Theoretically, PhaseWin retains near-greedy approximation guarantees under mild monotone submodular assumptions. Empirically, PhaseWin achieves over 95% of greedy attribution faithfulness using only 20% of the computational budget, and consistently outperforms other attribution baselines across object detection and visual grounding tasks with Grounding DINO and Florence-2. PhaseWin establishes a new state of the art in scalable, high-faithfulness attribution for object-level multimodal models.

</details>


### [8] [Out-of-Distribution Detection with Positive and Negative Prompt Supervision Using Large Language Models](https://arxiv.org/abs/2511.10923)
*Zhixia He,Chen Zhao,Minglai Shao,Xintao Wu,Xujiang Zhao,Dong Li,Qin Tian,Linlin Yu*

Main category: cs.CV

TL;DR: 通过利用类别特定的正负面提示来改善视觉语言模型中的分布外检测。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型（VLM）的分布外（OOD）检测方法，特别是使用负面提示的方法，可能由于提示中包含广泛的非分布内（non-ID）特征而导致次优结果，这些特征可能重叠或具有误导性。

Method: 提出了一种正负面提示监督方法，首先使用大型语言模型（LLM）初始化类别特定的正负面提示。然后优化这些提示，使正面提示关注类别内的特征，负面提示突出类别边界周围的特征。最后，采用基于图的架构将优化后的提示表示中的语义感知监督聚合起来，并将其传播到视觉分支，以增强基于能量的OOD检测器。

Result: 该方法在CIFAR-100和ImageNet-1K这两个基准测试以及八个OOD数据集和五种不同的LLM上进行了广泛的实验，证明其性能优于最先进的基线方法。

Conclusion: 所提出的正负面提示监督方法通过鼓励负面提示捕捉类别间特征并将此语义知识转移到视觉模型，有效地增强了OOD检测性能。

Abstract: Out-of-distribution (OOD) detection is committed to delineating the classification boundaries between in-distribution (ID) and OOD images. Recent advances in vision-language models (VLMs) have demonstrated remarkable OOD detection performance by integrating both visual and textual modalities. In this context, negative prompts are introduced to emphasize the dissimilarity between image features and prompt content. However, these prompts often include a broad range of non-ID features, which may result in suboptimal outcomes due to the capture of overlapping or misleading information. To address this issue, we propose Positive and Negative Prompt Supervision, which encourages negative prompts to capture inter-class features and transfers this semantic knowledge to the visual modality to enhance OOD detection performance. Our method begins with class-specific positive and negative prompts initialized by large language models (LLMs). These prompts are subsequently optimized, with positive prompts focusing on features within each class, while negative prompts highlight features around category boundaries. Additionally, a graph-based architecture is employed to aggregate semantic-aware supervision from the optimized prompt representations and propagate it to the visual branch, thereby enhancing the performance of the energy-based OOD detector. Extensive experiments on two benchmarks, CIFAR-100 and ImageNet-1K, across eight OOD datasets and five different LLMs, demonstrate that our method outperforms state-of-the-art baselines.

</details>


### [9] [Facial Expression Recognition with YOLOv11 and YOLOv12: A Comparative Study](https://arxiv.org/abs/2511.10940)
*Umma Aymon,Nur Shazwani Kamarudin,Ahmad Fakhri Ab. Nasir*

Main category: cs.CV

TL;DR: YOLOv11n and YOLOv12n, lightweight models, are evaluated for facial expression recognition (FER) using object detection. YOLOv12n performs better overall on KDEF, while YOLOv11n shows higher precision on FER2013. Both models struggle with visually similar expressions, highlighting a trade-off between sensitivity and precision, making them suitable for real-time applications.


<details>
  <summary>Details</summary>
Motivation: To investigate the performance of lightweight YOLOv11n and YOLOv12n models in a unified detection and classification framework for facial expression recognition (FER) in unconstrained, real-world environments.

Method: Two benchmark FER datasets (FER2013 and KDEF) were converted into object detection format. The performance of YOLOv11n and YOLOv12n was evaluated using mAP 0.5, precision, and recall, with analysis of confusion matrices.

Result: YOLOv12n achieved the highest mAP 0.5 (95.6) on KDEF. On FER2013, YOLOv12n had a higher mAP (63.8) than YOLOv11n, indicating better sensitivity to varied expressions. YOLOv11n showed higher precision (65.2) on FER2013, suggesting fewer false positives. Both models exhibited more confusion between visually similar expressions on FER2013 compared to KDEF.

Conclusion: Lightweight YOLO models offer an effective balance between performance and efficiency for FER. YOLOv12n is more sensitive to varied expressions, while YOLOv11n provides better precision in noisy conditions. The models demonstrate adaptability to both controlled and real-world scenarios, making them suitable for real-time, resource-constrained emotion-aware AI applications.

Abstract: Facial Expression Recognition remains a challenging task, especially in unconstrained, real-world environments. This study investigates the performance of two lightweight models, YOLOv11n and YOLOv12n, which are the nano variants of the latest official YOLO series, within a unified detection and classification framework for FER. Two benchmark classification datasets, FER2013 and KDEF, are converted into object detection format and model performance is evaluated using mAP 0.5, precision, recall, and confusion matrices. Results show that YOLOv12n achieves the highest overall performance on the clean KDEF dataset with a mAP 0.5 of 95.6, and also outperforms YOLOv11n on the FER2013 dataset in terms of mAP 63.8, reflecting stronger sensitivity to varied expressions. In contrast, YOLOv11n demonstrates higher precision 65.2 on FER2013, indicating fewer false positives and better reliability in noisy, real-world conditions. On FER2013, both models show more confusion between visually similar expressions, while clearer class separation is observed on the cleaner KDEF dataset. These findings underscore the trade-off between sensitivity and precision, illustrating how lightweight YOLO models can effectively balance performance and efficiency. The results demonstrate adaptability across both controlled and real-world conditions, establishing these models as strong candidates for real-time, resource-constrained emotion-aware AI applications.

</details>


### [10] [Heterogeneous Complementary Distillation](https://arxiv.org/abs/2511.10942)
*Liuchi Xu,Hao Zheng,Lu Wang,Lisheng Xu,Jun Cheng*

Main category: cs.CV

TL;DR: HCD是一种简单有效的异构知识蒸馏框架，通过对齐共享的logits来整合教师和学生互补的特征。它使用卷积投影仪和自适应池化处理学生特征，并将其与教师的特征拼接，然后通过CFM模块映射以产生共享logits。SDD将共享logits分解为n个子logits，并与教师logits融合以进行分类。OL损失确保子logits的多样性。HCD在CIFAR-100、CUB200和ImageNet-1K数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法在处理异构架构（如ViT到ResNet18）时面临挑战，因为它们在空间特征表示上存在差异。现有的异构蒸馏方法计算成本高、设计复杂或过度依赖logit对齐，限制了它们利用互补特征的能力。

Method: 提出了一种名为HCD的框架，该框架整合了教师和学生的互补特征，通过对齐共享logits来映射表示。HCD将学生模型的中间特征通过卷积投影仪和自适应池化进行处理，然后与教师模型倒数第二层的特征拼接，并通过一个包含全连接层的CFM模块映射以产生共享logits。此外，还引入了SDD，将共享logits划分为n个子logits，并与教师logits融合以进行分类。通过引入正交损失（OL）来确保子logits的多样性并减少冗余知识传递。

Result: HCD在CIFAR-100、细粒度（如CUB200）和ImageNet-1K数据集上的广泛实验表明，其性能优于最先进的知识蒸馏方法。

Conclusion: HCD通过保留学生特有的优势并利用教师知识，增强了学生的鲁棒性和泛化能力，是异构知识蒸馏的有效解决方案。

Abstract: Knowledge distillation (KD)transfers the dark knowledge from a complex teacher to a compact student. However, heterogeneous architecture distillation, such as Vision Transformer (ViT) to ResNet18, faces challenges due to differences in spatial feature representations.Traditional KD methods are mostly designed for homogeneous architectures and hence struggle to effectively address the disparity. Although heterogeneous KD approaches have been developed recently to solve these issues, they often incur high computational costs and complex designs, or overly rely on logit alignment, which limits their ability to leverage the complementary features. To overcome these limitations, we propose Heterogeneous Complementary Distillation (HCD),a simple yet effective framework that integrates complementary teacher and student features to align representations in shared logits.These logits are decomposed and constrained to facilitate diverse knowledge transfer to the student. Specifically, HCD processes the student's intermediate features through convolutional projector and adaptive pooling, concatenates them with teacher's feature from the penultimate layer and then maps them via the Complementary Feature Mapper (CFM) module, comprising fully connected layer,to produce shared logits.We further introduce Sub-logit Decoupled Distillation (SDD) that partitions the shared logits into n sub-logits, which are fused with teacher's logits to rectify classification.To ensure sub-logit diversity and reduce redundant knowledge transfer, we propose an Orthogonality Loss (OL).By preserving student-specific strengths and leveraging teacher knowledge,HCD enhances robustness and generalization in students.Extensive experiments on the CIFAR-100, Fine-grained (e.g., CUB200)and ImageNet-1K datasets demonstrate that HCD outperforms state-of-the-art KD methods,establishing it as an effective solution for heterogeneous KD.

</details>


### [11] [Divide, Conquer and Unite: Hierarchical Style-Recalibrated Prototype Alignment for Federated Medical Image Segmentation](https://arxiv.org/abs/2511.10945)
*Xingyue Zhao,Wenke Huang,Xingguang Wang,Haoyu Zhao,Linghao Zhuang,Anwen Jiang,Guancheng Wan,Mang Ye*

Main category: cs.CV

TL;DR: 联邦学习在医学影像分析中面临特征异质性挑战，现有方法仅关注末层特征或累积层间风格偏差。本文提出FedBCS，通过域不变上下文原型对齐来解决这些问题，引入频域自适应风格重校准来构建原型，并设计了上下文感知的双层原型对齐方法，以实现更细粒度的表示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理医学影像时，由于不同机构的扫描设备或协议不同，导致特征异质性问题，现有方法主要关注末层特征或存在层间风格偏差累积的问题。

Method: 提出FedBCS，通过域不变上下文原型对齐来解决特征异质性问题。具体包括：1. 引入频域自适应风格重校准到原型构建中，以解耦内容-风格表示并学习最优风格参数，实现更鲁棒的域不变原型。2. 设计上下文感知的双层原型对齐，从编码器和解码器的不同层提取域不变原型，并融合上下文信息进行细粒度表示对齐。

Result: 在两个公共数据集上的大量实验证明了该方法具有卓越的性能。

Conclusion: FedBCS通过域不变上下文原型对齐有效解决了联邦学习中的特征异质性挑战，提升了医学影像分析的性能。

Abstract: Federated learning enables multiple medical institutions to train a global model without sharing data, yet feature heterogeneity from diverse scanners or protocols remains a major challenge. Many existing works attempt to address this issue by leveraging model representations (e.g., mean feature vectors) to correct local training; however, they often face two key limitations: 1) Incomplete Contextual Representation Learning: Current approaches primarily focus on final-layer features, overlooking critical multi-level cues and thus diluting essential context for accurate segmentation. 2) Layerwise Style Bias Accumulation: Although utilizing representations can partially align global features, these methods neglect domain-specific biases within intermediate layers, allowing style discrepancies to build up and reduce model robustness. To address these challenges, we propose FedBCS to bridge feature representation gaps via domain-invariant contextual prototypes alignment. Specifically, we introduce a frequency-domain adaptive style recalibration into prototype construction that not only decouples content-style representations but also learns optimal style parameters, enabling more robust domain-invariant prototypes. Furthermore, we design a context-aware dual-level prototype alignment method that extracts domain-invariant prototypes from different layers of both encoder and decoder and fuses them with contextual information for finer-grained representation alignment. Extensive experiments on two public datasets demonstrate that our method exhibits remarkable performance.

</details>


### [12] [DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2511.10948)
*Ren Zhang,Huilai Li,Chao qi,Guoliang Xu,Tianyu Zhou,Wei wei,Jianqin Yin*

Main category: cs.CV

TL;DR: 该研究提出DEFT-LLM，一种用于微表情识别（MER）的多模态大语言模型（MLLM），通过多专家解耦实现运动语义对齐，解决了现有模型中静态外观与动态运动线索纠缠以及文本标签与面部肌肉运动语义鸿沟的问题。


<details>
  <summary>Details</summary>
Motivation: 解决微表情识别中静态外观与动态运动线索的纠缠，以及现有数据集文本标签与实际面部肌肉运动的语义鸿沟问题。

Method: 提出DEFT-LLM，包含Uni-MER数据集（利用光流和动作单元标签保证时空一致性和运动对应性）和三专家解耦架构（将面部动态分解为结构、动态纹理和运动语义），并将Uni-MER的知识注入DEFT-LLM。

Result: 在多个MER基准测试中达到了最先进的性能，并在局部面部运动的可解释建模方面显示出优势。

Conclusion: DEFT-LLM通过多专家解耦和Uni-MER数据集实现了运动语义对齐，有效弥合了文本监督与物理运动之间的差距，提高了微表情识别的精度和可解释性。

Abstract: Micro expression recognition (MER) is crucial for inferring genuine emotion. Applying a multimodal large language model (MLLM) to this task enables spatio-temporal analysis of facial motion and provides interpretable descriptions. However, there are still two core challenges: (1) The entanglement of static appearance and dynamic motion cues prevents the model from focusing on subtle motion; (2) Textual labels in existing MER datasets do not fully correspond to underlying facial muscle movements, creating a semantic gap between text supervision and physical motion. To address these issues, we propose DEFT-LLM, which achieves motion semantic alignment by multi-expert disentanglement. We first introduce Uni-MER, a motion-driven instruction dataset designed to align text with local facial motion. Its construction leverages dual constraints from optical flow and Action Unit (AU) labels to ensure spatio-temporal consistency and reasonable correspondence to the movements. We then design an architecture with three experts to decouple facial dynamics into independent and interpretable representations (structure, dynamic textures, and motion-semantics). By integrating the instruction-aligned knowledge from Uni-MER into DEFT-LLM, our method injects effective physical priors for micro expressions while also leveraging the cross modal reasoning ability of large language models, thus enabling precise capture of subtle emotional cues. Experiments on multiple challenging MER benchmarks demonstrate state-of-the-art performance, as well as a particular advantage in interpretable modeling of local facial motion.

</details>


### [13] [Language-Guided Graph Representation Learning for Video Summarization](https://arxiv.org/abs/2511.10953)
*Wenrui Li,Wei Han,Hengyu Man,Wangmeng Zuo,Xiaopeng Fan,Yonghong Tian*

Main category: cs.CV

TL;DR: The paper proposes LGRLN, a novel network for video summarization that addresses challenges in capturing global dependencies and multimodal customization by using a language-guided graph representation. It outperforms existing methods and significantly reduces inference time and model parameters.


<details>
  <summary>Details</summary>
Motivation: Existing video summarization methods struggle with capturing global dependencies and multimodal user customization, and the assumption that temporal proximity equals semantic proximity is flawed. 

Method: LGRLN constructs a video graph from frames, uses a dual-threshold graph convolution for relational reasoning within the graph, and employs a language-guided cross-modal embedding module for summary generation, modeling the output with a mixture of Bernoulli distribution solved by the EM algorithm.

Result: LGRLN outperforms existing methods on multiple benchmarks, with significant reductions in inference time (87.8%) and model parameters (91.7%).

Conclusion: LGRLN effectively addresses the limitations of existing video summarization techniques by leveraging graph representation and language guidance, achieving superior performance and efficiency.

Abstract: With the rapid growth of video content on social media, video summarization has become a crucial task in multimedia processing. However, existing methods face challenges in capturing global dependencies in video content and accommodating multimodal user customization. Moreover, temporal proximity between video frames does not always correspond to semantic proximity. To tackle these challenges, we propose a novel Language-guided Graph Representation Learning Network (LGRLN) for video summarization. Specifically, we introduce a video graph generator that converts video frames into a structured graph to preserve temporal order and contextual dependencies. By constructing forward, backward and undirected graphs, the video graph generator effectively preserves the sequentiality and contextual relationships of video content. We designed an intra-graph relational reasoning module with a dual-threshold graph convolution mechanism, which distinguishes semantically relevant frames from irrelevant ones between nodes. Additionally, our proposed language-guided cross-modal embedding module generates video summaries with specific textual descriptions. We model the summary generation output as a mixture of Bernoulli distribution and solve it with the EM algorithm. Experimental results show that our method outperforms existing approaches across multiple benchmarks. Moreover, we proposed LGRLN reduces inference time and model parameters by 87.8% and 91.7%, respectively. Our codes and pre-trained models are available at https://github.com/liwrui/LGRLN.

</details>


### [14] [Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition](https://arxiv.org/abs/2511.10958)
*Gunho Jung,Heejo Kong,Seong-Whan Lee*

Main category: cs.CV

TL;DR: This paper proposes TG-DFER, a text-guided framework to improve dynamic facial expression recognition (DFER) by addressing the many-to-one labeling problem using Multiple Instance Learning (MIL). It incorporates a vision-language pre-trained (VLP) model for semantic guidance via textual descriptions, uses visual prompts for aligning text and visual features, and employs a multi-grained temporal network to model both short-term dynamics and long-term flow. Experiments show TG-DFER enhances generalization, interpretability, and temporal sensitivity in weakly supervised settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in Dynamic Facial Expression Recognition (DFER), specifically the many-to-one labeling problem and the limitations of existing Multiple Instance Learning (MIL) approaches which struggle with visual diversity and temporal complexity.

Method: The proposed method, TG-DFER, is a text-guided weakly supervised framework that enhances MIL-based DFER. It integrates a vision-language pre-trained (VLP) model for semantic guidance using textual descriptions and visual prompts to align textual emotion labels with visual features. A multi-grained temporal network is used to capture both short-term facial dynamics and long-range emotional flow.

Result: Extensive experimental results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity compared to existing methods under weak supervision.

Conclusion: TG-DFER effectively enhances MIL-based DFER by incorporating semantic guidance from text and coherent temporal modeling, leading to better performance in recognizing dynamic facial expressions with weak supervision.

Abstract: Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.

</details>


### [15] [ERMoE: Eigen-Reparameterized Mixture-of-Experts for Stable Routing and Interpretable Specialization](https://arxiv.org/abs/2511.10971)
*Anzhe Cheng,Shukai Duan,Shixuan Li,Chenzhong Yin,Mingxi Cheng,Heng Ping,Tamoghna Chattopadhyay,Sophia I Thomopoulos,Shahin Nazarian,Paul Thompson,Paul Bogdan*

Main category: cs.CV

TL;DR: ERMoE通过将专家重新参数化为学习到的正交特征基，并用“特征基得分”（输入特征与专家基的余弦相似度）取代门控logits，来解决Mixture-of-Experts（MoE）模型中的路由不稳定和专家利用率低的问题。这种方法无需额外的负载均衡损失，就能实现高精度，并在图像分类、图文检索和3D MRI脑龄预测等任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts（MoE）架构虽然能扩展模型容量，但存在路由日志与专家内部结构不匹配导致的路由不稳定和专家利用率低的问题，以及负载不平衡导致的瓶颈问题。现有的解决方案（如辅助负载均衡损失）可能会削弱专家专业化并损害下游性能。

Method: ERMoE将MoE的每个专家嵌入到一个学习到的正交特征基中，并用“特征基得分”（输入特征与专家基的余弦相似度）来代替传统的门控日志。这种基于内容的路由方式将token分配直接与专家的表示空间联系起来，稳定了专家利用率，促进了可解释的专业化，同时保持了稀疏性。

Result: ERMoE在ImageNet分类和图文检索（COCO、Flickr30K）基准测试中达到了最先进的准确率，并且专家负载分布更均衡。ERMoE-ba模型在3D MRI脑龄预测任务中将准确率提高了7%以上，并实现了具有解剖学可解释性的专家专业化。

Conclusion: ERMoE提出了一种新的稀疏专家模型架构原则，直接解决了路由不稳定的问题，并通过可扩展、可解释的专业化实现了性能提升。

Abstract: Mixture-of-Experts (MoE) architectures expand model capacity by sparsely activating experts but face two core challenges: misalignment between router logits and each expert's internal structure leads to unstable routing and expert underutilization, and load imbalances create straggler bottlenecks. Standard solutions, such as auxiliary load-balancing losses, can reduce load disparities but often weaken expert specialization and hurt downstream performance. To address these issues, we propose ERMoE, a sparse MoE transformer that reparameterizes each expert in a learned orthonormal eigenbasis and replaces learned gating logits with an "Eigenbasis Score", defined as the cosine similarity between input features and an expert's basis. This content-aware routing ties token assignments directly to experts' representation spaces, stabilizing utilization and promoting interpretable specialization without sacrificing sparsity. Crucially, ERMoE removes the need for explicit balancing losses and avoids the interfering gradients they introduce. We show that ERMoE achieves state-of-the-art accuracy on ImageNet classification and cross-modal image-text retrieval benchmarks (e.g., COCO, Flickr30K), while naturally producing flatter expert load distributions. Moreover, a 3D MRI variant (ERMoE-ba) improves brain age prediction accuracy by more than 7\% and yields anatomically interpretable expert specializations. ERMoE thus introduces a new architectural principle for sparse expert models that directly addresses routing instabilities and enables improved performance with scalable, interpretable specialization.

</details>


### [16] [Preserving Cross-Modal Consistency for CLIP-based Class-Incremental Learning](https://arxiv.org/abs/2511.10974)
*Haoran Chen,Houze Xu,Micah Goldblum,Daoguo Dong,Zuxuan Wu*

Main category: cs.CV

TL;DR: DMC and DMC-OT are proposed for class-incremental learning with CLIP, addressing issues like classifier bias and distributional drift to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Extending vision-language models like CLIP to continual learning settings is challenging due to classifier bias and distributional drift when adapting soft prompts. 

Method: DMC uses a two-stage framework to decouple vision encoder adaptation and textual soft prompt optimization, freezing one modality while adapting the other. DMC-OT enhances this by incorporating optimal-transport guided calibration for memory statistics and a task-specific prompting design.

Result: Both DMC and DMC-OT achieve state-of-the-art performance on various benchmark datasets (CIFAR-100, Imagenet-R, CUB-200, UCF-101). DMC-OT specifically improves accuracy by an average of 1.80% over DMC.

Conclusion: The proposed DMC and DMC-OT frameworks effectively address the challenges in CLIP-based class-incremental learning, leading to significant performance improvements and state-of-the-art results.

Abstract: Class-incremental learning (CIL) enables models to continuously learn new categories from sequential tasks without forgetting previously acquired knowledge. While recent advances in vision-language models such as CLIP have demonstrated strong generalization across domains, extending them to continual settings remains challenging. In particular, learning task-specific soft prompts for newly introduced classes often leads to severe classifier bias, as the text prototypes overfit to recent categories when prior data are unavailable. In this paper, we propose DMC, a simple yet effective two-stage framework for CLIP-based CIL that decouples the adaptation of the vision encoder and the optimization of textual soft prompts. Each stage is trained with the other frozen, allowing one modality to act as a stable semantic anchor for the other to preserve cross-modal alignment. Furthermore, current CLIP-based CIL approaches typically store class-wise Gaussian statistics for generative replay, yet they overlook the distributional drift that arises when the vision encoder is updated over time. To address this issue, we introduce DMC-OT, an enhanced version of DMC that incorporates an optimal-transport guided calibration strategy to align memory statistics across evolving encoders, along with a task-specific prompting design that enhances inter-task separability. Extensive experiments on CIFAR-100, Imagenet-R, CUB-200, and UCF-101 demonstrate that both DMC and DMC-OT achieve state-of-the-art performance, with DMC-OT further improving accuracy by an average of 1.80%.

</details>


### [17] [PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs](https://arxiv.org/abs/2511.10979)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Hang Wu,Yiwei Wang*

Main category: cs.CV

TL;DR: Video LLMs have temporal inconsistency issues due to multimodal RoPE, causing attention instability. PAS, a training-free mechanism using phase offsets and aggregation, smooths the temporal kernel and reduces phase sensitivity, improving robustness with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the temporal inconsistency in Video LLMs, where small shifts in frame timing lead to unstable attention and suppression of relevant frames, often caused by the extension of Rotary Position Embeddings (RoPE) to video (multimodal RoPE).

Method: The paper proposes Phase Aggregated Smoothing (PAS), a simple, training-free mechanism. PAS introduces small, opposed phase offsets across different attention heads and then aggregates their outputs. This process preserves the spectral magnitude for each head while smoothing the temporal kernel and reducing phase sensitivity, without altering the fundamental structure of the positional encoding.

Result: Experiments on various video understanding benchmarks, under similar token budgets, demonstrate consistent improvements achieved by PAS. These improvements are accompanied by negligible computational overhead, indicating its efficiency.

Conclusion: PAS is presented as a plug-and-play solution that can significantly enhance the temporal encoding capabilities of Video LLMs, providing robustness against temporal shifts without requiring additional training or substantial computational resources.

Abstract: Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.

</details>


### [18] [Binary Verification for Zero-Shot Vision](https://arxiv.org/abs/2511.10983)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 该研究提出了一个无需训练、二元的零样本视觉验证工作流，利用现成的视觉语言模型（VLM）。它包括两个步骤：首先，将开放式问题量化为多项选择题（MCQ），提供一组简短、明确的候选答案；其次，将MCQ二值化，对每个候选答案提出一个真/假问题，并确定性地进行判断：如果只有一个答案为真，则选中该答案；否则，在剩余的合理候选答案中重新进行MCQ。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视觉能力受限于开放式提问的模糊性和VLM的局限性，需要改进验证方法。

Method: 提出一个包含“量化”和“二值化”两步的工作流：量化将开放式问题转化为MCQ，二值化将MCQ转化为一系列真/假问题，以提高准确性。

Result: 该工作流在涉及指代表达式识别（REC）、空间推理（Spatial-Map, Spatial-Grid, Spatial-Maze）和BLINK-Jigsaw等多个任务上均取得了显著的性能提升，表明了其通用性。与直接回答开放式问题相比，量化为MCQ带来了显著的收益，而真/假二值化进一步提供了稳定的性能提升。

Conclusion: 该工作流易于实现且具有通用性，通过推理时的设计而非特定任务的训练，能够有效提升现有VLM的零样本视觉能力。

Abstract: We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.

</details>


### [19] [Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids](https://arxiv.org/abs/2511.11077)
*Ke Ma,Yizhou Fang,Jean-Baptiste Weibel,Shuai Tan,Xinggang Wang,Yang Xiao,Yi Fang,Tian Xia*

Main category: cs.CV

TL;DR: 该研究提出了Phys-Liquid数据集，包含97,200张模拟图像和3D网格，用于解决透明可变形液体几何和体积估算难题，特别是在机器人操作中。数据集涵盖多种场景、光照、颜色和容器旋转。研究还提出了一个四阶段的重建和估算流程（液体分割、多视图掩模生成、3D网格重建、真实世界尺度），并在实验中证明了其在液体几何和体积重建方面的准确性和一致性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前数据集缺乏在动态场景下模拟的、包含物理信息的、逼真的液体行为数据，而机器人进行精确液体操作（如分配、吸取和混合）时，不可避免地会引起容器的变形，这使得准确评估液体状态变得复杂。

Method: 提出一个包含97,200张模拟图像和相应3D网格的、包含物理信息的Phys-Liquid数据集，涵盖多种实验室场景、光照条件、液体颜色和容器旋转，以捕捉液体动力学。同时，提出一个包含液体分割、多视图掩模生成、3D网格重建和真实世界尺度四个阶段的重建和估算流程。

Result: 实验结果表明，相比现有基准，该研究提出的方法在重建液体几何和体积方面提高了准确性和一致性。

Conclusion: Phys-Liquid数据集和相关的验证方法能够促进未来透明液体感知任务的进展。

Abstract: Estimating the geometric and volumetric properties of transparent deformable liquids is challenging due to optical complexities and dynamic surface deformations induced by container movements. Autonomous robots performing precise liquid manipulation tasks, such as dispensing, aspiration, and mixing, must handle containers in ways that inevitably induce these deformations, complicating accurate liquid state assessment. Current datasets lack comprehensive physics-informed simulation data representing realistic liquid behaviors under diverse dynamic scenarios. To bridge this gap, we introduce Phys-Liquid, a physics-informed dataset comprising 97,200 simulation images and corresponding 3D meshes, capturing liquid dynamics across multiple laboratory scenes, lighting conditions, liquid colors, and container rotations. To validate the realism and effectiveness of Phys-Liquid, we propose a four-stage reconstruction and estimation pipeline involving liquid segmentation, multi-view mask generation, 3D mesh reconstruction, and real-world scaling. Experimental results demonstrate improved accuracy and consistency in reconstructing liquid geometry and volume, outperforming existing benchmarks. The dataset and associated validation methods facilitate future advancements in transparent liquid perception tasks. The dataset and code are available at https://dualtransparency.github.io/Phys-Liquid/.

</details>


### [20] [CLUE: Controllable Latent space of Unprompted Embeddings for Diversity Management in Text-to-Image Synthesis](https://arxiv.org/abs/2511.10993)
*Keunwoo Park,Jihye Chae,Joong Ho Ahn,Jihoon Kweon*

Main category: cs.CV

TL;DR: CLUE是一个文本到图像生成模型框架，它通过固定的提示格式在数据有限的领域实现多样化且稳定的图像生成，无需额外数据。它基于Stable Diffusion，并引入了风格编码器和新的注意力层来处理图像和提示，生成风格嵌入。通过KL散度，潜在空间实现了独立于提示的连续图像特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型在生成多样化图像和保持稳定性方面面临挑战，尤其是在数据量有限的专业领域（如医学）。

Method: CLUE框架基于Stable Diffusion架构，通过一个风格编码器处理图像和提示以生成风格嵌入，并将这些嵌入输入到U-Net架构的一个新的第二注意力层。通过KL散度，使得潜在空间能够在高斯区域内连续表示图像特征，且不依赖于提示。

Result: 在																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																														otitis media数据集上，CLUE将FID降低到9.30（相比之下为46.81），并将召回率提高到70.29%（相比之下为49.60%）。在1000%的尺度下，仅使用合成数据训练的分类器达到了83.21%的F1分数（相比之下为73.83%）。将合成数据与等量真实数据结合使用，达到了94.76%的F1分数，优于仅使用真实数据的情况。在外部数据集上，仅使用合成数据训练在1000%的尺度下达到了76.77%的F1分数（相比之下为60.61%）。结合方法达到了85.78%的F1分数，优于仅使用内部数据集的情况。

Conclusion: 这些结果表明，CLUE能够在数据集有限的情况下实现多样化且稳定的图像生成，并且可以作为领域特定应用的有效数据增强方法。

Abstract: Text-to-image synthesis models require the ability to generate diverse images while maintaining stability. To overcome this challenge, a number of methods have been proposed, including the collection of prompt-image datasets and the integration of additional data modalities during training. Although these methods have shown promising results in general domains, they face limitations when applied to specialized fields such as medicine, where only limited types and insufficient amounts of data are available. We present CLUE (Controllable Latent space of Unprompted Embeddings), a generative model framework that achieves diverse generation while maintaining stability through fixed-format prompts without requiring any additional data. Based on the Stable Diffusion architecture, CLUE employs a Style Encoder that processes images and prompts to generate style embeddings, which are subsequently fed into a new second attention layer of the U-Net architecture. Through Kullback-Leibler divergence, the latent space achieves continuous representation of image features within Gaussian regions, independent of prompts. Performance was assessed on otitis media dataset. CLUE reduced FID to 9.30 (vs. 46.81) and improved recall to 70.29% (vs. 49.60%). A classifier trained on synthetic-only data at 1000% scale achieved an F1 score of 83.21% (vs. 73.83%). Combining synthetic data with equal amounts of real data achieved an F1 score of 94.76%, higher than when using only real data. On an external dataset, synthetic-only training achieved an F1 score of 76.77% (vs. 60.61%) at 1000% scale. The combined approach achieved an F1 score of 85.78%, higher than when using only the internal dataset. These results demonstrate that CLUE enables diverse yet stable image generation from limited datasets and serves as an effective data augmentation method for domain-specific applications.

</details>


### [21] [PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities](https://arxiv.org/abs/2511.10997)
*Jiajun Chen,Sai Cheng,Yutao Yuan,Yirui Zhang,Haitao Yuan,Peng Peng,Yi Zhong*

Main category: cs.CV

TL;DR: PROMISE框架通过结合提示学习和分层对比学习，解决了多模态模型在处理缺失模态时的性能下降问题，并在基准数据集上取得了优于现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理缺失模态时，由于表示学习不一致和生成方法过于简单，未能充分保持跨模态一致性，导致性能不佳。

Method: 提出了一种名为PROMISE的新型多模态框架，它将多模态提示学习整合到分层对比学习框架中，并设计了一个提示-注意力机制，以在特定模态缺失时动态生成鲁棒且一致的表示。

Result: 实验证明，PROMISE框架在处理缺失模态方面优于现有的最先进的多模态方法，并在基准数据集上进行了验证，并通过消融研究得到了证实。

Conclusion: PROMISE框架能够有效弥合完整和不完整数据之间的表示差距，在缺失模态的条件下实现鲁棒的跨模态表示学习。

Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.

</details>


### [22] [EmoVid: A Multimodal Emotion Video Dataset for Emotion-Centric Video Understanding and Generation](https://arxiv.org/abs/2511.11002)
*Zongyang Qiu,Bingyuan Wang,Xingbei Chen,Yingqing He,Zeyu Wang*

Main category: cs.CV

TL;DR: EmoVid是一个包含卡通动画、电影片段和动画贴纸的多模态、带情绪标注的视频数据集，用于创意媒体。它包含情绪标签、视觉属性和文本说明，并被用于训练情绪条件视频生成模型，在文本到视频和图像到视频任务上取得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成系统主要关注低级视觉指标，忽略了情感维度，并且缺乏连接情绪理解和生成任务的资源，尤其是在风格化和非现实的背景下。EmuVid数据集旨在弥补这一空白。

Method: 开发了一个情绪条件视频生成技术，通过对Wan2.1模型进行微调，并利用EmoVid数据集进行训练。数据集包含情绪标注、视觉属性（亮度、色彩度、色调）和文本说明。

Result: 所提出的情绪条件视频生成技术在文本到视频和图像到视频任务上，在量化指标和生成视频的视觉质量上都显示出显著的改进。

Conclusion: EmoVid数据集为情感视频计算树立了新的基准，提供了对艺术化视频中视觉情感分析的深刻见解，并为增强视频生成中的情感表达提供了实用的方法。

Abstract: Emotion plays a pivotal role in video-based expression, but existing video generation systems predominantly focus on low-level visual metrics while neglecting affective dimensions. Although emotion analysis has made progress in the visual domain, the video community lacks dedicated resources to bridge emotion understanding with generative tasks, particularly for stylized and non-realistic contexts. To address this gap, we introduce EmoVid, the first multimodal, emotion-annotated video dataset specifically designed for creative media, which includes cartoon animations, movie clips, and animated stickers. Each video is annotated with emotion labels, visual attributes (brightness, colorfulness, hue), and text captions. Through systematic analysis, we uncover spatial and temporal patterns linking visual features to emotional perceptions across diverse video forms. Building on these insights, we develop an emotion-conditioned video generation technique by fine-tuning the Wan2.1 model. The results show a significant improvement in both quantitative metrics and the visual quality of generated videos for text-to-video and image-to-video tasks. EmoVid establishes a new benchmark for affective video computing. Our work not only offers valuable insights into visual emotion analysis in artistically styled videos, but also provides practical methods for enhancing emotional expression in video generation.

</details>


### [23] [VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models](https://arxiv.org/abs/2511.11007)
*Xinlei Yu,Chengming Xu,Guibin Zhang,Zhangquan Chen,Yudong Zhang,Yongbo He,Peng-Tao Jiang,Jiangning Zhang,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: VisMem通过引入短期和长期潜在视觉记忆来解决视觉语言模型中的视觉处理瓶颈，提高了11.8%的性能。


<details>
  <summary>Details</summary>
Motivation: 克服视觉语言模型在复杂视觉任务中存在的“视觉处理瓶颈”，解决其在长期生成中视觉证据丢失和情境化视觉经验不足的问题。

Method: 提出了一种受人类记忆理论启发的认知对齐框架VisMem，该框架包含一个用于精细感知保留的短期模块和一个用于抽象语义巩固的长期模块，并在推理过程中动态调用这些模块。

Result: VisMem在各种视觉理解、推理和生成基准测试中，相对于原始模型平均性能提升了11.8%，并超越了所有对比方法。

Conclusion: VisMem通过引入潜在空间记忆增强，为视觉语言模型提供了一种新的范式，能够同时保持感知保真度和语义一致性。

Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

</details>


### [24] [SP-Guard: Selective Prompt-adaptive Guidance for Safe Text-to-Image Generation](https://arxiv.org/abs/2511.11014)
*Sumin Yu,Taesup Moon*

Main category: cs.CV

TL;DR: SP-Guard通过估算提示的有害性并应用选择性引导掩码来仅引导不安全区域，从而在不牺牲生成质量的情况下提高扩散模型T2I的安全性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型容易生成有害内容，需要更安全的方法。现有方法缺乏基于提示的自适应性和仅针对不安全区域的选择性。

Method: SP-Guard估计提示的有害性，并使用选择性引导掩码来引导不安全区域的生成。

Result: SP-Guard比现有方法生成更安全、修改更少的人工制品。

Conclusion: SP-Guard在提高图像生成安全性和可控性方面取得了成功，并强调了对模型行为的透明度和控制的重要性。

Abstract: While diffusion-based T2I models have achieved remarkable image generation quality, they also enable easy creation of harmful content, raising social concerns and highlighting the need for safer generation. Existing inference-time guiding methods lack both adaptivity--adjusting guidance strength based on the prompt--and selectivity--targeting only unsafe regions of the image. Our method, SP-Guard, addresses these limitations by estimating prompt harmfulness and applying a selective guidance mask to guide only unsafe areas. Experiments show that SP-Guard generates safer images than existing methods while minimizing unintended content alteration. Beyond improving safety, our findings highlight the importance of transparency and controllability in image generation.

</details>


### [25] [SUPER Decoder Block for Reconstruction-Aware U-Net Variants](https://arxiv.org/abs/2511.11015)
*Siheon Joo,Hongjo Kim*

Main category: cs.CV

TL;DR: SUPER是一种即插即用解码器块，用于解决U-Net变体在逆问题中信息丢失的问题，通过利用小波的完美重构（PR）特性和选择性抑制（SS）冗余特征，从而在不增加显著计算成本的情况下提高高频细节的恢复能力，并在裂缝分割和图像去噪等任务中得到验证。


<details>
  <summary>Details</summary>
Motivation: 现有的基于U-Net的逆问题求解方法存在信息丢失问题，限制了高频细节的恢复。需要一种新的架构来解决这个问题。

Method: 提出了一种名为Selectively Suppressed Perfect Reconstruction (SUPER)的即插即用解码器块。SUPER利用小波的完美重构（PR）性质防止信息丢失，并选择性抑制（SS）冗余特征。它不依赖于刚性的帧变换约束，可以集成到不同的U-Net变体中。

Result: SUPER在裂缝分割任务中显著提高了对小于4像素的裂缝的分割性能，特别是在高频细节占主导的情况下。在图像去噪任务中，SUPER也实现了PSNR的适度提升，证明了其在不同频率下的鲁棒性。SUPER在保持计算成本相当的情况下，通过增加参数量丰富了表示多样性。

Conclusion: SUPER解码器块可以作为各种U-Net变体的即插即用模块，克服了固有的重建瓶颈，提高了表示丰富度，并在高频保真度和全局相干性方面表现出色，实现了统一的、感知重建的框架。

Abstract: Skip-connected encoder-decoder architectures (U-Net variants) are widely adopted for inverse problems but still suffer from information loss, limiting recovery of fine high-frequency details. We present Selectively Suppressed Perfect Reconstruction (SUPER), which exploits the perfect reconstruction (PR) property of wavelets to prevent information degradation while selectively suppressing (SS) redundant features. Free from rigid framelet constraints, SUPER serves as a plug-and-play decoder block for diverse U-Net variants, eliminating their intrinsic reconstruction bottlenecks and enhancing representational richness. Experiments across diverse crack benchmarks, including state-of-the-art (SOTA) models, demonstrate the structural potential of the proposed SUPER Decoder Block. Maintaining comparable computational cost, SUPER enriches representational diversity through increased parameterization. In small-scale in-domain experiments on the CrackVision12K dataset, SUPER markedly improves thin-crack segmentation performance, particularly for cracks narrower than 4 px, underscoring its advantage in high-frequency dominant settings. In smartphone image denoising on SIDD, where low-frequency components prevail, SUPER still achieves a moderate gain in PSNR, confirming its robustness across low- and high-frequency regimes. These results validate its plug-and-play generality across U-Net variants, achieving high-frequency fidelity and global coherence within a unified, reconstruction-aware framework.

</details>


### [26] [EmbryoDiff: A Conditional Diffusion Framework with Multi-Focal Feature Fusion for Fine-Grained Embryo Developmental Stage Recognition](https://arxiv.org/abs/2511.11027)
*Yong Sun,Zhengjie Zhang,Junyu Shi,Zhiyuan Zhang,Lijiang Liu,Qiang Nie*

Main category: cs.CV

TL;DR: EmbryoDiff是一个基于扩散的两阶段框架，用于在体外受精（IVF）中识别精细的胚胎发育阶段。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在利用胚胎发育的先验知识和处理细胞遮挡引起的特征歧义方面存在不足。

Method: EmbryoDiff首先使用一个帧级编码器提取多焦点特征，然后通过多焦点特征融合策略构建3D感知的形态学表示，并注入混合语义-边界条件来改善扩散去噪过程，从而进行胚胎分期分类。

Result: 在两个基准数据集上，EmbryoDiff取得了最先进的性能，单步去噪准确率分别达到82.8%和81.3%。

Conclusion: EmbryoDiff通过利用发育先验和融合多焦点信息，能够有效解决细胞遮挡问题，提高胚胎发育分期识别的准确性和鲁棒性，达到当前最优水平。

Abstract: Identification of fine-grained embryo developmental stages during In Vitro Fertilization (IVF) is crucial for assessing embryo viability. Although recent deep learning methods have achieved promising accuracy, existing discriminative models fail to utilize the distributional prior of embryonic development to improve accuracy. Moreover, their reliance on single-focal information leads to incomplete embryonic representations, making them susceptible to feature ambiguity under cell occlusions. To address these limitations, we propose EmbryoDiff, a two-stage diffusion-based framework that formulates the task as a conditional sequence denoising process. Specifically, we first train and freeze a frame-level encoder to extract robust multi-focal features. In the second stage, we introduce a Multi-Focal Feature Fusion Strategy that aggregates information across focal planes to construct a 3D-aware morphological representation, effectively alleviating ambiguities arising from cell occlusions. Building on this fused representation, we derive complementary semantic and boundary cues and design a Hybrid Semantic-Boundary Condition Block to inject them into the diffusion-based denoising process, enabling accurate embryonic stage classification. Extensive experiments on two benchmark datasets show that our method achieves state-of-the-art results. Notably, with only a single denoising step, our model obtains the best average test performance, reaching 82.8% and 81.3% accuracy on the two datasets, respectively.

</details>


### [27] [Accelerating Controllable Generation via Hybrid-grained Cache](https://arxiv.org/abs/2511.11031)
*Lin Liu,Huixia Ben,Shuo Wang,Jinda Lu,Junxiang Qiu,Shengeng Tang,Yanbin Hao*

Main category: cs.CV

TL;DR: 为了提高可控生成模型的效率，提出了一种混合粒度缓存（HGC）方法，通过采用不同粒度的缓存策略来减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的可控生成模型在处理控制条件和内容生成时计算需求高，导致生成效率普遍较低。

Method: 提出混合粒度缓存（HGC）方法，包括粗粒度缓存（块级）和细粒度缓存（提示级）。粗粒度缓存基于特征重用，动态绕过编码器-解码器块中的冗余计算。细粒度缓存作用于模块内部，重用交叉注意力图并扩展到相邻步骤的相应模块计算。

Result: 在四个基准数据集上验证了HGC的有效性，特别是在平衡生成效率和视觉质量方面的优势。例如，在COCO-Stuff分割基准上，HGC将计算成本（MACs）显著降低了63%，同时保持了1.5%以内的语义保真度损失。

Conclusion: HGC方法能够有效地提高可控生成模型的效率，同时保持良好的视觉质量。

Abstract: Controllable generative models have been widely used to improve the realism of synthetic visual content. However, such models must handle control conditions and content generation computational requirements, resulting in generally low generation efficiency. To address this issue, we propose a Hybrid-Grained Cache (HGC) approach that reduces computational overhead by adopting cache strategies with different granularities at different computational stages. Specifically, (1) we use a coarse-grained cache (block-level) based on feature reuse to dynamically bypass redundant computations in encoder-decoder blocks between each step of model reasoning. (2) We design a fine-grained cache (prompt-level) that acts within a module, where the fine-grained cache reuses cross-attention maps within consecutive reasoning steps and extends them to the corresponding module computations of adjacent steps. These caches of different granularities can be seamlessly integrated into each computational link of the controllable generation process. We verify the effectiveness of HGC on four benchmark datasets, especially its advantages in balancing generation efficiency and visual quality. For example, on the COCO-Stuff segmentation benchmark, our HGC significantly reduces the computational cost (MACs) by 63% (from 18.22T to 6.70T), while keeping the loss of semantic fidelity (quantized performance degradation) within 1.5%.

</details>


### [28] [CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging](https://arxiv.org/abs/2511.11034)
*Pooja Singh,Siddhant Ujjain,Tapan Kumar Gandhi,Sandeep Kumar*

Main category: cs.CV

TL;DR: Recent advances in multimodal large language models (LLMs) enable unified processing of visual and textual inputs for medical AI. However, their compositional generalization ability across unseen combinations of imaging modality, anatomy, and task type is underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets into a unified visual question answering (VQA) format, creating 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on Related, Unrelated, and zero-overlap MAT splits. Models trained on Related splits achieve 83.2% classification accuracy and 0.75 segmentation cIoU, with performance dropping significantly under Unrelated and zero-overlap conditions. We also demonstrate cross-task transfer, where segmentation performance improves by 7% cIoU when trained using classification-only data. Traditional models show modest gains, while multimodal LLMs excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating generalization in medical vision-language models.


<details>
  <summary>Details</summary>
Motivation: To evaluate the compositional generalization ability of multimodal large language models (LLMs) in the medical domain, particularly across unseen combinations of imaging modality, anatomy, and task type.

Method: Introduced CrossMed, a benchmark using a structured Modality-Anatomy-Task (MAT) schema. Reformulated four public datasets (CheXpert, SIIM-ACR, BraTS 2020, MosMedData) into a unified visual question answering (VQA) format, creating 20,200 multiple-choice QA instances. Evaluated two open-source multimodal LLMs (LLaVA-Vicuna-7B, Qwen2-VL-7B) on Related, Unrelated, and zero-overlap MAT splits. Also evaluated traditional models (ResNet-50, U-Net) for comparison. Investigated cross-task transfer learning from classification to segmentation.

Result: Models trained on Related splits achieved 83.2% classification accuracy and 0.75 segmentation cIoU. Performance significantly dropped on Unrelated and zero-overlap splits, indicating benchmark difficulty. Cross-task transfer improved segmentation performance by 7% cIoU. Multimodal LLMs demonstrated superior compositional generalization compared to traditional models.

Conclusion: CrossMed provides a rigorous benchmark for evaluating compositional generalization in medical multimodal LLMs. Current multimodal LLMs show significant performance drops when faced with unseen combinations of modality, anatomy, and task, highlighting the need for further research in this area. The benchmark also demonstrates the potential for cross-task transfer learning and the broad utility of the MAT framework.

Abstract: Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.

</details>


### [29] [NP-LoRA: Null Space Projection Unifies Subject and Style in LoRA Fusion](https://arxiv.org/abs/2511.11051)
*Chuheng Chen,Xiaofei Zhou,Geyuan Zhang,Yong Huang*

Main category: cs.CV

TL;DR: LoRA融合存在权重主导和结构性干扰问题，导致生成保真度下降。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA融合方法依赖于权重合并，容易导致一个LoRA主导另一个，引起干扰并降低生成保真度。

Method: 提出了一种名为NP-LoRA的基于投影的框架，通过提取主方向并将其投影到正交零空间来防止结构性干扰。还引入了软投影机制来控制主题保真度和风格一致性之间的权衡。

Result: NP-LoRA在多个主干和LoRA对上一致地提高了融合质量，超过了强基线（如DINO和CLIP度量，以及人类和LLM偏好得分）。

Conclusion: NP-LoRA通过强制子空间分离来解决LoRA融合中的结构性干扰问题，从而提高生成质量。

Abstract: Low-Rank Adaptation (LoRA) fusion has emerged as a key technique for reusing and composing learned subject and style representations for controllable generation without costly retraining. However, existing methods rely on weight-based merging, where one LoRA often dominates the other, leading to interference and degraded fidelity. This interference is structural: separately trained LoRAs occupy low-rank high-dimensional subspaces, leading to non-orthogonal and overlapping representations. In this work, we analyze the internal structure of LoRAs and find their generative behavior is dominated by a few principal directions in the low-rank subspace, which should remain free from interference during fusion. To achieve this, we propose Null Space Projection LoRA (NP-LoRA), a projection-based framework for LoRA fusion that enforces subspace separation to prevent structural interference among principal directions. Specifically, we first extract principal style directions via singular value decomposition (SVD) and then project the subject LoRA into its orthogonal null space. Furthermore, we introduce a soft projection mechanism that enables smooth control over the trade-off between subject fidelity and style consistency. Experiments show NP-LoRA consistently improves fusion quality over strong baselines (e.g., DINO and CLIP-based metrics, with human and LLM preference scores), and applies broadly across backbones and LoRA pairs without retraining.

</details>


### [30] [CareCom: Generative Image Composition with Calibrated Reference Features](https://arxiv.org/abs/2511.11060)
*Jiaxuan Chen,Bo Zhang,Qingdong He,Jinlong Peng,Li Niu*

Main category: cs.CV

TL;DR: 生成式图像合成方法通过引入多参考图像并结合全局和局部特征校准来同时保留细节和调整前景姿态/视图。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成方法在细节保留和前景姿态/视图调整方面存在不足。

Method: 提出一种多参考图像的生成式合成模型，并对前景参考图像的全局和局部特征进行校准，以增强其与背景信息的兼容性。

Result: 实验证明，校准后的参考特征能够为原始参考特征提供有用的全局和局部姿态/视图信息，从而提升生成模型的性能。

Conclusion: 校准后的参考特征能够显著受益于生成模型。

Abstract: Image composition aims to seamlessly insert foreground object into background. Despite the huge progress in generative image composition, the existing methods are still struggling with simultaneous detail preservation and foreground pose/view adjustment. To address this issue, we extend the existing generative composition model to multi-reference version, which allows using arbitrary number of foreground reference images. Furthermore, we propose to calibrate the global and local features of foreground reference images to make them compatible with the background information. The calibrated reference features can supplement the original reference features with useful global and local information of proper pose/view. Extensive experiments on MVImgNet and MureCom demonstrate that the generative model can greatly benefit from the calibrated reference features.

</details>


### [31] [LiteAttention: A Temporal Sparse Attention for Diffusion Transformers](https://arxiv.org/abs/2511.11062)
*Dor Shmilovich,Tony Wu,Aviad Dahan,Yuval Domb*

Main category: cs.CV

TL;DR: Diffusion Transformers在视频生成方面质量优异，但注意力机制的二次复杂度导致延迟过高。LiteAttention利用稀疏注意力模式在去噪步骤中的时间相干性，通过提前标记非必要图块并向前传播跳过决策，消除了冗余的注意力计算，实现了在不牺牲质量的情况下大幅加速视频扩散模型。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在视频生成中质量优异，但注意力机制的二次复杂度导致延迟过高。现有加速方法在计算开销、估计误差和效率之间存在权衡。

Method: LiteAttention利用扩散注意力稀疏模式在去噪步骤中的时间相干性。通过早期标记非必要图块并向前传播跳过决策，实现了演化计算跳过，无需重复的分析开销，结合了动态方法和静态方法的优点。

Result: 在FlashAttention的基础上实现高度优化的LiteAttention内核，在生产视频扩散模型上实现了显著的加速，且没有质量损失。

Conclusion: LiteAttention通过利用注意力稀疏模式的时间相干性，有效地解决了扩散Transformer的延迟问题，实现了高效且不损失质量的视频生成。

Abstract: Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+δ$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.

</details>


### [32] [From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening](https://arxiv.org/abs/2511.11065)
*Muskaan Chopra,Lorenz Sparrenberg,Armin Berger,Sarthak Khanna,Jan H. Terheyden,Rafet Sifa*

Main category: cs.CV

TL;DR: 深度学习在糖尿病视网膜病变（DR）筛查中的应用研究，重点关注2016-2025年的进展、方法、结果和挑战，并为临床应用提出建议。


<details>
  <summary>Details</summary>
Motivation: 早期检测对减少全球视力损失至关重要，深度学习在DR筛查中取得了显著进展。

Method: 系统性地回顾了50多项研究和20多个数据集，重点考察了自监督学习、半监督学习、域泛化、联邦训练和混合神经符号模型等方法，以及评估协议、报告标准和可复现性挑战。

Result: 分析了不同方法在各种数据集上的性能，并指出了多中心验证和临床信任方面的开放性问题。

Conclusion: 提出了一个关于可复现、隐私保护和临床可部署的DR人工智能的实际议程，并指出许多创新也适用于大规模医学影像分析。

Abstract: Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.

</details>


### [33] [S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation](https://arxiv.org/abs/2511.11066)
*Jiechao Gao,Chang Liu,Yuangang Li*

Main category: cs.CV

TL;DR: 现有的放射学报告生成（RRG）方法主要通过监督微调（SFT）来优化图像-报告对的跨模态对齐，但这未能实现基于解剖的对齐，导致生成质量不佳。本文提出了S2D-Align，一种新颖的SFT范式，通过利用不同粒度的辅助信号来实现基于解剖的对齐。S2D-Align采用从浅入深的策略，逐步丰富对齐过程：从粗略的影像-报告配对开始，引入参考报告进行实例级引导，最终利用关键词将生成内容锚定在特定的解剖细节上。通过内存适配器实现了不同对齐阶段的特征共享。在MIMIC-CXR和IU X-Ray数据集上的实验结果表明，S2D-Align取得了最先进的性能，并且消融研究验证了其多阶段、辅助引导方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调（SFT）方法主要关注图像-报告对的实例级对齐，未能实现基于解剖的对齐，导致报告生成质量不佳。

Method: 提出了一种名为S2D-Align的新颖SFT范式，采用从浅入深的策略，逐步引入粗略的影像-报告配对、参考报告引导以及关键词的解剖细节锚定。利用内存适配器桥接不同对齐阶段，实现特征共享。

Result: 在MIMIC-CXR和IU X-Ray数据集上取得了最先进的性能，消融研究验证了该方法的有效性。

Conclusion: S2D-Align通过多阶段、辅助引导的方法，有效增强了复杂多模态生成任务的接地能力，为提高报告生成质量提供了一个有前景的方向。

Abstract: Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.

</details>


### [34] [Evaluating Latent Generative Paradigms for High-Fidelity 3D Shape Completion from a Single Depth Image](https://arxiv.org/abs/2511.11074)
*Matthias Humt,Ulrich Hillenbrand,Rudolph Triebel*

Main category: cs.CV

TL;DR: 生成模型在3D数据领域已得到广泛应用，但仍未就任务的适用模型达成共识。本文比较了两种有前途的生成模型（扩散模型和自回归模型）在3D形状生成和补全任务上的表现，并进行了全面的定量评估。结果显示，扩散模型在从单幅深度图像进行多模态形状补全方面表现最佳，而自回归模型在相同离散潜在空间下可媲美甚至超越扩散模型。


<details>
  <summary>Details</summary>
Motivation: 评估两种有前途的生成模型（扩散模型和自回归模型）在3D形状生成和补全任务上的适用性，特别是探讨了使用部分3D数据作为条件信息进行引导的可行性。

Method: 将 Denoising Diffusion Probabilistic Models 和 Autoregressive Causal Transformers 两种模型应用于生成性形状建模和补全任务，并与基线判别模型进行了比较，同时进行广泛的消融研究。

Result: 1. 具有连续潜在变量的扩散模型在多模态形状补全任务上表现优于判别模型和自回归模型，在从单幅含噪深度图像进行补全时达到了最先进的性能。 2. 在相同的离散潜在空间下，自回归模型在这些任务上的表现可以匹配甚至超过扩散模型。

Conclusion: 扩散模型在处理连续潜在变量时在3D形状补全任务上表现出色，而自回归模型在离散潜在空间下也具有很强的竞争力，表明两种模型在不同设置下都有其优势。

Abstract: While generative models have seen significant adoption across a wide range of data modalities, including 3D data, a consensus on which model is best suited for which task has yet to be reached. Further, conditional information such as text and images to steer the generation process are frequently employed, whereas others, like partial 3D data, have not been thoroughly evaluated. In this work, we compare two of the most promising generative models--Denoising Diffusion Probabilistic Models and Autoregressive Causal Transformers--which we adapt for the tasks of generative shape modeling and completion. We conduct a thorough quantitative evaluation and comparison of both tasks, including a baseline discriminative model and an extensive ablation study. Our results show that (1) the diffusion model with continuous latents outperforms both the discriminative model and the autoregressive approach and delivers state-of-the-art performance on multi-modal shape completion from a single, noisy depth image under realistic conditions and (2) when compared on the same discrete latent space, the autoregressive model can match or exceed diffusion performance on these tasks.

</details>


### [35] [A Space-Time Transformer for Precipitation Forecasting](https://arxiv.org/abs/2511.11090)
*Levi Harris,Tianlong Chen*

Main category: cs.CV

TL;DR:  SaTformer是一个基于全空间时间注意力的视频Transformer模型，用于从卫星数据预测极端降水。


<details>
  <summary>Details</summary>
Motivation: 传统的数值天气预报（NWP）模型在计算成本和短期预报（0-4小时）的性能方面存在局限性。而现有的人工智能天气预测（AI-WP）方法较少应用视频理解架构。

Method: 提出SaTformer模型，该模型基于全空间时间注意力机制，并采用将降水回归重构为分类问题和类别加权损失函数的方法来处理长尾分布的降水数据集。

Result: SaTformer在NeurIPS Weather4Cast 2025累积降水挑战赛中取得第一名。

Conclusion: SaTformer模型成功地从卫星辐射数据中预测极端降水，并能有效处理长尾分布的降水数据集。

Abstract: Meteorological agencies around the world rely on real-time flood guidance to issue live-saving advisories and warnings. For decades traditional numerical weather prediction (NWP) models have been state-of-the-art for precipitation forecasting. However, physically-parameterized models suffer from a few core limitations: first, solving PDEs to resolve atmospheric dynamics is computationally demanding, and second, these methods degrade in performance at nowcasting timescales (i.e., 0-4 hour lead-times). Motivated by these shortcomings, recent work proposes AI-weather prediction (AI-WP) alternatives that learn to emulate analysis data with neural networks. While these data-driven approaches have enjoyed enormous success across diverse spatial and temporal resolutions, applications of video-understanding architectures for weather forecasting remain underexplored. To address these gaps, we propose SaTformer: a video transformer built on full space-time attention that skillfully forecasts extreme precipitation from satellite radiances. Along with our novel architecture, we introduce techniques to tame long-tailed precipitation datasets. Namely, we reformulate precipitation regression into a classification problem, and employ a class-weighted loss to address label imbalances. Our model scored first place on the NeurIPS Weather4Cast 2025 Cumulative Rainfall challenge. Code and model weights are available: https://github.com/leharris3/satformer

</details>


### [36] [Machine-Learning Based Detection of Coronary Artery Calcification Using Synthetic Chest X-Rays](https://arxiv.org/abs/2511.11093)
*Dylan Saeed,Ramtin Gharleghi,Susann Bier,Sonit Singh*

Main category: cs.CV

TL;DR: DRRs are a viable alternative to CT scans for training AI models to detect coronary artery calcification (CAC), offering a scalable, label-rich foundation for research and development.


<details>
  <summary>Details</summary>
Motivation: CAC prediction using CT is accurate but costly, while chest X-rays (CXRs) are cheap but lack reliable labels for deep learning. DRRs, projected from CT scans, offer a scalable solution with precise labels.

Method: The study systematically evaluated DRRs as a training domain for CAC detection using 667 CT scans. They explored model capacity, super-resolution, preprocessing, and training strategies, including lightweight CNNs, super-resolution with contrast enhancement, and curriculum learning.

Result: Lightweight CNNs trained from scratch outperformed larger pre-trained networks. Super-resolution combined with contrast enhancement improved performance. Curriculum learning helped stabilize training with weak supervision. The best model achieved a mean AUC of 0.754, comparable to or better than existing CXR-based methods.

Conclusion: DRRs provide a scalable and label-rich foundation for CAC detection, demonstrating potential for future transfer learning and domain adaptation to real CXRs.

Abstract: Coronary artery calcification (CAC) is a strong predictor of cardiovascular events, with CT-based Agatston scoring widely regarded as the clinical gold standard. However, CT is costly and impractical for large-scale screening, while chest X-rays (CXRs) are inexpensive but lack reliable ground truth labels, constraining deep learning development. Digitally reconstructed radiographs (DRRs) offer a scalable alternative by projecting CT volumes into CXR-like images while inheriting precise labels. In this work, we provide the first systematic evaluation of DRRs as a surrogate training domain for CAC detection. Using 667 CT scans from the COCA dataset, we generate synthetic DRRs and assess model capacity, super-resolution fidelity enhancement, preprocessing, and training strategies. Lightweight CNNs trained from scratch outperform large pretrained networks; pairing super-resolution with contrast enhancement yields significant gains; and curriculum learning stabilises training under weak supervision. Our best configuration achieves a mean AUC of 0.754, comparable to or exceeding prior CXR-based studies. These results establish DRRs as a scalable, label-rich foundation for CAC detection, while laying the foundation for future transfer learning and domain adaptation to real CXRs.

</details>


### [37] [VIDEOP2R: Video Understanding from Perception to Reasoning](https://arxiv.org/abs/2511.11113)
*Yifan Jiang,Yueying Wang,Rui Zhao,Toufiq Parag,Zhimin Chen,Zhenyu Liao,Jayakrishnan Unnikrishnan*

Main category: cs.CV

TL;DR: VideoP2R是一个新的框架，通过将感知和推理视为独立过程来提高大型视频语言模型的视频推理能力，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 由于在大型视频语言模型（LVLMs）中扩展强化微调（RFT）的挑战，需要开发新的方法来增强视频推理能力。

Method: VideoP2R框架包括一个监督微调（SFT）阶段，用于生成一个过程感知的链式思考（CoT）数据集（VideoP2R-CoT-162K），以及一个强化学习（RL）阶段，采用过程感知的组相对策略优化（PA-GRPO）算法，为感知和推理提供单独的奖励。

Result: VideoP2R在七个视频推理和理解基准测试中的六个取得了最先进（SotA）的性能。模型消融研究证实了过程感知建模和PA-GRPO的有效性，并表明模型的感知输出对于下游推理来说信息量充足。

Conclusion: VideoP2R框架通过显式建模感知和推理过程，并采用PA-GRPO算法，在提高LVLMs的视频推理能力方面是有效的。

Abstract: Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.

</details>


### [38] [Hindsight Distillation Reasoning with Knowledge Encouragement Preference for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2511.11132)
*Yu Zhao,Ying Zhang,Xuhui Sui,Baohang Zhou,Li Shen,Dacheng Tao*

Main category: cs.CV

TL;DR: KBVQA需要结合外部知识。现有方法要么依赖隐式知识，要么依赖检索增强生成，但推理过程仍然是隐式的。我们提出了HinD框架，结合KEPO，以激活和利用MLLMs的内部知识推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有KBVQA方法推理过程不明确，缺乏多步推理轨迹。

Method: 1. 提出Hindsight-Zero数据集：提示冻结的7B MLLM完成问题和 ground truth answer之间的推理过程。2. 自我蒸馏Hindsight-Zero得到CoT Generator和Knowledge Generator。3. 使用KEPO优化Knowledge Generator，偏好不那么自信但有用的知识。4. 利用生成的CoT和知识进行答案预测。

Result: 在OK-VQA和A-OKVQA数据集上，HinD框架通过激活7B MLLM的推理能力，在没有商业模型API或外部知识的情况下取得了优越的性能。

Conclusion: HinD框架结合KEPO，能够有效利用MLLMs的内部知识推理能力，提升KBVQA性能。

Abstract: Knowledge-based Visual Question Answering (KBVQA) necessitates external knowledge incorporation beyond cross-modal understanding. Existing KBVQA methods either utilize implicit knowledge in multimodal large language models (MLLMs) via in-context learning or explicit knowledge via retrieval augmented generation. However, their reasoning processes remain implicit, without explicit multi-step trajectories from MLLMs. To address this gap, we provide a Hindsight Distilled Reasoning (HinD) framework with Knowledge Encouragement Preference Optimization (KEPO), designed to elicit and harness internal knowledge reasoning ability in MLLMs. First, to tackle the reasoning supervision problem, we propose to emphasize the hindsight wisdom of MLLM by prompting a frozen 7B-size MLLM to complete the reasoning process between the question and its ground truth answer, constructing Hindsight-Zero training data. Then we self-distill Hindsight-Zero into Chain-of-Thought (CoT) Generator and Knowledge Generator, enabling the generation of sequential steps and discrete facts. Secondly, to tackle the misalignment between knowledge correctness and confidence, we optimize the Knowledge Generator with KEPO, preferring under-confident but helpful knowledge over the over-confident but unhelpful one. The generated CoT and sampled knowledge are then exploited for answer prediction. Experiments on OK-VQA and A-OKVQA validate the effectiveness of HinD, showing that HinD with elicited reasoning from 7B-size MLLM achieves superior performance without commercial model APIs or outside knowledge.

</details>


### [39] [OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation](https://arxiv.org/abs/2511.11162)
*Zhanpeng Wang,Shuting Cao,Yuhang Lu,Yuhan Li,Na Lei,Zhongxuan Luo*

Main category: cs.CV

TL;DR: DDIB是一种新的图像翻译方法，但效率低且可能出现轨迹偏差。OT-ALD是一个新的框架，利用最优传输理论解决这些问题，提高了效率和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决DDIB图像翻译方法效率低和翻译轨迹偏差的问题，同时保留其优点。

Method: 计算一个从源域到目标域的潜在分布的最优传输图，并使用映射后的分布作为目标域中反向扩散过程的起点。

Result: OT-ALD消除了潜在分布不匹配，提高了采样效率（平均20.29%），并降低了FID分数（平均2.6），在图像质量方面取得了更好的平衡。

Conclusion: OT-ALD通过最优传输理论解决了DDIB的局限性，实现了更快的图像翻译和更高的图像质量。

Abstract: The Dual Diffusion Implicit Bridge (DDIB) is an emerging image-to-image (I2I) translation method that preserves cycle consistency while achieving strong flexibility. It links two independently trained diffusion models (DMs) in the source and target domains by first adding noise to a source image to obtain a latent code, then denoising it in the target domain to generate the translated image. However, this method faces two key challenges: (1) low translation efficiency, and (2) translation trajectory deviations caused by mismatched latent distributions. To address these issues, we propose a novel I2I translation framework, OT-ALD, grounded in optimal transport (OT) theory, which retains the strengths of DDIB-based approach. Specifically, we compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point for the reverse diffusion process in the target domain. Our error analysis confirms that OT-ALD eliminates latent distribution mismatches. Moreover, OT-ALD effectively balances faster image translation with improved image quality. Experiments on four translation tasks across three high-resolution datasets show that OT-ALD improves sampling efficiency by 20.29% and reduces the FID score by 2.6 on average compared to the top-performing baseline models.

</details>


### [40] [Reverberation: Learning the Latencies Before Forecasting Trajectories](https://arxiv.org/abs/2511.11164)
*Conghao Wong,Ziqian Zou,Beihao Xia,Xinge You*

Main category: cs.CV

TL;DR: 该研究提出了一种名为Reverberation（Rev）的轨迹预测模型，通过引入“reverberation transform”来显式学习和预测不同智能体对轨迹改变事件的反应延迟（latency），从而实现更可控和可解释的轨迹预测。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹预测方法在显式学习和预测智能体对轨迹改变事件的反应延迟方面存在挑战，这种延迟会影响预测的因果连续性和合理性。

Method: 提出了一种新的reverberation transform，并构建了相应的Rev模型。该模型使用两个可学习的reverberation kernels来模拟和预测每个智能体的延迟偏好及其随机性。

Result: 在多个数据集（包括行人、车辆）上的实验表明，Rev模型在实现具有竞争力的预测精度的同时，还能揭示不同智能体和场景下的可解释延迟动态。定性分析也验证了reverberation transform的有效性。

Conclusion: Rev模型及其reverberation transform为轨迹预测任务提供了一种新的、可控且可解释的延迟建模方法，在实际应用中具有潜力。

Abstract: Bridging the past to the future, connecting agents both spatially and temporally, lies at the core of the trajectory prediction task. Despite great efforts, it remains challenging to explicitly learn and predict latencies, the temporal delays with which agents respond to different trajectory-changing events and adjust their future paths, whether on their own or interactively. Different agents may exhibit distinct latency preferences for noticing, processing, and reacting to any specific trajectory-changing event. The lack of consideration of such latencies may undermine the causal continuity of the forecasting system and also lead to implausible or unintended trajectories. Inspired by the reverberation curves in acoustics, we propose a new reverberation transform and the corresponding Reverberation (short for Rev) trajectory prediction model, which simulates and predicts different latency preferences of each agent as well as their stochasticity by using two explicit and learnable reverberation kernels, allowing for the controllable trajectory prediction based on these forecasted latencies. Experiments on multiple datasets, whether pedestrians or vehicles, demonstrate that Rev achieves competitive accuracy while revealing interpretable latency dynamics across agents and scenarios. Qualitative analyses further verify the properties of the proposed reverberation transform, highlighting its potential as a general latency modeling approach.

</details>


### [41] [Explainable Deep Convolutional Multi-Type Anomaly Detection](https://arxiv.org/abs/2511.11165)
*Alex George,Lyudmila Mihaylova,Sean Anderson*

Main category: cs.CV

TL;DR: MultiTypeFCDD是一个轻量级的卷积框架，可以区分不同类型的异常，而无需为每个对象类别训练单独的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释异常检测方法在区分异常类型方面存在不足，并且通常需要为每个对象类别训练单独的模型，这在计算上成本很高。

Method: MultiTypeFCDD使用图像级标签来学习和生成多通道热图，每个通道对应一个特定的异常类型。该模型是一个单一的、统一的框架，能够区分多种对象类别中的异常类型。

Result: 在Real-IAD数据集上的评估结果显示，MultiTypeFCDD的性能与最先进的复杂模型相当，但参数量和推理时间大大减少。

Conclusion: MultiTypeFCDD提供了一种实用且可行的解决方案，适用于计算资源受限的实际应用中需要可解释的多类型异常检测的场景。

Abstract: Most explainable anomaly detection methods often identify anomalies but lack the capability to differentiate the type of anomaly. Furthermore, they often require the costly training and maintenance of separate models for each object category. The lack of specificity is a significant research gap, as identifying the type of anomaly (e.g., "Crack" vs. "Scratch") is crucial for accurate diagnosis that facilitates cost-saving operational decisions across diverse application domains. While some recent large-scale Vision-Language Models (VLMs) have begun to address this, they are computationally intensive and memory-heavy, restricting their use in real-time or embedded systems. We propose MultiTypeFCDD, a simple and lightweight convolutional framework designed as a practical alternative for explainable multi-type anomaly detection. MultiTypeFCDD uses only image-level labels to learn and produce multi-channel heatmaps, where each channel is trained to correspond to a specific anomaly type. The model functions as a single, unified framework capable of differentiating anomaly types across multiple object categories, eliminating the need to train and manage separate models for each object category. We evaluated our proposed method on the Real-IAD dataset and it delivers results competitive with state-of-the-art complex models at significantly reduced parametric load and inference times. This makes it a highly practical and viable solution for real-world applications where computational resources are tightly constrained.

</details>


### [42] [CATS-V2V: A Real-World Vehicle-to-Vehicle Cooperative Perception Dataset with Complex Adverse Traffic Scenarios](https://arxiv.org/abs/2511.11168)
*Hangyu Li,Bofeng Cao,Zhaohui Liang,Wuzhen Li,Juyoung Oh,Yuxuan Chen,Shixiao Liang,Hang Zhou,Chengyuan Ma,Jiaxi Liu,Zheng Li,Peng Zhang,KeKe Long,Maolin Liu,Jackson Jiang,Chunlei Yu,Shengxiang Liu,Hongkai Yu,Xiaopeng Li*

Main category: cs.CV

TL;DR: 该论文引入了CATS-V2V，一个用于复杂恶劣交通场景（CATS）下车对车（V2V）协同感知的真实世界数据集，旨在解决现有数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注普通交通场景，未能满足复杂恶劣交通场景下V2V协同感知对数据的需求。

Method: 收集了100个数据片段，包含10种天气和光照条件，跨越10个不同地点。数据包括LiDAR点云、相机图像、GNSS和IMU记录。提出了一种基于目标的时序对齐方法。

Result: 该数据集是迄今为止规模最大、支持最全面、质量最高的数据集，包含60K帧10Hz的LiDAR点云、1.26M张30Hz的多视角相机图像以及750K条GNSS和IMU记录，并提供时间一致的3D边界框和静态场景注释。

Conclusion: CATS-V2V数据集旨在推动V2V协同感知在复杂恶劣交通场景下的发展，为自动驾驶社区提供支持。

Abstract: Vehicle-to-Vehicle (V2V) cooperative perception has great potential to enhance autonomous driving performance by overcoming perception limitations in complex adverse traffic scenarios (CATS). Meanwhile, data serves as the fundamental infrastructure for modern autonomous driving AI. However, due to stringent data collection requirements, existing datasets focus primarily on ordinary traffic scenarios, constraining the benefits of cooperative perception. To address this challenge, we introduce CATS-V2V, the first-of-its-kind real-world dataset for V2V cooperative perception under complex adverse traffic scenarios. The dataset was collected by two hardware time-synchronized vehicles, covering 10 weather and lighting conditions across 10 diverse locations. The 100-clip dataset includes 60K frames of 10 Hz LiDAR point clouds and 1.26M multi-view 30 Hz camera images, along with 750K anonymized yet high-precision RTK-fixed GNSS and IMU records. Correspondingly, we provide time-consistent 3D bounding box annotations for objects, as well as static scenes to construct a 4D BEV representation. On this basis, we propose a target-based temporal alignment method, ensuring that all objects are precisely aligned across all sensor modalities. We hope that CATS-V2V, the largest-scale, most supportive, and highest-quality dataset of its kind to date, will benefit the autonomous driving community in related tasks.

</details>


### [43] [Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA](https://arxiv.org/abs/2511.11169)
*Ayush Pandey,Jai Bardhan,Ishita Jain,Ramya S Hebbalaguppe,Rohan Raju Dhanakshirur,Lovekesh Vig*

Main category: cs.CV

TL;DR: AlignVQA是一个基于辩论的多智能体框架，通过让多个专门的视觉语言模型（VLMs）生成答案并进行两阶段交互（由通才智能体进行批判、完善和聚合），来提高VQA系统对其答案正确性的置信度估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着VQA系统在医疗诊断和自动导航等高风险领域的应用日益广泛，确保其置信度估计的可靠性至关重要，因为这些系统常常给出过于自信的回答。

Method: AlignVQA框架促使多个专门的VLM采用不同的提示策略生成候选答案，然后由通才智能体进行两阶段交互，对其进行批判、完善和聚合，从而获得更准确的置信度估计。该研究还引入了一种名为aligncal的可微校准感知损失函数，用于微调专门的智能体。

Result: 通过在多个基准VQA数据集上的实证结果表明，AlignVQA能够显著减少校准差异，并且经过训练的专门智能体能够提供更准确的置信度估计。

Conclusion: AlignVQA框架和aligncal损失函数能够有效提高VQA系统的校准性能，使其置信度估计更能准确地反映其预测性能。

Abstract: In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.

</details>


### [44] [Dynamic Gaussian Scene Reconstruction from Unsynchronized Videos](https://arxiv.org/abs/2511.11175)
*Zhixin Xu,Hengyu Zhou,Yuan Liu,Wenhan Xue,Hao Pan,Wenping Wang,Bin Wang*

Main category: cs.CV

TL;DR: 本研究提出了一种用于非同步多视角视频的高质量4D高斯溅射重建的时间对齐策略，解决了现有技术依赖时间同步的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决了现实世界中多视角视频数据因相机触发延迟或独立设置导致的时间不同步问题，这会影响重建质量。

Method: 提出了一种粗到精的时间对齐模块，用于估计和补偿每个相机的时移，首先确定粗略的帧级偏移，然后进行亚帧精度优化。

Result: 实验表明，该方法能够有效处理时间失配的视频，并显著提升基线方法的性能。

Conclusion: 该策略可以无缝集成到现有的4D高斯溅射框架中，提高了其处理异步数据的鲁棒性。

Abstract: Multi-view video reconstruction plays a vital role in computer vision, enabling applications in film production, virtual reality, and motion analysis. While recent advances such as 4D Gaussian Splatting (4DGS) have demonstrated impressive capabilities in dynamic scene reconstruction, they typically rely on the assumption that input video streams are temporally synchronized. However, in real-world scenarios, this assumption often fails due to factors like camera trigger delays or independent recording setups, leading to temporal misalignment across views and reduced reconstruction quality. To address this challenge, a novel temporal alignment strategy is proposed for high-quality 4DGS reconstruction from unsynchronized multi-view videos. Our method features a coarse-to-fine alignment module that estimates and compensates for each camera's time shift. The method first determines a coarse, frame-level offset and then refines it to achieve sub-frame accuracy. This strategy can be integrated as a readily integrable module into existing 4DGS frameworks, enhancing their robustness when handling asynchronous data. Experiments show that our approach effectively processes temporally misaligned videos and significantly enhances baseline methods.

</details>


### [45] [Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation](https://arxiv.org/abs/2511.11177)
*Quoc-Huy Trinh,Mustapha Abdullahi,Do Duy Hung Trinh,Bo Zhao,Debesh Jha*

Main category: cs.CV

TL;DR: Transformer架构的MLLM计算成本高且在细粒度任务上表现不佳。Viper-F1使用高效的Liquid State-Space Dynamics替代Transformer，并引入Token-Grid Correlation Module来增强视觉-语言的关联性，实现了高精度、高效率的细粒度理解。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM计算成本高，难以在资源受限场景部署；小型模型在细粒度推理任务上表现不佳。

Method: 提出Viper-F1，一种混合状态空间视觉-语言模型，用Liquid State-Space Dynamics替代Attention，并引入Token-Grid Correlation Module，通过FiLM条件作用于状态空间动力学，以增强视觉定位能力。

Result: Viper-F1在多个基准测试中取得了准确、细粒度的理解，并显著提高了效率。

Conclusion: Viper-F1通过使用高效的Liquid State-Space Dynamics和增强视觉关联性的方法，克服了现有MLLM的效率和细粒度处理的局限性。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled impressive progress in vision-language understanding, yet their high computational cost limits deployment in resource-constrained scenarios such as robotic manipulation, personal assistants, and smart cameras. Most existing methods rely on Transformer-based cross-attention, whose quadratic complexity hinders efficiency. Moreover, small vision-language models often struggle to precisely capture fine-grained, task-relevant visual regions, leading to degraded performance on fine-grained reasoning tasks that limit their effectiveness in the real world. To address these issues, we introduce Viper-F1, a Hybrid State-Space Vision-Language Model that replaces attention with efficient Liquid State-Space Dynamics. To further enhance visual grounding, we propose a Token-Grid Correlation Module, which computes lightweight correlations between text tokens and image patches and modulates the state-space dynamics via FiLM conditioning. This enables the model to selectively emphasize visual regions relevant to the textual prompt while maintaining linear-time inference. Experimental results across multiple benchmarks demonstrate that Viper-F1 achieves accurate, fine-grained understanding with significantly improved efficiency.

</details>


### [46] [A Comparison of Lightweight Deep Learning Models for Particulate-Matter Nowcasting in the Indian Subcontinent & Surrounding Regions](https://arxiv.org/abs/2511.11185)
*Ansh Kushwaha,Kaushik Gopalan*

Main category: cs.CV

TL;DR: 该论文提出了一个高效的框架，用于预测印度次大陆主要地区的PM1、PM2.5和PM10在6小时内的污染水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确预测未来6小时PM2.5、PM10和PM1空气质量指数的模型，并为Weather4Cast~2025年的污染任务提供支持。

Method: 利用欧洲中期天气预报中心（ECMWF）的哥白尼大气监测服务（CAMS）提供的全球大气成分预报（0.4度分辨率）作为模型输入。模型使用2021-2023年的CAMS分析数据进行训练（90/10随机划分），并在2024年的数据上进行独立评估。开发了三种轻量级的、参数特定的架构，以提高准确性、最小化系统偏差并实现快速推理。

Result: 与Aurora基础模型相比，该方法在RMSE、MAE、Bias和SSIM指标上均取得了显著的性能提升。

Conclusion: 紧凑且专门化的深度学习模型在有限空间区域的短期预测中非常有效，能够超越更通用的基础模型。

Abstract: This paper is a submission for the Weather4Cast~2025 complementary Pollution Task and presents an efficient framework for 6-hour lead-time nowcasting of PM$_1$, PM$_{2.5}$, and PM$_{10}$ across the Indian subcontinent and surrounding regions. The proposed approach leverages analysis fields from the Copernicus Atmosphere Monitoring Service (CAMS) Global Atmospheric Composition Forecasts at 0.4 degree resolution. A 256x256 spatial region, covering 28.4S-73.6N and 32E-134.0E, is used as the model input, while predictions are generated for the central 128x128 area spanning 2.8S-48N and 57.6E-108.4E, ensuring an India-centric forecast domain with sufficient synoptic-scale context. Models are trained on CAMS analyses from 2021-2023 using a shuffled 90/10 split and independently evaluated on 2024 data. Three lightweight parameter-specific architectures are developed to improve accuracy, minimize systematic bias, and enable rapid inference. Evaluation using RMSE, MAE, Bias, and SSIM demonstrates substantial performance gains over the Aurora foundation model, underscoring the effectiveness of compact & specialized deep learning models for short-range forecasts on limited spatial domains.

</details>


### [47] [Computationally-efficient deep learning models for nowcasting of precipitation: A solution for the Weather4cast 2025 challenge](https://arxiv.org/abs/2511.11197)
*Anushree Bhuskute,Kaushik Gopalan,Jeet Shah*

Main category: cs.CV

TL;DR: 本研究提出了一种基于卷积门控循环单元（ConvGRU）的迁移学习框架，用于Weather4Cast 2025竞赛的短期降雨预测。


<details>
  <summary>Details</summary>
Motivation: 使用单一的SEVIRI红外通道（10.8 μm波长）作为输入，包含一小时内的四次观测，旨在进行短期降雨预测。

Method: 采用两阶段训练策略：第一阶段，训练ConvGRU预测SEVIRI的亮温，以捕捉时空模式；第二阶段，通过经验非线性变换将预测场映射到OPERA兼容的降雨率。对于事件预测任务，将预测的降雨量通过3D事件检测和时空特征提取来识别和表征降雨事件。

Result: 该模型在累积降雨量任务中取得了第二名的成绩。在事件预测任务中，使用相同的模型取得了与基线模型相似的分数。

Conclusion: 基于ConvGRU的迁移学习框架在短期降雨预测方面表现出良好性能，特别是在累积降雨量任务中取得了优异成绩。

Abstract: This study presents a transfer-learning framework based on Convolutional Gated Recurrent Units (ConvGRU) for short-term rainfall prediction in the Weather4Cast 2025 competition. A single SEVIRI infrared channel (10.8 μm wavelength) is used as input, which consists of four observations over a one-hour period. A two-stage training strategy is applied to generate rainfall estimates up to four hours ahead. In the first stage, ConvGRU is trained to forecast the brightness temperatures from SEVIRI, enabling the model to capture relevant spatiotemporal patterns. In the second stage, an empirically derived nonlinear transformation maps the predicted fields to OPERA-compatible rainfall rates.
  For the event-prediction task, the transformed rainfall forecasts are processed using 3D event detection followed by spatiotemporal feature extraction to identify and characterize precipitation events. Our submission achieved 2nd place in the cumulative rainfall task. Further, the same model was used out-of-the-box for the event prediction task, and resulted in similar scores as the baseline model to the competition.

</details>


### [48] [Geospatial Chain of Thought Reasoning for Enhanced Visual Question Answering on Satellite Imagery](https://arxiv.org/abs/2511.11198)
*Shambhavi Shanker,Manikandan Padmanaban,Jagabondhu Hazra*

Main category: cs.CV

TL;DR: Geospatial CoT reasoning with DPO improves VQA for satellite imagery in climate applications.


<details>
  <summary>Details</summary>
Motivation: Existing VQA models lack structured reasoning for complex geospatial queries in climate domains.

Method: Propose a VQA framework integrating Chain of Thought (CoT) reasoning with Direct Preference Optimization (DPO) to generate intermediate rationales for detection, classification, spatial relations, and comparative analysis.

Result: CoT supervision improves accuracy by 34.9% over direct baselines, and DPO further enhances accuracy and reasoning quality.

Conclusion: The developed system advances VQA for multispectral Earth observation, enabling richer geospatial reasoning and more effective climate use cases.

Abstract: Geospatial chain of thought (CoT) reasoning is essential for advancing Visual Question Answering (VQA) on satellite imagery, particularly in climate related applications such as disaster monitoring, infrastructure risk assessment, urban resilience planning, and policy support. Existing VQA models enable scalable interpretation of remote sensing data but often lack the structured reasoning required for complex geospatial queries. We propose a VQA framework that integrates CoT reasoning with Direct Preference Optimization (DPO) to improve interpretability, robustness, and accuracy. By generating intermediate rationales, the model better handles tasks involving detection, classification, spatial relations, and comparative analysis, which are critical for reliable decision support in high stakes climate domains. Experiments show that CoT supervision improves accuracy by 34.9\% over direct baselines, while DPO yields additional gains in accuracy and reasoning quality. The resulting system advances VQA for multispectral Earth observation by enabling richer geospatial reasoning and more effective climate use cases.

</details>


### [49] [Questioning the Stability of Visual Question Answering](https://arxiv.org/abs/2511.11206)
*Amir Rosenfeld,Neta Glazer,Ethan Fetaya*

Main category: cs.CV

TL;DR: 视觉语言模型（VLM）在面对细微的、保持语义的输入变化时表现出高度不稳定性，即使是先进的模型也容易出错。然而，样本级别的稳定性可以用来预测模型答案的正确性，这为提升模型可靠性提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 评估视觉语言模型（VLM）在面对微小、保持语义的输入变化时的鲁棒性，并利用样本稳定性来预测模型性能。

Method: 对一系列VLM模型和数据集进行大规模、系统性的研究，测试其在像素级变化、几何变换、重述和多语言改写等无害扰动下的表现。同时，分析样本稳定性和正确性之间的关系，并尝试使用小模型来预测大模型的正确性。

Result: 发现当前的VLM模型，包括GPT-4o和Gemini 2.0 Flash，对细微的视觉和文本扰动非常敏感，导致大量样本的预测答案发生改变。样本稳定性的确是衡量模型正确性的有力指标，稳定的样本更可能被正确回答。利用这一点，可以通过小模型的稳定性模式高精度地预测大模型的正确性。

Conclusion: 目前的VLM存在根本性的脆弱性，对细微输入的敏感性暴露了其不足。未来的研究应超越对抗性扰动，关注模型应保持不变性的基本鲁棒性评估。

Abstract: Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.

</details>


### [50] [One-to-N Backdoor Attack in 3D Point Cloud via Spherical Trigger](https://arxiv.org/abs/2511.11210)
*Dongmei Shan,Wei Lian,Chongxia Wang*

Main category: cs.CV

TL;DR: 提出了一种新的针对3D视觉的一对多后门攻击框架，该框架基于可配置的球形触发器，能够用单个触发器编码多个目标类别，并在多个数据集和模型架构上实现了高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的3D点云后门攻击主要局限于一对一的范式，无法满足一对多的攻击需求。

Method: 提出了一种新颖的、可配置的球形触发器，并建立了3D中一对多后门攻击的理论基础，利用球体的空间属性作为参数空间，使单个触发器设计能够编码多个目标类别。

Result: 实验结果在多个数据集和模型架构上进行了系统验证，证明了中毒模型可以将不同的触发器配置映射到不同的目标标签，攻击成功率高达100%，同时保持了对干净数据的准确性。

Conclusion: 这项工作为3D视觉中的多目标威胁建立了关键的基准，并为保护未来的3D驱动智能系统提供了基础理解。

Abstract: Backdoor attacks represent a critical threat to deep learning systems, particularly in safety-sensitive 3D domains such as autonomous driving and robotics. However, existing backdoor attacks for 3D point clouds have been limited to a rigid one-to-one paradigm. To address this, we present the first one-to-N backdoor framework for 3D vision, based on a novel, configurable spherical trigger. Our key insight is to leverage the spatial properties of spheres as a parameter space, allowing a single trigger design to encode multiple target classes. We establish a theoretical foundation for one-to-N backdoor attacks in 3D, demonstrating that poisoned models can map distinct trigger configurations to different target labels. Experimental results systematically validate this conclusion across multiple datasets and model architectures, achieving high attack success rates (up to 100\%) while maintaining accuracy on clean data. This work establishes a crucial benchmark for multi-target threats in 3D vision and provides the foundational understanding needed to secure future 3D-driven intelligent systems.

</details>


### [51] [MAFM^3: Modular Adaptation of Foundation Models for Multi-Modal Medical AI](https://arxiv.org/abs/2511.11212)
*Mohammad Areeb Qazi,Munachiso S Nwadike,Ibrahim Almakky,Mohammad Yaqub,Numan Saeed*

Main category: cs.CV

TL;DR: MAFM^3是一个框架，它允许一个基础模型通过轻量级模块化组件扩展到不同的医学成像领域、任务和模态，而不是为每个领域、模态或任务构建单独的模型。


<details>
  <summary>Details</summary>
Motivation: 在医学成像领域，数据稀缺使得为每个领域、模态或任务进行预训练具有挑战性。MAFM^3旨在通过一个单一的基础模型来解决这个问题，并能通过模块化组件进行扩展。

Method: MAFM^3框架通过轻量级模块化组件实现基础模型的适应性扩展，这些组件作为专门的技能集，允许系统根据输入类型或临床目标在推理时灵活激活相应的功能。

Result: 将一个最初用于分类的胸部CT基础模型调整为预后和分割模块，在两个任务上的性能均得到提升。通过结合PET扫描，MAFM^3在Dice分数上比各自的基线提高了5%。

Conclusion: 基础模型通过模块化组件可以演变成多任务、多模态系统，用于医学成像，而不受限于其初始训练范围。

Abstract: Foundational models are trained on extensive datasets to capture the general trends of a domain. However, in medical imaging, the scarcity of data makes pre-training for every domain, modality, or task challenging. Instead of building separate models, we propose MAFM^3 (Modular Adaptation of Foundation Models for Multi-Modal Medical AI), a framework that enables a single foundation model to expand into diverse domains, tasks, and modalities through lightweight modular components. These components serve as specialized skill sets that allow the system to flexibly activate the appropriate capability at the inference time, depending on the input type or clinical objective. Unlike conventional adaptation methods that treat each new task or modality in isolation, MAFM^3 provides a unified and expandable framework for efficient multitask and multimodality adaptation. Empirically, we validate our approach by adapting a chest CT foundation model initially trained for classification into prognosis and segmentation modules. Our results show improved performance on both tasks. Furthermore, by incorporating PET scans, MAFM^3 achieved an improvement in the Dice score 5% compared to the respective baselines. These findings establish that foundation models, when equipped with modular components, are not inherently constrained to their initial training scope but can evolve into multitask, multimodality systems for medical imaging. The code implementation of this work can be found at https://github.com/Areeb2735/CTscan_prognosis_VLM

</details>


### [52] [RealisticDreamer: Guidance Score Distillation for Few-shot Gaussian Splatting](https://arxiv.org/abs/2511.11213)
*Ruocheng Wu,Haolan He,Yufei Wang,Zhihao Li,Bihan Wen*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）因其高质量的实时渲染能力在3D场景表示方面备受关注。然而，当输入包含稀疏训练视图时，3DGS由于缺乏中间视图监督而容易过拟合。


<details>
  <summary>Details</summary>
Motivation: 受到近期视频扩散模型（VDM）的成功启发，提出一种名为引导分数蒸馏（GSD）的框架，从预训练的VDM中提取丰富的多视图一致性先验，以解决3DGS中稀疏训练视图容易过拟合的问题。

Method: GSD框架利用预训练的VDM提取多视图一致性先验，并通过引导分数蒸馏（GSD）来监督多个相邻视图的渲染图像，从而指导高斯泼溅表示朝着VDM的生成方向发展。为了解决VDM生成方向带来的目标运动和随机相机轨迹问题，引入了一种统一的引导形式，包括基于真实深度图的深度扭曲引导和基于语义图像特征的引导，以确保VDM的分数更新方向与正确的相机姿态和精确的几何体对齐。

Result: 实验结果表明，该方法在多个数据集上优于现有方法。

Conclusion: GSD框架通过利用VDM的多视图一致性先验，并结合深度扭曲和语义特征引导，有效解决了3DGS在稀疏视图下的过拟合问题，并在多个数据集上取得了优于现有方法的性能。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained great attention in the 3D scene representation for its high-quality real-time rendering capabilities. However, when the input comprises sparse training views, 3DGS is prone to overfitting, primarily due to the lack of intermediate-view supervision. Inspired by the recent success of Video Diffusion Models (VDM), we propose a framework called Guidance Score Distillation (GSD) to extract the rich multi-view consistency priors from pretrained VDMs. Building on the insights from Score Distillation Sampling (SDS), GSD supervises rendered images from multiple neighboring views, guiding the Gaussian splatting representation towards the generative direction of VDM. However, the generative direction often involves object motion and random camera trajectories, making it challenging for direct supervision in the optimization process. To address this problem, we introduce an unified guidance form to correct the noise prediction result of VDM. Specifically, we incorporate both a depth warp guidance based on real depth maps and a guidance based on semantic image features, ensuring that the score update direction from VDM aligns with the correct camera pose and accurate geometry. Experimental results show that our method outperforms existing approaches across multiple datasets.

</details>


### [53] [Positional Bias in Multimodal Embedding Models: Do They Favor the Beginning, the Middle, or the End?](https://arxiv.org/abs/2511.11216)
*Kebin Wu,Fatima Albreiki*

Main category: cs.CV

TL;DR: Positional bias is prevalent in multimodal representation models for image-text retrieval, affecting text and image encoders differently, and is influenced by factors like positional encoding, training loss, and data pairing.


<details>
  <summary>Details</summary>
Motivation: Investigate positional bias in multimodal representation models, particularly for image-text retrieval, as it's underexplored compared to text generation models.

Method: Distinguish context importance from positional bias, assess bias presence and extent in different models and datasets, and analyze factors contributing to the bias.

Result: Positional bias is prevalent in multimodal models, with text encoders biased towards the beginning and image encoders biased at both beginning and end. Bias is amplified by positional encoding, training loss, context importance, and image-text pair usage.

Conclusion: Positional bias exists in multimodal models for image-text retrieval and is influenced by a combination of encoding schemes, training dynamics, and data characteristics.

Abstract: Positional bias - where models overemphasize certain positions regardless of content - has been shown to negatively impact model performance across various tasks. While recent research has extensively examined positional bias in text generation models, its presence and effects in representation models remain underexplored. Even less is known about such biases in multimodal models. In this work, we investigate positional bias in multimodal representation models, specifically in the context of image-text retrieval. We begin by distinguishing between context importance and positional bias, and then assess the presence and extent of positional bias across different models and datasets. Our experiments demonstrate that positional bias is prevalent in multimodal models, but manifests differently across modalities: text encoders tend to exhibit bias toward the beginning of the input, whereas image encoders show bias at both the beginning and end. Furthermore, we find that this bias arises from, or is amplified by, a combination of factors, including the positional encoding scheme, training loss, context importance, and the nature of using image-text pairs in multimodal training.

</details>


### [54] [3D Gaussian and Diffusion-Based Gaze Redirection](https://arxiv.org/abs/2511.11231)
*Abiram Panchalingam,Indu Bodala,Stuart Middleton*

Main category: cs.CV

TL;DR: DiT-Gaze使用Diffusion Transformer（DiT）、弱监督和正交约束损耗来提高3D注视重定向模型的真实感和准确性，在合成训练数据方面设定了新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 为了生成高保真度的增强数据，以提高注视估计器的泛化能力，需要高保真度的注视重定向。

Method: 提出了一种名为DiT-Gaze的框架，结合了Diffusion Transformer（DiT）、跨注视角度的弱监督以及正交约束损耗。DiT用于更高保真度的图像合成，弱监督策略使用合成生成的中间注视角度，以在训练期间提供平滑的注视方向流形，而正交约束损耗则在数学上强制解耦注视、头部姿势和表情的内部表示。

Result: DiT-Gaze在感知质量和重定向准确性方面均设定了新的最先进水平，将最先进的注视误差降低了4.1%，达到了6.353度。

Conclusion: DiT-Gaze通过其新颖的架构和训练策略，在生成用于改进注视估计器的合成训练数据方面，提供了优于现有方法的能力。

Abstract: High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.

</details>


### [55] [DoReMi: A Domain-Representation Mixture Framework for Generalizable 3D Understanding](https://arxiv.org/abs/2511.11232)
*Mingwei Xing,Xinliang Wang,Yifeng Shi*

Main category: cs.CV

TL;DR: 3D深度学习在多域上的泛化能力受限于现有数据集规模和点云异质性。本文提出DoReMi（Domain-Representation Mixture）框架，结合领域感知专家分支和统一表示分支，通过领域引导空间路由（DSR）和熵控制动态分配（EDA）实现协同学习，并在ScanNet Val和S3DIS等基准上取得了80.1%和77.2%的mIoU。


<details>
  <summary>Details</summary>
Motivation: 现有3D深度学习方法在多域融合时存在负迁移问题，且忽视了领域感知和领域通用特征的协同作用。

Method: 提出DoReMi（Domain-Representation Mixture）框架，这是一个混合专家（MoE）框架，包含领域感知专家分支和统一表示分支。通过领域引导空间路由（DSR）动态激活领域感知专家，通过熵控制动态分配（EDA）实现专家的高效利用。统一表示分支通过多属性自监督学习预训练并冻结，以保留跨域几何和结构先验。

Result: 在ScanNet Val上达到80.1%的mIoU，在S3DIS上达到77.2%的mIoU，性能优于或媲美现有方法。

Conclusion: DoReMi框架能够有效解决多源点云的异质性问题，实现领域感知和领域通用知识的协同学习，在3D理解任务中表现出强大的性能和作为未来研究基础框架的潜力。

Abstract: The generalization of 3D deep learning across multiple domains remains limited by the limited scale of existing datasets and the high heterogeneity of multi-source point clouds. Point clouds collected from different sensors (e.g., LiDAR scans and mesh-derived point clouds) exhibit substantial discrepancies in density and noise distribution, resulting in negative transfer during multi-domain fusion. Most existing approaches focus exclusively on either domain-aware or domain-general features, overlooking the potential synergy between them. To address this, we propose DoReMi (Domain-Representation Mixture), a Mixture-of-Experts (MoE) framework that jointly models Domain-aware Experts branch and a unified Representation branch to enable cooperative learning between specialized and generalizable knowledge. DoReMi dynamically activates domain-aware expert branch via Domain-Guided Spatial Routing (DSR) for context-aware expert selection and employs Entropy-Controlled Dynamic Allocation (EDA) for stable and efficient expert utilization, thereby adaptively modeling diverse domain distributions. Complemented by a frozen unified representation branch pretrained through robust multi-attribute self-supervised learning, DoReMi preserves cross-domain geometric and structural priors while maintaining global consistency. We evaluate DoReMi across multiple 3D understanding benchmarks. Notably, DoReMi achieves 80.1% mIoU on ScanNet Val and 77.2% mIoU on S3DIS, demonstrating competitive or superior performance compared to existing approaches, and showing strong potential as a foundation framework for future 3D understanding research. The code will be released soon.

</details>


### [56] [Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing](https://arxiv.org/abs/2511.11236)
*Cong Cao,Yujie Xu,Xiaodong Xu*

Main category: cs.CV

TL;DR: 提出了一种新颖的少样本图像编辑框架，使用MoE LoRA模型，能够有效地从少量配对数据中对通用图像编辑模型进行多风格微调，并取得了优于现有方法的性能，同时显著减少了参数量。


<details>
  <summary>Details</summary>
Motivation: 通用图像编辑模型在新风格下效果不佳，如何用少量配对数据有效微调通用模型到新风格是挑战。

Method: 提出了一种参数高效的多风格MoE LoRA，包含风格专有和风格共享路由机制；提出了一种度量引导方法来自动确定LoRA的秩；探索了LoRA在DiT模型中的最优插入位置；并结合了对抗学习和流匹配来指导扩散训练。

Result: 实验证明，该方法在参数量显著减少的情况下，性能优于现有的最先进方法。

Conclusion: 所提出的MoE LoRA框架能从少量配对数据中有效地对通用图像编辑模型进行多风格微调，性能优于现有方法。

Abstract: In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters.

</details>


### [57] [Beyond Flatlands: Unlocking Spatial Intelligence by Decoupling 3D Reasoning from Numerical Regression](https://arxiv.org/abs/2511.11239)
*Zhongbin Guo,Jiahe Liu,Yushan Li,Wenyu Gao,Zhen Yang,Chenzhi Li,Xinyue Zhang,Ping Jian*

Main category: cs.CV

TL;DR: 现有视觉语言模型 (VLMs) 在理解三维空间智能方面存在局限性，主要因为输入端几何感知编码器计算量大且仅提取二维特征，输出端离散分词器无法生成精确的连续数值。GEODE 模型通过解耦三维推理和数值生成来解决这两个瓶颈，引入了分离式推理模块 (DRM) 和直接回归头 (DRH)，在不显著增加参数量的情况下，实现了先进的空间推理能力，可媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理三维空间智能方面存在瓶颈，主要体现在输入端的计算成本和特征维度，以及输出端无法精确表示连续数值。

Method: 提出 GEODE 新架构，包含两个即插即用的模块：分离式推理模块 (DRM) 用于处理三维数据和二维视觉特征，提取空间推理链；直接回归头 (DRH) 用于进行精确的连续数值回归（如标量和三维边界框）。

Result: GEODE 模型在空间推理任务上取得了最先进的性能，表现优于参数量更大的模型。

Conclusion: GEODE 架构通过解耦输入和输出处理，有效解决了视觉语言模型在三维空间理解方面的局限性，并在保持较小模型规模的同时实现了高性能。

Abstract: Existing Vision Language Models (VLMs) architecturally rooted in "flatland" perception, fundamentally struggle to comprehend real-world 3D spatial intelligence. This failure stems from a dual-bottleneck: input-stage conflict between computationally exorbitant geometric-aware encoders and superficial 2D-only features, and output-stage misalignment where discrete tokenizers are structurally incapable of producing precise, continuous numerical values. To break this impasse, we introduce GEODE (Geometric-Output and Decoupled-Input Engine), a novel architecture that resolves this dual-bottleneck by decoupling 3D reasoning from numerical generation. GEODE augments main VLM with two specialized, plug-and-play modules: Decoupled Rationale Module (DRM) that acts as spatial co-processor, aligning explicit 3D data with 2D visual features via cross-attention and distilling spatial Chain-of-Thought (CoT) logic into injectable Rationale Tokens; and Direct Regression Head (DRH), an "Embedding-as-Value" paradigm which routes specialized control tokens to a lightweight MLP for precise, continuous regression of scalars and 3D bounding boxes. The synergy of these modules allows our 1.5B parameter model to function as a high-level semantic dispatcher, achieving state-of-the-art spatial reasoning performance that rivals 7B+ models.

</details>


### [58] [Toward Gaze Target Detection of Young Autistic Children](https://arxiv.org/abs/2511.11244)
*Shijian Deng,Erin E. Kosloski,Siva Sai Nagender Vasireddy,Jia Li,Randi Sierra Sherwood,Feroz Mohamed Hatha,Siddhi Patel,Pamela R Rollins,Yapeng Tian*

Main category: cs.CV

TL;DR: 提出了一种新的、现实世界的AI应用，用于检测自闭症儿童的注视目标，并介绍了首个自闭症注视目标（AGT）数据集。


<details>
  <summary>Details</summary>
Motivation: 开发一种对自闭症儿童有影响的自动注视目标检测AI应用，以提高他们的生活质量，该应用旨在解决测量联合注意力这一ASD核心挑战。

Method: 提出了一种新的社会意识粗到精（SACF）注视检测框架，该框架利用场景的社会背景来克服自闭症数据集中常见的类别不平衡问题。该框架采用双通路架构，包含专门用于社会和非社会注视的专家模型，并由上下文感知门控模块指导。

Result: 该框架在注视目标检测方面取得了新的最先进的性能，显著优于现有方法，尤其是在代表面部注视的关键少数类方面。

Conclusion: 所提出的SACF框架在自闭症儿童的注视目标检测方面取得了最先进的性能，并解决了由于自闭症儿童倾向于减少对人脸的注视而导致的数据集类别不平衡问题。

Abstract: The automatic detection of gaze targets in autistic children through artificial intelligence can be impactful, especially for those who lack access to a sufficient number of professionals to improve their quality of life. This paper introduces a new, real-world AI application for gaze target detection in autistic children, which predicts a child's point of gaze from an activity image. This task is foundational for building automated systems that can measure joint attention-a core challenge in Autism Spectrum Disorder (ASD). To facilitate the study of this challenging application, we collected the first-ever Autism Gaze Target (AGT) dataset. We further propose a novel Socially Aware Coarse-to-Fine (SACF) gaze detection framework that explicitly leverages the social context of a scene to overcome the class imbalance common in autism datasets-a consequence of autistic children's tendency to show reduced gaze to faces. It utilizes a two-pathway architecture with expert models specialized in social and non-social gaze, guided by a context-awareness gate module. The results of our comprehensive experiments demonstrate that our framework achieves new state-of-the-art performance for gaze target detection in this population, significantly outperforming existing methods, especially on the critical minority class of face-directed gaze.

</details>


### [59] [CountSteer: Steering Attention for Object Counting in Diffusion Models](https://arxiv.org/abs/2511.11253)
*Hyemin Boo,Hyoryung Kim,Myungjin Lee,Seunghyeon Lee,Jiyoung Lee,Jang-Hwan Choi,Hyunsoo Cho*

Main category: cs.CV

TL;DR: Text-to-image models struggle with numerical instructions, but implicitly understand counting accuracy. CountSteer, a training-free method, steers cross-attention hidden states during inference to improve object-count accuracy by ~4% without sacrificing visual quality.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image diffusion models often fail to accurately follow numerical instructions, creating a gap between language and visual representation. The goal is to improve the models' ability to generate images with specified object counts.

Method: CountSteer, a training-free approach, works by steering the model's cross-attention hidden states during inference. This leverages the model's implicit awareness of numerical correctness to guide the generation process more precisely.

Result: CountSteer improved object-count accuracy by approximately 4% in experiments, while maintaining the visual quality of the generated images.

Conclusion: The proposed CountSteer method offers a simple yet effective way to enhance the control and semantic reliability of text-to-image generation, specifically by improving the accurate generation of specified object counts.

Abstract: Text-to-image diffusion models generate realistic and coherent images but often fail to follow numerical instructions in text, revealing a gap between language and visual representation. Interestingly, we found that these models are not entirely blind to numbers-they are implicitly aware of their own counting accuracy, as their internal signals shift in consistent ways depending on whether the output meets the specified count. This observation suggests that the model already encodes a latent notion of numerical correctness, which can be harnessed to guide generation more precisely. Building on this intuition, we introduce CountSteer, a training-free method that improves generation of specified object counts by steering the model's cross-attention hidden states during inference. In our experiments, CountSteer improved object-count accuracy by about 4% without compromising visual quality, demonstrating a simple yet effective step toward more controllable and semantically reliable text-to-image generation.

</details>


### [60] [Discovering Meaningful Units with Visually Grounded Semantics from Image Captions](https://arxiv.org/abs/2511.11262)
*Melika Behjati,James Henderson*

Main category: cs.CV

TL;DR: 该模型通过在架构中对字幕标记进行分组来捕获语言的细粒度表示，以实现更好的视觉-语言理解。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型主要通过将图像块与语言标记对齐来获取细粒度知识，但图像块本身没有意义，单个标记也不一定包含可落地信息。文本中的词语分组才描述场景的不同方面。

Method: 提出一种在架构中对字幕标记进行分组的模型，使其表征与编码器输出的对象表征对齐，以捕获语言的细粒度表示。

Result: 通过学习对标记进行分组，视觉-语言模型能更好地细粒度地理解视觉和语言。此外，模型发现的标记分组在定性和定量上都与文本中的可落地短语高度相似。

Conclusion: 所提出的模型通过对字幕标记进行分组，能够实现更好的细粒度视觉-语言理解，并且发现的标记分组与文本中的可落地短语高度相似。

Abstract: Fine-grained knowledge is crucial for vision-language models to obtain a better understanding of the real world. While there has been work trying to acquire this kind of knowledge in the space of vision and language, it has mostly focused on aligning the image patches with the tokens on the language side. However, image patches do not have any meaning to the human eye, and individual tokens do not necessarily carry groundable information in the image. It is groups of tokens which describe different aspects of the scene. In this work, we propose a model which groups the caption tokens as part of its architecture in order to capture a fine-grained representation of the language. We expect our representations to be at the level of objects present in the image, and therefore align our representations with the output of an image encoder trained to discover objects. We show that by learning to group the tokens, the vision-language model has a better fine-grained understanding of vision and language. In addition, the token groups that our model discovers are highly similar to groundable phrases in text, both qualitatively and quantitatively.

</details>


### [61] [GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving](https://arxiv.org/abs/2511.11266)
*Fabian Schmidt,Markus Enzweiler,Abhinav Valada*

Main category: cs.CV

TL;DR: 现有的面向自动驾驶的视觉-语言模型在处理交通场景中的拓扑关系和动态交互方面存在局限，因为它们通常在缺乏明确关系依赖监督的情况下进行训练。本研究提出了一种新颖的模型无关方法，将基于语言的驾驶模型与交通场景图的结构化关系上下文相结合，以解决此问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在缺乏显式编码关系依赖性的监督情况下进行训练，限制了它们从原始传感器数据中推断交通实体之间相互影响的能力。

Method: 提出了一种新颖的模型无关方法，通过将交通场景图（以不同抽象级别和格式序列化）整合到模型中，并使用结构化提示模板进行，来增强基于语言的驾驶模型。

Result: 在公开的LangAuto基准测试中的广泛评估表明，场景图条件化可以显著提高最先进方法的驾驶性能，LMDrive的驾驶得分提高了15.6%，BEVDriver提高了17.5%。

Conclusion: 通过场景图条件化进行训练，模型可以更好地内化和理解关系先验，即使在测试时不需要场景图输入也能提高驾驶性能。

Abstract: Vision-language models have recently emerged as promising planners for autonomous driving, where success hinges on topology-aware reasoning over spatial structure and dynamic interactions from multimodal input. However, existing models are typically trained without supervision that explicitly encodes these relational dependencies, limiting their ability to infer how agents and other traffic entities influence one another from raw sensor data. In this work, we bridge this gap with a novel model-agnostic method that conditions language-based driving models on structured relational context in the form of traffic scene graphs. We serialize scene graphs at various abstraction levels and formats, and incorporate them into the models via structured prompt templates, enabling a systematic analysis of when and how relational supervision is most beneficial. Extensive evaluations on the public LangAuto benchmark show that scene graph conditioning of state-of-the-art approaches yields large and persistent improvement in driving performance. Notably, we observe up to a 15.6\% increase in driving score for LMDrive and 17.5\% for BEVDriver, indicating that models can better internalize and ground relational priors through scene graph-conditioned training, even without requiring scene graph input at test-time. Code, fine-tuned models, and our scene graph dataset are publicly available at https://github.com/iis-esslingen/GraphPilot.

</details>


### [62] [Φeat: Physically-Grounded Feature Representation](https://arxiv.org/abs/2511.11270)
*Giuseppe Vecchio,Adrien Kaiser,Rouffet Romain,Rosalie Martin,Elena Garces,Tamy Boubekeur*

Main category: cs.CV

TL;DR: Vision foundation models entangle semantics with physical factors like geometry and illumination. We introduce $Φ$eat, a physically-grounded visual backbone that learns material properties (reflectance, mesostructure) using self-supervision by contrasting spatial crops and physical augmentations of materials under varying conditions. This approach provides a strong prior for physics-aware perception tasks without explicit labels.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised vision models entangle high-level semantics with low-level physical factors, limiting their use in tasks requiring explicit physical reasoning. There is a need for a backbone that is sensitive to material identity, including reflectance and geometric mesostructure.

Method: We propose $Φ$eat, a physically-grounded visual backbone. It uses a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. This self-supervised training strategy does not use explicit labels.

Result: Feature similarity analysis and material selection show that $Φ$eat captures physically-grounded structure beyond semantic grouping. The learned representations are robust and invariant to external physical factors, providing a strong prior for downstream tasks.

Conclusion: Unsupervised physical feature learning, as demonstrated by $Φ$eat, holds significant promise for developing physics-aware perception systems in computer vision and graphics.

Abstract: Foundation models have emerged as effective backbones for many vision tasks. However, current self-supervised features entangle high-level semantics with low-level physical factors, such as geometry and illumination, hindering their use in tasks requiring explicit physical reasoning. In this paper, we introduce $Φ$eat, a novel physically-grounded visual backbone that encourages a representation sensitive to material identity, including reflectance cues and geometric mesostructure. Our key idea is to employ a pretraining strategy that contrasts spatial crops and physical augmentations of the same material under varying shapes and lighting conditions. While similar data have been used in high-end supervised tasks such as intrinsic decomposition or material estimation, we demonstrate that a pure self-supervised training strategy, without explicit labels, already provides a strong prior for tasks requiring robust features invariant to external physical factors. We evaluate the learned representations through feature similarity analysis and material selection, showing that $Φ$eat captures physically-grounded structure beyond semantic grouping. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics. These findings highlight the promise of unsupervised physical feature learning as a foundation for physics-aware perception in vision and graphics.

</details>


### [63] [Coordinative Learning with Ordinal and Relational Priors for Volumetric Medical Image Segmentation](https://arxiv.org/abs/2511.11276)
*Haoyi Wang*

Main category: cs.CV

TL;DR: 现有的体积医学图像分割方法在处理解剖结构和有限标注时存在挑战，它们主要依赖硬阈值区分正负样本，丢失了连续的解剖相似性信息，并且忽略了解剖结构在跨患者间的全局方向一致性，导致特征空间扭曲，无法捕捉跨患者共享的典型解剖流形。针对这些问题，提出了一种协调序关系解剖学习（CORAL）方法，用于捕捉体积图像的局部和全局结构。CORAL 使用对比排序目标来利用连续解剖相似性，确保切片间的关系特征距离与其解剖位置差异成正比。此外，CORAL 引入了序目标来强制全局方向一致性，使学习到的特征分布与跨患者的典型解剖过程对齐。通过协调学习框架，CORAL 在有限标注设置下，在基准数据集上取得了最先进的性能，同时学习到了具有有意义解剖结构的表示。代码可在 https://github.com/haoyiwang25/CORAL 获取。


<details>
  <summary>Details</summary>
Motivation: 现有的体积医学图像分割方法在处理解剖结构和有限标注时存在挑战，过度依赖硬阈值区分正负样本，丢失了连续的解剖相似性信息，并且忽略了解剖结构在跨患者间的全局方向一致性，导致特征空间扭曲，无法捕捉跨患者共享的典型解剖流形。

Method: 提出协调序关系解剖学习（CORAL）方法，通过对比排序目标利用连续解剖相似性，确保切片间的关系特征距离与其解剖位置差异成正比；同时引入序目标强制全局方向一致性，使学习到的特征分布与跨患者的典型解剖过程对齐。

Result: CORAL 在有限标注设置下，在基准数据集上取得了最先进的性能，同时学习到了具有有意义解剖结构的表示。

Conclusion: CORAL 通过协调学习框架，成功解决了现有方法在体积医学图像分割中的局限性，有效捕捉了局部和全局的解剖结构信息，并在有限标注条件下实现了优越的分割性能。

Abstract: Volumetric medical image segmentation presents unique challenges due to the inherent anatomical structure and limited availability of annotations. While recent methods have shown promise by contrasting spatial relationships between slices, they rely on hard binary thresholds to define positive and negative samples, thereby discarding valuable continuous information about anatomical similarity. Moreover, these methods overlook the global directional consistency of anatomical progression, resulting in distorted feature spaces that fail to capture the canonical anatomical manifold shared across patients. To address these limitations, we propose Coordinative Ordinal-Relational Anatomical Learning (CORAL) to capture both local and global structure in volumetric images. First, CORAL employs a contrastive ranking objective to leverage continuous anatomical similarity, ensuring relational feature distances between slices are proportional to their anatomical position differences. In addition, CORAL incorporates an ordinal objective to enforce global directional consistency, aligning the learned feature distribution with the canonical anatomical progression across patients. Learning these inter-slice relationships produces anatomically informed representations that benefit the downstream segmentation task. Through this coordinative learning framework, CORAL achieves state-of-the-art performance on benchmark datasets under limited-annotation settings while learning representations with meaningful anatomical structure. Code is available at https://github.com/haoyiwang25/CORAL.

</details>


### [64] [SimuFreeMark: A Noise-Simulation-Free Robust Watermarking Against Image Editing](https://arxiv.org/abs/2511.11295)
*Yichao Tang,Mingyang Li,Di Miao,Sheng Li,Zhenxing Qian,Xinpeng Zhang*

Main category: cs.CV

TL;DR: SimuFreeMark是一种无需噪声模拟的AIGC图像水印框架，利用图像低频分量的稳定性来嵌入水印，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习水印方法在训练时依赖手工制作的噪声模拟层，泛化能力受限，无法应对未知的失真。需要开发一种能抵抗常规信号处理和语义编辑攻击的新型水印方法。

Method: SimuFreeMark利用图像低频分量的固有稳定性，将水印直接嵌入到低频分量的深度特征空间中。它利用预训练的变分自编码器（VAE）将水印与图像的结构稳定性关联起来，从而完全消除了训练过程中的噪声模拟。

Result: SimuFreeMark在广泛的常规攻击和语义攻击下，其性能优于现有最先进的方法，同时保持了更高的视觉质量。

Conclusion: SimuFreeMark通过利用图像低频分量的鲁棒性，成功实现了一种无需噪声模拟即可进行水印嵌入的框架，在保持高质量的同时，有效抵抗了各种攻击。

Abstract: The advancement of artificial intelligence generated content (AIGC) has created a pressing need for robust image watermarking that can withstand both conventional signal processing and novel semantic editing attacks. Current deep learning-based methods rely on training with hand-crafted noise simulation layers, which inherently limit their generalization to unforeseen distortions. In this work, we propose $\textbf{SimuFreeMark}$, a noise-$\underline{\text{simu}}$lation-$\underline{\text{free}}$ water$\underline{\text{mark}}$ing framework that circumvents this limitation by exploiting the inherent stability of image low-frequency components. We first systematically establish that low-frequency components exhibit significant robustness against a wide range of attacks. Building on this foundation, SimuFreeMark embeds watermarks directly into the deep feature space of the low-frequency components, leveraging a pre-trained variational autoencoder (VAE) to bind the watermark with structurally stable image representations. This design completely eliminates the need for noise simulation during training. Extensive experiments demonstrate that SimuFreeMark outperforms state-of-the-art methods across a wide range of conventional and semantic attacks, while maintaining superior visual quality.

</details>


### [65] [AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models](https://arxiv.org/abs/2511.11299)
*Haokun Chen,Jianing Li,Yao Zhang,Jinhe Bi,Yan Xia,Jindong Gu,Volker Tresp*

Main category: cs.CV

TL;DR: MLLMs处理海量数据引起数据隐私担忧，本文提出AUVIC视觉概念遗忘框架，利用对抗性扰动精确遗忘目标视觉概念，并构建VCUBench进行评估，实验证明AUVIC在遗忘目标概念的同时，对非目标概念影响极小。


<details>
  <summary>Details</summary>
Motivation: 宏大模型在海量数据集上的优化带来了显著的数据隐私问题，同时‘被遗忘权’等法规要求实现机器遗忘，以避免昂贵的模型重新训练。

Method: 提出AUVIC框架，利用对抗性扰动实现精确的视觉概念遗忘，从而隔离目标概念，避免影响相关实体。

Result: AUVIC在目标概念遗忘率方面达到了最先进水平，并且对非目标概念的性能影响极小。此外，文章构建了首个用于评估视觉概念遗忘（尤其是在群体背景下）的基准VCUBench。

Conclusion: AUVIC在解决MLLMs中的视觉概念遗忘问题上是有效的，并且提出的VCUBench为后续研究提供了标准。

Abstract: Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.

</details>


### [66] [DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding](https://arxiv.org/abs/2511.11313)
*Tanveer Hannan,Dimitrios Mallios,Parth Pathak,Faegheh Sardari,Thomas Seidl,Gedas Bertasius,Mohsen Fayyaz,Sunando Sengupta*

Main category: cs.CV

TL;DR: DocSLM是一个高效的小型视觉-语言模型，用于在内存受限的边缘设备上进行长文档理解。它使用分层多模态压缩器来编码页面信息，并采用流式弃用机制处理长输入，从而在减少内存消耗的同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLVM在处理长文档时内存占用过高，不适用于资源受限的边缘设备。

Method: DocSLM采用分层多模态压缩器来编码视觉、文本和布局信息，并结合流式弃用机制和基于熵的不确定性校准器来处理长文档。

Result: DocSLM在多个长多模态文档基准测试中，在视觉标记、参数和延迟方面均优于现有方法，同时保持了相当的性能。

Conclusion: DocSLM能够为轻量级边缘设备提供可靠的多模态文档理解能力，显著降低了内存和计算需求。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated strong multimodal reasoning capabilities on long and complex documents. However, their high memory footprint makes them impractical for deployment on resource-constrained edge devices. We present DocSLM, an efficient Small Vision-Language Model designed for long-document understanding under constrained memory resources. DocSLM incorporates a Hierarchical Multimodal Compressor that jointly encodes visual, textual, and layout information from each page into a fixed-length sequence, greatly reducing memory consumption while preserving both local and global semantics. To enable scalable processing over arbitrarily long inputs, we introduce a Streaming Abstention mechanism that operates on document segments sequentially and filters low-confidence responses using an entropy-based uncertainty calibrator. Across multiple long multimodal document benchmarks, DocSLM matches or surpasses state-of-the-art methods while using 82\% fewer visual tokens, 75\% fewer parameters, and 71\% lower latency, delivering reliable multimodal document understanding on lightweight edge devices. Code is available in the supplementary material.

</details>


### [67] [YCB-Ev SD: Synthetic event-vision dataset for 6DoF object pose estimation](https://arxiv.org/abs/2511.11344)
*Pavel Rojtberg,Julius Kühn*

Main category: cs.CV

TL;DR: We introduce YCB-Ev SD, a synthetic dataset for event-camera 6DoF object pose estimation, featuring 50,000 PBR-rendered event sequences. We also evaluate event representations for CNNs, finding that time-surfaces with linear decay and dual-channel polarity perform best, with polarity being the most significant factor.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive synthetic resources for event-based vision, as compared to frame-based computer vision, hinders research in areas like 6DoF object pose estimation using event cameras.

Method: Synthesized 50,000 event sequences (34 ms duration) from PBR scenes of YCB-Video objects using the BOP methodology. Employed simulated linear camera motion for full scene coverage. Systematically evaluated event representations (time-surfaces with linear/exponential decay, dual-channel/single-channel polarity) for CNN-based inference.

Result: Demonstrated that time-surfaces with linear decay and dual-channel polarity encoding yield superior 6DoF object pose estimation performance. Polarity information was found to be the most crucial factor for performance gains, and linear temporal encoding preserved motion information more effectively than exponential decay.

Conclusion: YCB-Ev SD is a valuable synthetic dataset that addresses the need for event-based vision resources. The evaluation of event representations provides crucial insights into optimal strategies for CNN-based pose estimation using event data, highlighting the importance of polarity and linear temporal encoding.

Abstract: We introduce YCB-Ev SD, a synthetic dataset of event-camera data at standard definition (SD) resolution for 6DoF object pose estimation. While synthetic data has become fundamental in frame-based computer vision, event-based vision lacks comparable comprehensive resources. Addressing this gap, we present 50,000 event sequences of 34 ms duration each, synthesized from Physically Based Rendering (PBR) scenes of YCB-Video objects following the Benchmark for 6D Object Pose (BOP) methodology. Our generation framework employs simulated linear camera motion to ensure complete scene coverage, including background activity.
  Through systematic evaluation of event representations for CNN-based inference, we demonstrate that time-surfaces with linear decay and dual-channel polarity encoding achieve superior pose estimation performance, outperforming exponential decay and single-channel alternatives by significant margins. Our analysis reveals that polarity information contributes most substantially to performance gains, while linear temporal encoding preserves critical motion information more effectively than exponential decay. The dataset is provided in a structured format with both raw event streams and precomputed optimal representations to facilitate immediate research use and reproducible benchmarking.
  The dataset is publicly available at https://huggingface.co/datasets/paroj/ycbev_sd.

</details>


### [68] [Free3D: 3D Human Motion Emerges from Single-View 2D Supervision](https://arxiv.org/abs/2511.11368)
*Sheng Liu,Yuanzhi Liang,Sidan Du*

Main category: cs.CV

TL;DR: Free3D通过使用2D动作数据合成逼真的3D动作，无需3D动作注释，即可实现与3D监督模型相当甚至更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D人体运动生成模型在泛化能力上存在局限，主要是由于过度依赖精确的3D监督，导致模型倾向于拟合固定的坐标模式，而不是学习对鲁棒泛化至关重要的3D结构和运动语义线索。

Method: Free3D提出了一种名为ML-RQ（Motion-Lifting Residual Quantized VAE）的变压器模型，将2D运动序列映射到3D一致的潜在空间，并引入了一系列无3D监督的正则化目标（视图一致性、方向连贯性和物理合理性）来强制模型生成高质量的3D运动。

Result: 在仅使用2D动作数据进行训练的情况下，Free3D能够生成多样化、时间连贯且语义对齐的3D动作，其性能可以与完全依赖3D监督的方法相媲美，甚至在某些方面超越了它们。

Conclusion: 放宽对显式3D监督的依赖可以促使模型进行更强的结构推理和泛化，为3D运动生成提供了一种可扩展且数据高效的范例。

Abstract: Recent 3D human motion generation models demonstrate remarkable reconstruction accuracy yet struggle to generalize beyond training distributions. This limitation arises partly from the use of precise 3D supervision, which encourages models to fit fixed coordinate patterns instead of learning the essential 3D structure and motion semantic cues required for robust generalization.To overcome this limitation, we propose Free3D, a framework that synthesizes realistic 3D motions without any 3D motion annotations. Free3D introduces a Motion-Lifting Residual Quantized VAE (ML-RQ) that maps 2D motion sequences into 3D-consistent latent spaces, and a suite of 3D-free regularization objectives enforcing view consistency, orientation coherence, and physical plausibility. Trained entirely on 2D motion data, Free3D generates diverse, temporally coherent, and semantically aligned 3D motions, achieving performance comparable to or even surpassing fully 3D-supervised counterparts. These results suggest that relaxing explicit 3D supervision encourages stronger structural reasoning and generalization, offering a scalable and data-efficient paradigm for 3D motion generation.

</details>


### [69] [Disentangling Emotional Bases and Transient Fluctuations: A Low-Rank Sparse Decomposition Approach for Video Affective Analysis](https://arxiv.org/abs/2511.11406)
*Feng-Qi Cui,Jinyang Huang,Ziyu Jia,Xinyu Li,Xin Yan,Xiaokang Zhou,Meng Wang*

Main category: cs.CV

TL;DR: We propose the Low-Rank Sparse Emotion Understanding Framework (LSEF) to address model instability and representational degradation in Video-based Affective Computing (VAC). LSEF models affective dynamics as a hierarchical low-rank sparse compositional process using three modules: SEM for emotional bases, DDM for transient signals, and CIM for coherence. It


<details>
  <summary>Details</summary>
Motivation: The core limitation in Video-based Affective Computing (VAC) is the lack of a hierarchical structural mechanism to disentangle distinct affective components, leading to model instability and representational degradation due to complex emotional dynamics. Different emotional fluctuations have different meanings under different emotional contexts.

Method: The paper proposes the unified Low-Rank Sparse Emotion Understanding Framework (LSEF), grounded in the Low-Rank Sparse Principle. LSEF employs three plug-and-play modules: the Stability Encoding Module (SEM) to capture low-rank emotional bases, the Dynamic Decoupling Module (DDM) to isolate sparse transient signals, and the Consistency Integration Module (CIM) to reconstruct multi-scale stability and reactivity coherence. The framework is optimized by a Rank Aware Optimization (RAO) strategy.                    

Result: Extensive experiments across multiple datasets confirm that LSEF significantly enhances robustness and dynamic discrimination.

Conclusion: The effectiveness and generality of hierarchical low-rank sparse modeling for understanding affective dynamics are validated.

Abstract: Video-based Affective Computing (VAC), vital for emotion analysis and human-computer interaction, suffers from model instability and representational degradation due to complex emotional dynamics. Since the meaning of different emotional fluctuations may differ under different emotional contexts, the core limitation is the lack of a hierarchical structural mechanism to disentangle distinct affective components, i.e., emotional bases (the long-term emotional tone), and transient fluctuations (the short-term emotional fluctuations). To address this, we propose the Low-Rank Sparse Emotion Understanding Framework (LSEF), a unified model grounded in the Low-Rank Sparse Principle, which theoretically reframes affective dynamics as a hierarchical low-rank sparse compositional process. LSEF employs three plug-and-play modules, i.e., the Stability Encoding Module (SEM) captures low-rank emotional bases; the Dynamic Decoupling Module (DDM) isolates sparse transient signals; and the Consistency Integration Module (CIM) reconstructs multi-scale stability and reactivity coherence. This framework is optimized by a Rank Aware Optimization (RAO) strategy that adaptively balances gradient smoothness and sensitivity. Extensive experiments across multiple datasets confirm that LSEF significantly enhances robustness and dynamic discrimination, which further validates the effectiveness and generality of hierarchical low-rank sparse modeling for understanding affective dynamics.

</details>


### [70] [Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://arxiv.org/abs/2511.11410)
*Jiaxi Huang,Dongxu Wu,Hanwei Zhu,Lingyu Zhu,Jun Xing,Xu Wang,Baoliang Chen*

Main category: cs.CV

TL;DR: 该论文提出了Q-Doc框架，用于评估多模态大语言模型（MLLMs）在文档图像质量评估（DIQA）方面的能力。研究发现MLLMs在DIQA方面存在局限性，但通过思维链（CoT）提示可以显著提高其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在文档图像质量评估（DIQA）方面的潜力尚未被充分探索，需要一个系统性的评估框架来研究这一领域。

Method: 提出一个名为Q-Doc的三层评估框架，分别从粗粒度（评分）、中粒度（失真类型识别）和细粒度（失真严重性评估）三个层面来评估MLLMs的DIQA能力。

Result: 评估结果表明，MLLMs在DIQA方面具有初步能力，但在评分一致性、失真类型识别和严重性判断方面存在明显不足。然而，研究发现思维链（CoT）提示能够显著提升MLLMs在所有评估层面的表现。

Conclusion: Q-Doc框架为DIQA在MLLMs中的应用提供了一个基准，揭示了MLLMs在图像质量感知方面的显著缺陷，并指出了改进的方向。思维链提示被证明是提升MLLMs在这方面能力的关键方法。

Abstract: The rapid advancement of Multi-modal Large Language Models (MLLMs) has expanded their capabilities beyond high-level vision tasks. Nevertheless, their potential for Document Image Quality Assessment (DIQA) remains underexplored. To bridge this gap, we propose Q-Doc, a three-tiered evaluation framework for systematically probing DIQA capabilities of MLLMs at coarse, middle, and fine granularity levels. a) At the coarse level, we instruct MLLMs to assign quality scores to document images and analyze their correlation with Quality Annotations. b) At the middle level, we design distortion-type identification tasks, including single-choice and multi-choice tests for multi-distortion scenarios. c) At the fine level, we introduce distortion-severity assessment where MLLMs classify distortion intensity against human-annotated references. Our evaluation demonstrates that while MLLMs possess nascent DIQA abilities, they exhibit critical limitations: inconsistent scoring, distortion misidentification, and severity misjudgment. Significantly, we show that Chain-of-Thought (CoT) prompting substantially enhances performance across all levels. Our work provides a benchmark for DIQA capabilities in MLLMs, revealing pronounced deficiencies in their quality perception and promising pathways for enhancement. The benchmark and code are publicly available at:
  https://github.com/cydxf/Q-Doc.

</details>


### [71] [BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.11421)
*Lan Li,Tao Hu,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.CV

TL;DR: BOFA框架通过仅在CLIP的桥层进行正交低秩融合及跨模态混合原型来应对遗忘并提升在持续学习中的分类性能，无需额外参数或推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的持续学习方法在适应下游任务时常需额外模块，增加遗忘风险且未能充分利用跨模态表示的潜力。

Method: BOFA框架将模型适应限制在CLIP的桥层，利用正交低秩融合（Orthogonal Low-Rank Fusion）约束参数更新至与历史任务特征正交的安全子空间，并采用跨模态混合原型（结合稳定的文本原型和桥层提取的视觉原型）。

Result: BOFA在标准基准测试中实现了优于现有方法的准确性和效率。

Conclusion: BOFA是一种无需额外参数或推理成本的有效持续学习框架，通过桥层正交融合和跨模态混合原型成功解决了遗忘问题并提升了性能。

Abstract: Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.

</details>


### [72] [Shrinking the Teacher: An Adaptive Teaching Paradigm for Asymmetric EEG-Vision Alignment](https://arxiv.org/abs/2511.11422)
*Lukun Wu,Jie Li,Ziqi Ren,Kaifan Zhang,Xinbo Gao*

Main category: cs.CV

TL;DR: 视觉和脑部信号（EEG）之间的关系本质上是不对称的，存在保真度和语义两个关键鸿沟。现有方法忽视了这种不对称性，在对齐时将两者视为对等，导致泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了视觉和EEG信号之间关系的不对称性，导致泛化能力差。

Method: 提出了一种自适应教学范式，让“教师”模态（视觉）在任务的指导下动态地收缩和调整其知识结构，使其语义特征与“学生”模态（EEG）的能力相匹配。通过包含无残差设计和瓶颈结构的ShrinkAdapter模块来实现。

Result: 在零样本脑到图像检索任务上达到了60.2%的top-1准确率，比之前的最先进方法提高了9.8%。

Conclusion: 提出了一种新的非对称对齐视角：教师需要收缩和适应以弥合视觉-大脑的差距。

Abstract: Decoding visual features from EEG signals is a central challenge in neuroscience, with cross-modal alignment as the dominant approach. We argue that the relationship between visual and brain modalities is fundamentally asymmetric, characterized by two critical gaps: a Fidelity Gap (stemming from EEG's inherent noise and signal degradation, vs. vision's high-fidelity features) and a Semantic Gap (arising from EEG's shallow conceptual representation, vs. vision's rich semantic depth). Previous methods often overlook this asymmetry, forcing alignment between the two modalities as if they were equal partners and thereby leading to poor generalization. To address this, we propose the adaptive teaching paradigm. This paradigm empowers the ``teacher" modality (vision) to dynamically shrink and adjust its knowledge structure under task guidance, tailoring its semantically dense features to match the ``student" modality (EEG)'s capacity. We implement this paradigm with the ShrinkAdapter, a simple yet effective module featuring a residual-free design and a bottleneck structure. Through extensive experiments, we validate the underlying rationale and effectiveness of our paradigm. Our method achieves a top-1 accuracy of 60.2\% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods by a margin of 9.8\%. Our work introduces a new perspective for asymmetric alignment: the teacher must shrink and adapt to bridge the vision-brain gap.

</details>


### [73] [WEAVE: Unleashing and Benchmarking the In-context Interleaved Comprehension and Generation](https://arxiv.org/abs/2511.11434)
*Wei Chow,Jiachun Pan,Yongyuan Liang,Mingze Zhou,Xue Song,Liyu Jia,Saining Zhang,Siliang Tang,Juncheng Li,Fengda Zhang,Weijia Wu,Hanwang Zhang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: WEAVE是一个用于多模态理解和生成的新套件，包含WEAVE-100k数据集和WEAVEBench基准。它解决了现有数据集在单轮交互方面的不足，专注于需要上下文理解的多轮图像创建和编辑任务。 experimentos表明，WEAVE可以提高模型的视觉理解、图像编辑和跨模态协作能力，并促进模型发展视觉记忆能力，但同时揭示了当前模型在多轮、上下文感知图像生成和编辑方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和基准主要关注单轮交互，无法捕捉真实世界图像创建和编辑中多轮、上下文依赖的特性。

Method: 提出了WEAVE套件，包含WEAVE-100k（大规模数据集）和WEAVEBench（人类注释基准）。WEAVE-100k包含100K个交错样本，涉及370K个对话轮次和500K张图像，涵盖需要上下文推理的理解、编辑和生成任务。WEAVEBench包含100个基于480张图像的任务，采用混合VLM审判评估框架。

Result: 在WEAVE-100k上训练可以提升视觉理解、图像编辑和理解-生成协作能力。WEAVE有助于UMMs发展视觉记忆能力。WEAVEBench的评估揭示了当前方法在多轮、上下文感知图像生成和编辑方面的局限性。

Conclusion: WEAVE套件为多模态社区研究上下文交错的理解和生成提供了基础和视角。

Abstract: Recent advances in unified multimodal models (UMMs) have enabled impressive progress in visual comprehension and generation. However, existing datasets and benchmarks focus primarily on single-turn interactions, failing to capture the multi-turn, context-dependent nature of real-world image creation and editing. To address this gap, we present WEAVE, the first suite for in-context interleaved cross-modality comprehension and generation. Our suite consists of two complementary parts. WEAVE-100k is a large-scale dataset of 100K interleaved samples spanning over 370K dialogue turns and 500K images, covering comprehension, editing, and generation tasks that require reasoning over historical context. WEAVEBench is a human-annotated benchmark with 100 tasks based on 480 images, featuring a hybrid VLM judger evaluation framework based on both the reference image and the combination of the original image with editing instructions that assesses models' abilities in multi-turn generation, visual memory, and world-knowledge reasoning across diverse domains. Experiments demonstrate that training on WEAVE-100k enables vision comprehension, image editing, and comprehension-generation collaboration capabilities. Furthermore, it facilitates UMMs to develop emergent visual-memory capabilities, while extensive evaluations on WEAVEBench expose the persistent limitations and challenges of current approaches in multi-turn, context-aware image generation and editing. We believe WEAVE provides a view and foundation for studying in-context interleaved comprehension and generation for multi-modal community.

</details>


### [74] [The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models](https://arxiv.org/abs/2511.11435)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: 该研究探讨了文本到图像的扩散模型在处理“多模态图标性”时的泛化与记忆之间的模糊性，即图像和文本唤起文化共享联想的情况。研究引入了一个评估框架，区分了对文化参考的“识别”（模型是否能识别）和“实现”（模型如何通过复制或重新解释来描绘）。通过评估五个扩散模型在767个维基数据文化参考上的表现，该框架能比现有方法更有效地区分复制与转变。语言敏感性实验表明，即使在文本提示改变的情况下，模型也常常会复现标志性的视觉结构。研究还发现，文化对齐不仅与训练数据频率相关，还与文本独特性、参考流行度和创作日期相关。最终，研究强调扩散模型的价值在于其转化和重新语境化文化知识的能力，而不仅仅是复制，从而推动评估超越简单的文本-图像匹配，实现更丰富的语境理解。


<details>
  <summary>Details</summary>
Motivation: 该工作旨在解决文本到图像扩散模型在泛化与记忆之间的模糊性问题，特别关注“多模态图标性”现象，即图像与文本共同唤起文化共享联想的实例。与以往侧重遗忘的研究不同，本研究关注模型“记住什么”以及“如何记住”，重点在于区分对文化参考的识别能力和描绘能力。

Method: 引入了一个评估框架，将模型的评估分为“识别”（recognition）和“实现”（realization）两个维度。“识别”衡量模型是否能辨认出文化参考，“实现”衡量模型通过复制或重新解释来描绘该参考的能力。通过计算量化这两个维度。评估了五个扩散模型在767个维基数据支持的文化参考（包括静态和动态图像）上的表现。进行了提示扰动实验，使用同义词替换和字面图像描述来评估语言敏感性。

Result: 评估结果显示，该研究提出的框架比现有的基于相似度的方法更能有效地将复制与转变区分开来。模型在面对文本提示改变时，即使是同义词替换或字面描述，也常常会复现标志性的视觉结构。分析表明，文化对齐不仅与训练数据中的频率有关，还与文本的独特性、参考的流行度以及创作日期等因素相关。

Conclusion: 该研究的结论是，扩散模型的真正价值不仅在于它们能够复制什么，更在于它们如何转化和重新语境化文化知识。这项工作推动了对扩散模型的评估方法，使其超越了简单的文本-图像匹配，朝着更深层次的语境理解迈进。

Abstract: Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.

</details>


### [75] [Hi-DREAM: Brain Inspired Hierarchical Diffusion for fMRI Reconstruction via ROI Encoder and visuAl Mapping](https://arxiv.org/abs/2511.11437)
*Guowei Zhang,Yun Zhao,Moein Khajehnejad,Adeel Razi,Levin Kuhlmann*

Main category: cs.CV

TL;DR: Hi-DREAM是一个受大脑启发的条件扩散模型，通过显式表示皮层组织来解决现有fMRI解码器的局限性，实现了对自然图像的先进重建，并揭示了不同视觉区域的功能贡献。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的fMRI解码器在将大脑活动映射到自然图像时，未能充分考虑视觉信息在皮层内的组织方式，导致早期、中期和晚期视觉区域的角色模糊。Hi-DREAM旨在通过模拟大脑的层级处理来解决这一问题。

Method: Hi-DREAM框架包含一个感兴趣区域（ROI）适配器，它将fMRI数据分组为早期/中期/晚期处理流，并将其转换为多尺度皮层金字塔，该金字塔与U-Net的深度相匹配。一个轻量级的、深度匹配的ControlNet在去噪过程中注入这些特定尺度的提示，从而使模型能够显式地利用皮层组织。

Result: Hi-DREAM在Natural Scenes Dataset（NSD）上进行了实验，结果显示该模型在高级语义指标上达到了最先进的性能，同时保持了具有竞争力的低级保真度。这表明将条件化按皮层层级进行结构化是一种比纯粹数据驱动的嵌入更有效的方法。

Conclusion: 将视觉信号的条件化过程按照皮层的层级结构进行组织，是一种比纯粹基于数据驱动的嵌入更有效的方法，并且能够为研究视觉皮层的提供有用的视角。Hi-DREAM通过显式整合皮层组织，实现了高效、可解释的图像重建，并能阐明不同视觉区域的功能作用。

Abstract: Mapping human brain activity to natural images offers a new window into vision and cognition, yet current diffusion-based decoders face a core difficulty: most condition directly on fMRI features without analyzing how visual information is organized across the cortex. This overlooks the brain's hierarchical processing and blurs the roles of early, middle, and late visual areas. We propose Hi-DREAM, a brain-inspired conditional diffusion framework that makes the cortical organization explicit. A region-of-interest (ROI) adapter groups fMRI into early/mid/late streams and converts them into a multi-scale cortical pyramid aligned with the U-Net depth (shallow scales preserve layout and edges; deeper scales emphasize objects and semantics). A lightweight, depth-matched ControlNet injects these scale-specific hints during denoising. The result is an efficient and interpretable decoder in which each signal plays a brain-like role, allowing the model not only to reconstruct images but also to illuminate functional contributions of different visual areas. Experiments on the Natural Scenes Dataset (NSD) show that Hi-DREAM attains state-of-the-art performance on high-level semantic metrics while maintaining competitive low-level fidelity. These findings suggest that structuring conditioning by cortical hierarchy is a powerful alternative to purely data-driven embeddings and provides a useful lens for studying the visual cortex.

</details>


### [76] [VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models](https://arxiv.org/abs/2511.11438)
*Mingjie Xu,Jinpeng Chen,Yuzhi Zhao,Jason Chun Lok Li,Yue Qiu,Zekang Du,Mengyang Wu,Pingping Zhang,Kun Li,Hongzheng Yang,Wenao Ma,Jiaheng Wei,Qinbin Li,Kangcheng Liu,Wenqiang Lei*

Main category: cs.CV

TL;DR: 该研究提出了VP-Bench，一个用于评估多模态大语言模型（MLLMs）理解和利用视觉提示（如边界框）能力的基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能系统评估MLLMs理解视觉提示（VP）的能力，阻碍了对其在需要精确引用图像区域的任务中的应用。

Method: VP-Bench包含两个评估阶段：第一阶段评估模型感知30,000个不同形状和属性组合的VP的能力；第二阶段评估VP在现实世界问题解决任务中的实际效果。研究人员对28个MLLMs进行了评估，并分析了VP属性、问题设置和模型规模等因素对理解能力的影响。

Result: 通过VP-Bench的评估，研究揭示了不同MLLMs在理解和利用VP方面存在显著差异，并为未来模型改进提供了见解。

Conclusion: VP-Bench为研究MLLMs理解和解决基于视觉提示的引用问题提供了一个新的参考框架，有助于推动视觉-语言理解技术的发展。

Abstract: Multimodal large language models (MLLMs) have enabled a wide range of advanced vision-language applications, including fine-grained object recognition and contextual understanding. When querying specific regions or objects in an image, human users naturally use "visual prompts" (VPs), such as bounding boxes, to provide reference. However, no existing benchmark systematically evaluates the ability of MLLMs to interpret such VPs. This gap leaves it unclear whether current MLLMs can effectively recognize VPs, an intuitive prompting method for humans, and use them to solve problems. To address this limitation, we introduce VP-Bench, a benchmark for assessing MLLMs' capability in VP perception and utilization. VP-Bench employs a two-stage evaluation framework: Stage 1 examines models' ability to perceive VPs in natural scenes, using 30k visualized prompts spanning eight shapes and 355 attribute combinations. Stage 2 investigates the impact of VPs on downstream tasks, measuring their effectiveness in real-world problem-solving scenarios. Using VP-Bench, we evaluate 28 MLLMs, including proprietary systems (e.g., GPT-4o) and open-source models (e.g., InternVL3 and Qwen2.5-VL), and provide a comprehensive analysis of factors that affect VP understanding, such as variations in VP attributes, question arrangement, and model scale. VP-Bench establishes a new reference framework for studying how MLLMs comprehend and resolve grounded referring questions.

</details>


### [77] [From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs](https://arxiv.org/abs/2511.11440)
*Massimo Rizzoli,Simone Alghisi,Seyed Mahed Mousavi,Giuseppe Riccardi*

Main category: cs.CV

TL;DR: 通过平衡的合成数据微调视觉语言模型（VLMs）可以提高模型在真实世界数据上的表现，并减少常见偏差。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉语言模型（VLMs）微调依赖于真实世界数据的收集和标注，容易引入偏差、错误和分布不平衡，导致过拟合和性能不均衡。现有合成数据生成方法缺乏对分布偏差和标注质量的控制。

Method: 1. 设计了一种新的数据生成和标注方法，确保生成的数据没有偏差、分布不平衡和标注错误。通过全面采样物体属性（颜色、形状、大小、位置）来自动构建数据集。 2. 使用该标注数据集，对最先进的VLMs进行微调，并在绝对位置任务上评估其向真实世界数据的性能迁移能力。对合成和真实世界基准进行广泛评估。

Result: 1. 在平衡的合成数据上微调能够产生跨视觉场景的均匀性能，并减轻常见偏差。 2. 在合成数据上微调显著提高了在真实世界数据（COCO）上的性能，优于在匹配设置下微调的模型。

Conclusion: 平衡的合成数据是提高VLMs在真实世界场景中泛化能力和鲁棒性的有效途径。

Abstract: Fine-tuning Vision-Language Models (VLMs) is a common strategy to improve performance following an ad-hoc data collection and annotation of real-world scenes. However, this process is often prone to biases, errors, and distribution imbalance, resulting in overfitting and imbalanced performance. Although a few studies have tried to address this problem by generating synthetic data, they lacked control over distribution bias and annotation quality. To address these challenges, we redesign the fine-tuning process in two ways. First, we control the generation of data and its annotations, ensuring it is free from bias, distribution imbalance, and annotation errors. We automatically construct the dataset by comprehensively sampling objects' attributes, including color, shape, size, and position within the scene. Secondly, using this annotated dataset, we fine-tune state-of-the-art VLMs and assess performance transferability to real-world data on the absolute position task. We conduct exhaustive evaluations on both synthetic and real-world benchmarks. Our experiments reveal two key findings: 1) fine-tuning on balanced synthetic data yields uniform performance across the visual scene and mitigates common biases; and 2) fine-tuning on synthetic stimuli significantly improves performance on real-world data (COCO), outperforming models fine-tuned in the matched setting.

</details>


### [78] [VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation](https://arxiv.org/abs/2511.11450)
*Maximilian Rokuss,Moritz Langenberg,Yannick Kirchhoff,Fabian Isensee,Benjamin Hamm,Constantin Ulrich,Sebastian Regnery,Lukas Bauer,Efthimios Katsigiannopulos,Tobias Norajitra,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: VoxTell是一个用于医学图像分割的视觉-语言模型，可以通过文本提示生成3D掩码，并在多种模态的扫描数据上实现了最先进的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够理解自由格式文本描述并将其转换为3D医学图像分割掩码的视觉-语言模型。

Method: 通过多阶段的视觉-语言融合，在多个尺度上对齐文本和视觉特征，并在62K+的CT、MRI和PET图像数据上进行训练，涵盖1K+的解剖和病理类别。

Result: 在不同的模态和未见过的数据集上实现了最先进的零样本分割性能，对熟悉的类别表现优异，并能泛化到相关的未见类别，同时展示了强大的跨模态迁移能力、对语言变化的鲁棒性以及从真实世界文本中进行特定实例分割的准确性。

Conclusion: VoxTell能够有效地将自由形式的文本描述映射到3D医学图像分割掩码，并且在各种条件下都表现出强大的性能和泛化能力。

Abstract: We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell

</details>


### [79] [Rethinking Efficient Mixture-of-Experts for Remote Sensing Modality-Missing Classification](https://arxiv.org/abs/2511.11460)
*Qinghao Gao,Jianhai Qu,Yunsong Li,Weiqiang Dong*

Main category: cs.CV

TL;DR: 该研究提出了一种名为MaMOL的新框架，用于解决遥感多模态分类中数据缺失的问题，该框架通过双路由机制和参数高效的专家更新，提高了模型的鲁棒性、泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态分类方法在处理数据缺失时表现不佳，现有方法计算成本高且假设训练时数据完整，限制了其在真实世界中的应用。

Method: 提出了一种名为MaMOL（Missing-aware Mixture-of-Loras）的新框架，将模态缺失重新构建为多任务学习问题，并引入了任务导向的动态路由和模态特定共享的静态路由。该框架通过轻量级专家更新和共享专家复用来实现参数高效的适应。

Result: 在多个遥感数据集上进行了实验，结果表明MaMOL在不同缺失率下表现出优越的鲁棒性和泛化能力，同时计算开销最小。此外，在自然图像数据集上的迁移实验验证了其可扩展性和跨域适用性。

Conclusion: MaMOL是一个通用且高效的解决方案，适用于不完整的多模态学习场景，特别是在遥感领域，能够有效解决数据缺失导致的性能下降问题。

Abstract: Multimodal classification in remote sensing often suffers from missing modalities caused by environmental interference, sensor failures, or atmospheric effects, which severely degrade classification performance. Existing two-stage adaptation methods are computationally expensive and assume complete multimodal data during training, limiting their generalization to real-world incompleteness. To overcome these issues, we propose a Missing-aware Mixture-of-Loras (MaMOL) framework that reformulates modality missing as a multi-task learning problem. MaMOL introduces a dual-routing mechanism: a task-oriented dynamic router that adaptively activates experts for different missing patterns, and a modality-specific-shared static router that maintains stable cross-modal knowledge sharing. Unlike prior methods that train separate networks for each missing configuration, MaMOL achieves parameter-efficient adaptation via lightweight expert updates and shared expert reuse. Experiments on multiple remote sensing benchmarks demonstrate superior robustness and generalization under varying missing rates, with minimal computational overhead. Moreover, transfer experiments on natural image datasets validate its scalability and cross-domain applicability, highlighting MaMOL as a general and efficient solution for incomplete multimodal learning.

</details>


### [80] [ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation](https://arxiv.org/abs/2511.11483)
*Kaishen Wang,Ruibo Chen,Tong Zheng,Heng Huang*

Main category: cs.CV

TL;DR: ImAgent是一个无需训练的统一多模态智能体，通过策略控制器引导，多个生成动作动态交互和自我组织，以提高图像保真度和语义对齐度，并且在测试时可以高效扩展，无需额外的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）模型在生成逼真图像方面取得了很大进展，但在处理模糊或不明确的提示时，仍然存在随机性和不一致性问题。现有的解决方法（如提示重写、N选最佳采样和自我细化）虽然能缓解这些问题，但通常需要额外的模块并且独立运行，这会影响测试时间的扩展效率并增加计算开销。

Method: ImAgent通过一个策略控制器来指导多个生成动作动态交互和自我组织，从而在不依赖外部模型的情况下增强图像保真度和语义对齐度。它将推理、生成和自我评估集成在同一个框架内，以实现高效的测试时间扩展。

Result: 在图像生成和编辑任务上的大量实验表明，ImAgent在骨干模型的基础上持续改进，并且在骨干模型失败的情况下也能超越其他强大的基线模型。

Conclusion: ImAgent展示了统一多模态智能体在测试时间自适应和高效图像生成方面的潜力。

Abstract: Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.

</details>


### [81] [Multimodal Posterior Sampling-based Uncertainty in PD-L1 Segmentation from H&E Images](https://arxiv.org/abs/2511.11486)
*Roman Kinakh,Gonzalo R. Ríos-Muñoz,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: nnUNet-B是一个基于H&E染色切片图像的贝叶斯分割框架，通过多模态后验采样（MPS）直接推断PD-L1表达，无需IHC。该方法在肺鳞状细胞癌数据集上表现良好，并能提供像素级的模型不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 当前的免疫组织化学（IHC）方法在评估PD-L1表达时资源消耗大，需要更有效的方法来指导免疫治疗。

Method: 该方法基于nnUNet-v2，在周期性训练中采样不同的模型检查点来近似后验，从而实现准确分割和通过熵与标准差进行单点不确定性估计。

Result: 在肺鳞状细胞癌数据集上，nnUNet-B达到了0.805的平均Dice分数和0.709的平均IoU，性能与现有方法相当，并能生成像素级的不确定图。不确定性估计与分割误差相关，但目前校准仍不完美。

Conclusion: 基于H&E染色进行不确定性感知的PD-L1预测，是实现大规模、可解释的临床工作流程生物标志物评估的一个有前景的方向。

Abstract: Accurate assessment of PD-L1 expression is critical for guiding immunotherapy, yet current immunohistochemistry (IHC) based methods are resource-intensive. We present nnUNet-B: a Bayesian segmentation framework that infers PD-L1 expression directly from H&E-stained histology images using Multimodal Posterior Sampling (MPS). Built upon nnUNet-v2, our method samples diverse model checkpoints during cyclic training to approximate the posterior, enabling both accurate segmentation and epistemic uncertainty estimation via entropy and standard deviation. Evaluated on a dataset of lung squamous cell carcinoma, our approach achieves competitive performance against established baselines with mean Dice Score and mean IoU of 0.805 and 0.709, respectively, while providing pixel-wise uncertainty maps. Uncertainty estimates show strong correlation with segmentation error, though calibration remains imperfect. These results suggest that uncertainty-aware H&E-based PD-L1 prediction is a promising step toward scalable, interpretable biomarker assessment in clinical workflows.

</details>


### [82] [PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models](https://arxiv.org/abs/2511.11502)
*Nhat Hoang-Xuan,Minh Vu,My T. Thai,Manish Bhattarai*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLM）存在对象幻觉问题，通常是因为模型忽略图像而依赖先前生成的文本（prelim）。


<details>
  <summary>Details</summary>
Motivation: 识别并解决LVLM中的对象幻觉问题，这种问题源于模型对图像的依赖性减弱。

Method: 提出了一种名为“Prelim Attention Score”（PAS）的轻量级、无需训练的信号，该信号基于模型注意力权重计算，用于量化图像对模型推理的依赖程度。

Result: PAS能够实时检测对象幻觉，并且在多个模型和数据集上达到了最先进的检测效果。

Conclusion: PAS作为一种有效的、易于实现的幻觉检测方法，可以用于过滤和干预LVLM的输出，提高其可靠性。

Abstract: Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.

</details>


### [83] [Bridging Hidden States in Vision-Language Models](https://arxiv.org/abs/2511.11526)
*Benjamin Fein-Ashley,Jacob Fein-Ashley*

Main category: cs.CV

TL;DR: BRIDGE是一种新的视觉-语言模型（VLM），它通过在编码器顶部添加轻量级的跨注意力层来对齐图像和文本的隐藏状态，从而提高了在检索、视觉问答和视觉推理任务上的性能，同时保持了高效率。


<details>
  <summary>Details</summary>
Motivation: 直接对齐视觉和文本编码器的隐藏状态，因为这些状态包含了丰富的模态特定结构（视觉的空间布局；文本的句法和语义），这是一种匹配两种模态“思考”内容的自然方法。

Method: 提出了一种轻量级的融合模块，该模块由几个仅跨越的、双向的注意力层组成，位于两个编码器的顶部附近。每个层将视觉和文本编码器的隐藏状态序列投影到共享空间，跨模态地进行注意力计算，并将门控残差更新送回，并带有简单的稳定器以改善对齐。编码器保持非因果和强大的理解能力，而生成则通过可选的解码器干净地解耦。

Result: 在标准的检索、视觉问答（VQA）和视觉推理基准测试中，BRIDGE的性能优于同类VLM，同时保持了对比模型的双编码器效率。

Conclusion: BRIDGE通过在编码器顶部集成轻量级的跨注意力层，有效实现了视觉和语言隐藏状态的对齐，从而在多个视觉-语言任务中取得了最先进的性能，并保持了良好的效率。

Abstract: Vision-Language Models (VLMs) are a new family of models that align image content with natural language. Existing approaches typically fuse either (a) early: by mixing tokens/features inside the encoders, or (b) late: by comparing pooled embeddings. Many methods also tie fusion to an autoregressive decoder. However, the hidden states of both modalities already carry rich, modality-specific structure (spatial layout in vision; syntax and semantics in text), so directly aligning these states is a natural way to match what the two modalities "think". We propose a lightweight fusion module: a few cross-only, bidirectional attention layers placed near the top of both encoders. Each layer projects the vision and text encoder hidden-state sequences into a shared space, attends across modalities, and sends gated residual updates back, with simple stabilizers to improve alignment. The encoders remain non-causal and strong for understanding, while generation stays cleanly decoupled via an optional decoder. Across standard retrieval, VQA, and visual reasoning benchmarks, BRIDGE outperforms comparable VLMs while preserving the bi-encoder efficiency of contrastive models. We make our code publicly available at https://github.com/jfeinashley/BRIDGE.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [84] [Attentive Feature Aggregation or: How Policies Learn to Stop Worrying about Robustness and Attend to Task-Relevant Visual Cues](https://arxiv.org/abs/2511.10762)
*Nikolaos Tsagkas,Andreas Sochopoulos,Duolikun Danier,Sethu Vijayakumar,Alexandros Kouris,Oisin Mac Aodha,Chris Xiaoxuan Lu*

Main category: cs.RO

TL;DR: AFA通过一种轻量级、可训练的注意力池化机制，显著提高了在视觉扰动下的模仿学习策略的鲁棒性，而无需昂贵的再训练。


<details>
  <summary>Details</summary>
Motivation: 现有的模仿学习策略使用预训练视觉表示（PVRs）虽然强大，但容易受到与任务无关的场景信息影响，导致在受扰动场景中的鲁棒性差。

Method: 提出了一种名为Attentive Feature Aggregation（AFA）的轻量级、可训练的池化机制，该机制能够学习关注与任务相关的视觉线索，忽略无关的场景信息。

Result: 在模拟和现实世界的广泛实验表明，与标准的池化方法相比，AFA显著提高了在视觉扰动下的策略性能，并且不需要昂贵的数据集增强或PVR微调。

Conclusion: 忽略不相关的视觉信息是部署鲁棒且可泛化的模仿学习策略的关键一步。

Abstract: The adoption of pre-trained visual representations (PVRs), leveraging features from large-scale vision models, has become a popular paradigm for training visuomotor policies. However, these powerful representations can encode a broad range of task-irrelevant scene information, making the resulting trained policies vulnerable to out-of-domain visual changes and distractors. In this work we address visuomotor policy feature pooling as a solution to the observed lack of robustness in perturbed scenes. We achieve this via Attentive Feature Aggregation (AFA), a lightweight, trainable pooling mechanism that learns to naturally attend to task-relevant visual cues, ignoring even semantically rich scene distractors. Through extensive experiments in both simulation and the real world, we demonstrate that policies trained with AFA significantly outperform standard pooling approaches in the presence of visual perturbations, without requiring expensive dataset augmentation or fine-tuning of the PVR. Our findings show that ignoring extraneous visual information is a crucial step towards deploying robust and generalisable visuomotor policies. Project Page: tsagkas.github.io/afa

</details>


### [85] [From Framework to Reliable Practice: End-User Perspectives on Social Robots in Public Spaces](https://arxiv.org/abs/2511.10770)
*Samson Oruma,Ricardo Colomo-Palacios,Vasileios Gkioulos*

Main category: cs.RO

TL;DR: 在公共环境中部署的 ARI 社交机器人作为大学接待员，在安全、隐私、可用性和道德行为方面获得积极评价，但也面临可访问性和动态交互方面的挑战。该研究展示了如何在实际环境中实施安全和道德的社交机器人部署框架，并通过提供 GitHub 存储库来促进可再现性和降低入门门槛。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人在公共环境中的普及，用户的接受度不仅取决于技术可靠性，还取决于道德诚信、可访问性和用户信任。因此，有必要研究在实际环境中部署社交机器人并评估用户反馈。

Method: 在大学环境中部署 ARI 社交机器人作为接待员，并让 35 名学生和员工与机器人互动，然后提供关于安全、隐私、可用性、可访问性和透明度的结构化反馈。

Result: 大多数用户普遍认为机器人在物理安全、数据保护和道德行为方面表现良好，但也指出在可访问性、包容性和动态交互方面存在挑战。

Conclusion: 该研究表明，可以通过最终用户评估在实际环境中实施用于安全和道德部署的理论框架。此外，它还提供了一个包含可重用模板的 GitHub 存储库，以支持可再现性并降低新研究人员的门槛。通过结合用户观点和技术资源，该研究促进了关于人工智能和社会以及开发值得信赖、包容且符合道德的社交机器人的讨论。

Abstract: As social robots increasingly enter public environments, their acceptance depends not only on technical reliability but also on ethical integrity, accessibility, and user trust. This paper reports on a pilot deployment of an ARI social robot functioning as a university receptionist, designed in alignment with the SecuRoPS framework for secure and ethical social robot deployment. Thirty-five students and staff interacted with the robot and provided structured feedback on safety, privacy, usability, accessibility, and transparency. The results show generally positive perceptions of physical safety, data protection, and ethical behavior, while also highlighting challenges related to accessibility, inclusiveness, and dynamic interaction. Beyond the empirical findings, the study demonstrates how theoretical frameworks for ethical and secure design can be implemented in real-world contexts through end-user evaluation. It also provides a public GitHub repository containing reusable templates for ARI robot applications to support reproducibility and lower the entry barrier for new researchers. By combining user perspectives with practical technical resources, this work contributes to ongoing discussions in AI and society and supports the development of trustworthy, inclusive, and ethically responsible social robots for public spaces.

</details>


### [86] [$\rm{A}^{\rm{SAR}}$: $\varepsilon$-Optimal Graph Search for Minimum Expected-Detection-Time Paths with Path Budget Constraints for Search and Rescue](https://arxiv.org/abs/2511.10792)
*Eric Mugford,Jonathan D. Gammell*

Main category: cs.RO

TL;DR: A^SAR算法能在有限时间内找到搜索与救援规划的接近最优解，并在模拟和实际应用中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在搜索与救援（SAR）任务中，尤其是在海上SAR等场景下，由于生存时间短，需要最优搜索策略来提高成功率。然而，搜索规划问题具有高度不确定性，传统的随机优化方法虽然能处理大规模问题，但无法保证在有限时间内找到高质量的解。

Method: 提出了一种名为A^SAR的ε-最优搜索算法。该算法通过计算启发式函数来界定搜索空间，并结合图搜索方法，确保找到的解在用户指定的误差范围内（ε）接近最优解。

Result: A^SAR算法在模拟操作中找到了比现有优化方法更好、更快的解决方案。此外，该算法还在加拿大安大略湖的一次真实野外试验中成功部署，仅用150秒就定位了一个漂流的假人。

Conclusion: A^SAR算法为SAR规划提供了一种新的方法，能够提供具有严格最优性保证的解决方案，并在实际应用中取得了显著成效。

Abstract: Searches are conducted to find missing persons and/or objects given uncertain information, imperfect observers and large search areas in Search and Rescue (SAR). In many scenarios, such as Maritime SAR, expected survival times are short and optimal search could increase the likelihood of success. This optimization problem is complex for nontrivial problems given its probabilistic nature.
  Stochastic optimization methods search large problems by nondeterministically sampling the space to reduce the effective size of the problem. This has been used in SAR planning to search otherwise intractably large problems but the stochastic nature provides no formal guarantees on the quality of solutions found in finite time.
  This paper instead presents $\rm{A}^{\rm{SAR}}$, an $\varepsilon$-optimal search algorithm for SAR planning. It calculates a heuristic to bound the search space and uses graph-search methods to find solutions that are formally guaranteed to be within a user-specified factor, $\varepsilon$, of the optimal solution. It finds better solutions faster than existing optimization approaches in operational simulations. It is also demonstrated with a real-world field trial on Lake Ontario, Canada, where it was used to locate a drifting manikin in only 150s.

</details>


### [87] [An Investigation into Dynamically Extensible and Retractable Robotic Leg Linkages for Multi-task Execution in Search and Rescue Scenarios](https://arxiv.org/abs/2511.10816)
*William Harris,Lucas Yager,Syler Sylvester,Elizabeth Peiros,Micheal C. Yip*

Main category: cs.RO

TL;DR: 该研究提出了一种新型的动态伸缩机器人腿部概念，能够通过几何变换在有利高度和有利力两种模式间切换，以满足搜救（SAR）机器人在崎岖地形的快速通行和高强度救援任务需求。


<details>
  <summary>Details</summary>
Motivation: 搜救机器人需要快速穿越地形并执行高强度救援任务，这要求机器人既要有良好的地形适应能力，也要能输出可控的高强度力量。现有平台难以同时满足这两个要求。

Method: 该研究设计了一种动态可伸缩的五杆联动机构，使其能够通过几何变换在利于高度和利于力量的构型之间切换。通过搭建实验平台，在不同的连杆几何和工作模式下评估了腿部性能，并对步幅、力输出和稳定性进行了经验和分析评估。

Result: 实验结果表明，该变形腿在不同构型和工作模式下的表现均符合预期，能够实现利于高度和利于力量的转换，并在步幅、力输出和稳定性方面展现出良好的性能。

Conclusion: 研究表明，这种变形腿为搜救机器人提供了一种有前景的解决方案，使其能够快速导航复杂地形并有效执行救援任务。

Abstract: Search and rescue (SAR) robots are required to quickly traverse terrain and perform high-force rescue tasks, necessitating both terrain adaptability and controlled high-force output. Few platforms exist today for SAR, and fewer still have the ability to cover both tasks of terrain adaptability and high-force output when performing extraction. While legged robots offer significant ability to traverse uneven terrain, they typically are unable to incorporate mechanisms that provide variable high-force outputs, unlike traditional wheel-based drive trains. This work introduces a novel concept for a dynamically extensible and retractable robot leg. Leveraging a dynamically extensible and retractable five-bar linkage design, it allows for mechanically switching between height-advantaged and force-advantaged configurations via a geometric transformation. A testbed evaluated leg performance across linkage geometries and operating modes, with empirical and analytical analyses conducted on stride length, force output, and stability. The results demonstrate that the morphing leg offers a promising path toward SAR robots that can both navigate terrain quickly and perform rescue tasks effectively.

</details>


### [88] [MIGHTY: Hermite Spline-based Efficient Trajectory Planning](https://arxiv.org/abs/2511.10822)
*Kota Kondo,Yuwei Wu,Vijay Kumar,Jonathan P. How*

Main category: cs.RO

TL;DR: MIGHTY是一个基于Hermite样条的轨迹规划器，通过联合优化时空约束，克服了现有方法的局限性，并在仿真和实际硬件测试中取得了更好的计算效率和轨迹性能。


<details>
  <summary>Details</summary>
Motivation: 旨在克服现有硬约束轨迹规划器对计算资源的重度依赖以及软约束方法在时空解耦或搜索空间受限问题，提出一种更优的轨迹规划方法。

Method: 提出一种基于Hermite样条的规划器MIGHTY，该方法能联合优化时空约束，并充分利用样条的连续搜索空间进行探测。

Result: 在仿真环境中，MIGHTY相比现有方法，计算时间减少了9.3%，旅行时间减少了13.1%，成功率达到100%。在实际硬件测试中，MIGHTY成功完成了多趟在静态障碍物环境中高达6.7米/秒的高速飞行，以及在动态添加障碍物环境中的长时间飞行。

Conclusion: MIGHTY通过联合优化时空约束和利用样条的连续搜索空间，在计算效率和轨迹性能上均优于现有方法，并在真实场景中验证了其有效性。

Abstract: Hard-constraint trajectory planners often rely on commercial solvers and demand substantial computational resources. Existing soft-constraint methods achieve faster computation, but either (1) decouple spatial and temporal optimization or (2) restrict the search space. To overcome these limitations, we introduce MIGHTY, a Hermite spline-based planner that performs spatiotemporal optimization while fully leveraging the continuous search space of a spline. In simulation, MIGHTY achieves a 9.3% reduction in computation time and a 13.1% reduction in travel time over state-of-the-art baselines, with a 100% success rate. In hardware, MIGHTY completes multiple high-speed flights up to 6.7 m/s in a cluttered static environment and long-duration flights with dynamically added obstacles.

</details>


### [89] [Decentralized Swarm Control via SO(3) Embeddings for 3D Trajectories](https://arxiv.org/abs/2511.10858)
*Dimitria Silveria,Kleber Cabral,Peter Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: 该论文提出了一种新颖的去中心化方法，通过在基于李群的几何嵌入周围进行系统稳定，在信息共享最少的情况下实现多智能体系统的涌现行为，能够产生比现有基于四元数的方法更广泛的稳定、周期性轨迹。


<details>
  <summary>Details</summary>
Motivation: 提出了一种新颖的去中心化方法，在多智能体系统中实现具有最小信息共享的涌现行为，并能产生比现有方法更广泛的稳定、周期性轨迹。

Method: 基于李群SO(3)的几何嵌入，生成周期性曲线，并利用SO(3)特性消除对速度输入的需求，同时提出一种保证均匀粒子分离的相位控制器。

Result: 该方法能够生成比现有基于四元数的方法更广泛的周期性曲线，并且只需要位置输入，通过仿真和实验验证了其在复杂低层动力学和干扰下的适应性。

Conclusion: 该方法能够在信息共享最少的情况下，通过基于李群SO(3)的几何嵌入实现多智能体系统的涌现行为，并保证稳定性和均匀的粒子分离。

Abstract: This paper presents a novel decentralized approach for achieving emergent behavior in multi-agent systems with minimal information sharing. Based on prior work in simple orbits, our method produces a broad class of stable, periodic trajectories by stabilizing the system around a Lie group-based geometric embedding. Employing the Lie group SO(3), we generate a wider range of periodic curves than existing quaternion-based methods. Furthermore, we exploit SO(3) properties to eliminate the need for velocity inputs, allowing agents to receive only position inputs. We also propose a novel phase controller that ensures uniform agent separation, along with a formal stability proof. Validation through simulations and experiments showcases the method's adaptability to complex low-level dynamics and disturbances.

</details>


### [90] [WetExplorer: Automating Wetland Greenhouse-Gas Surveys with an Autonomous Mobile Robot](https://arxiv.org/abs/2511.10864)
*Jose Vasquez,Xuping Zhang*

Main category: cs.RO

TL;DR: WetExplorer是一个自动化机器人，可以对湿地中的温室气体进行采样，解决了手动采样耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 量化湿地温室气体对于气候建模和恢复评估至关重要，但手动采样方法效率低下。

Method: 开发了一个名为WetExplorer的自主履带机器人，集成了低地面压力移动、精确升降定位、双RTK传感器融合、避障规划和深度学习感知等功能，运行在ROS2框架下。

Result: 室外试验表明，传感器融合在定位精度上误差为1.71厘米，视觉模块的姿态估计精度分别为7毫米和3度。室内试验显示，运动规划系统能在避开障碍物的同时，将采样腔定位精度控制在70毫米以内，整个过程无需人工干预。

Conclusion: WetExplorer通过自动化温室气体采样流程，解决了手动采样瓶颈，能够实现高频率、多点位的温室气体测量，为饱和湿地环境下的长期、高密度数据集采集提供了可能。

Abstract: Quantifying greenhouse-gases (GHG) in wetlands is critical for climate modeling and restoration assessment, yet manual sampling is labor-intensive, and time demanding. We present WetExplorer, an autonomous tracked robot that automates the full GHG-sampling workflow. The robot system integrates low-ground-pressure locomotion, centimeter-accurate lift placement, dual-RTK sensor fusion, obstacle avoidance planning, and deep-learning perception in a containerized ROS2 stack. Outdoor trials verified that the sensor-fusion stack maintains a mean localization error of 1.71 cm, the vision module estimates object pose with 7 mm translational and 3° rotational accuracy, while indoor trials demonstrated that the full motion-planning pipeline positions the sampling chamber within a global tolerance of 70 mm while avoiding obstacles, all without human intervention. By eliminating the manual bottleneck, WetExplorer enables high-frequency, multi-site GHG measurements and opens the door for dense, long-duration datasets in saturated wetland terrain.

</details>


### [91] [Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation](https://arxiv.org/abs/2511.10874)
*Yorai Shaoul,Zhe Chen,Mohamed Naveed Gul Mohamed,Federico Pecora,Maxim Likhachev,Jiaoyang Li*

Main category: cs.RO

TL;DR: 本研究提出了一个统一的框架，用于协调多机器人、多物体进行非抓取式操作，解决了在复杂环境中进行大规模协作操作的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理多样化的物体和长时程任务时存在困难，要么依赖完全学习，要么依赖特权信息和手工设计的规划器。

Method: 本研究整合了流匹配协同生成和匿名多机器人运动规划。生成模型从视觉观测中协同生成接触形式和操作轨迹，同时一种新颖的运动规划器能够大规模地调度机器人，并支持物体级别的协调。

Result: 实验表明，该方法在运动规划和操作任务方面均优于基线方法，证明了生成式协同设计和集成规划在扩展协作操作至复杂的多主体、多物体场景中的优势。

Conclusion: 所提出的统一框架通过生成式协同设计和集成规划，有效地解决了多机器人、多物体非抓取式操作的挑战，并在复杂的模拟环境中取得了优于现有方法的性能。

Abstract: Coordinating a team of robots to reposition multiple objects in cluttered environments requires reasoning jointly about where robots should establish contact, how to manipulate objects once contact is made, and how to navigate safely and efficiently at scale. Prior approaches typically fall into two extremes -- either learning the entire task or relying on privileged information and hand-designed planners -- both of which struggle to handle diverse objects in long-horizon tasks. To address these challenges, we present a unified framework for collaborative multi-robot, multi-object non-prehensile manipulation that integrates flow-matching co-generation with anonymous multi-robot motion planning. Within this framework, a generative model co-generates contact formations and manipulation trajectories from visual observations, while a novel motion planner conveys robots at scale. Crucially, the same planner also supports coordination at the object level, assigning manipulated objects to larger target structures and thereby unifying robot- and object-level reasoning within a single algorithmic framework. Experiments in challenging simulated environments demonstrate that our approach outperforms baselines in both motion planning and manipulation tasks, highlighting the benefits of generative co-design and integrated planning for scaling collaborative manipulation to complex multi-agent, multi-object settings. Visit gco-paper.github.io for code and demonstrations.

</details>


### [92] [Terradynamics and design of tip-extending robotic anchors](https://arxiv.org/abs/2511.10901)
*Deniz Kerimoglu,Nicholas D. Naclerio,Sean Chu,Andrew Krohn,Vineet Kupunaram,Alexander Schepelmann,Daniel I. Goldman,Elliot W. Hawkes*

Main category: cs.RO

TL;DR: 通过模仿植物根系生长机制，研究人员开发了一种轻质、软体机器人锚，该锚能以小于其自身重量的力插入土壤，但提取力却远大于插入力，适用于难以到达或外星环境。


<details>
  <summary>Details</summary>
Motivation: 现有工程桩的驱动和提取力学不适用于难以到达或外星环境；植物根系的插入机制提供了新的思路。

Method: 1. 比较尖端延伸锚和传统桩式侵入体的土动力学特性。2. 提出增强锚定提取力与插入力比的设计原则。3. 基于这些原则设计并制造了一个轻质、软体、受根系启发的机器人锚。4. 在模拟火星土壤中测试该机器人的插入和锚定性能。

Result: 1. 确定了提高锚定提取力与插入力比的四个关键设计因素：超过临界深度、包含毛发状突起、近乎垂直延伸以及使用多个小型锚而非单个大型锚。 2. 开发了一个300克的软体机器人锚，其插入力小于自身重量，在火星土壤模拟物中能锚定120牛的力，锚定重量比达到40:1，并能部署45厘米深的温度传感器。

Conclusion: 模仿植物根系的尖端延伸机制可以开发出高效、轻便的机器人锚，尤其适用于地外探索等特殊环境。

Abstract: Most engineered pilings require substantially more force to be driven into the ground than they can resist during extraction. This requires relatively heavy equipment for insertion, which is problematic for anchoring in hard-to-access sites, including in extraterrestrial locations. In contrast, for tree roots, the external reaction force required to extract is much greater than required to insert--little more than the weight of the seed initiates insertion. This is partly due to the mechanism by which roots insert into the ground: tip extension. Proof-of-concept robotic prototypes have shown the benefits of using this mechanism, but a rigorous understanding of the underlying granular mechanics and how they inform the design of a robotic anchor is lacking. Here, we study the terradynamics of tip-extending anchors compared to traditional piling-like intruders, develop a set of design insights, and apply these to create a deployable robotic anchor. Specifically, we identify that to increase an anchor's ratio of extraction force to insertion force, it should: (i) extend beyond a critical depth; (ii) include hair-like protrusions; (iii) extend near-vertically, and (iv) incorporate multiple smaller anchors rather than a single large anchor. Synthesizing these insights, we developed a lightweight, soft robotic, root-inspired anchoring device that inserts into the ground with a reaction force less than its weight. We demonstrate that the 300 g device can deploy a series of temperature sensors 45 cm deep into loose Martian regolith simulant while anchoring with an average of 120 N, resulting in an anchoring-to-weight ratio of 40:1.

</details>


### [93] [Dexterous Manipulation Transfer via Progressive Kinematic-Dynamic Alignment](https://arxiv.org/abs/2511.10987)
*Wenbin Bai,Qiyu Chen,Xiangbo Lin,Jianwen Li,Quancheng Li,Hejiang Pan,Yi Sun*

Main category: cs.RO

TL;DR: 该研究提出了一个无需大量训练数据的手无关操纵迁移系统，可将人类手部操纵序列从演示视频转换为高质量的灵巧操纵轨迹，解决了机器人灵巧操纵数据稀疏的问题，平均迁移成功率为73%。


<details>
  <summary>Details</summary>
Motivation: 由于使用多指机器人手硬件平台收集操纵数据存在固有的困难和有限的可扩展性，导致数据严重稀缺，阻碍了数据驱动的灵巧操纵策略学习研究。

Method: 该系统设计了一个渐进式迁移框架：首先，基于运动学匹配建立灵巧手的基本控制信号；然后，训练具有动作空间重缩放和拇指引导初始化的残差策略，在统一奖励下动态优化接触交互；最后，计算腕部控制轨迹，以保留操作语义。

Result: 该系统可以自动生成流畅且语义正确的灵巧手操纵，忠实地再现人类意图，实现了高效率和强大的泛化能力，平均迁移成功率为73%。

Conclusion: 该框架提供了一种易于实现且可扩展的机器人灵巧操纵数据收集方法，能够利用人类手部操纵视频自动生成高质量的灵巧手操纵轨迹。

Abstract: The inherent difficulty and limited scalability of collecting manipulation data using multi-fingered robot hand hardware platforms have resulted in severe data scarcity, impeding research on data-driven dexterous manipulation policy learning. To address this challenge, we present a hand-agnostic manipulation transfer system. It efficiently converts human hand manipulation sequences from demonstration videos into high-quality dexterous manipulation trajectories without requirements of massive training data. To tackle the multi-dimensional disparities between human hands and dexterous hands, as well as the challenges posed by high-degree-of-freedom coordinated control of dexterous hands, we design a progressive transfer framework: first, we establish primary control signals for dexterous hands based on kinematic matching; subsequently, we train residual policies with action space rescaling and thumb-guided initialization to dynamically optimize contact interactions under unified rewards; finally, we compute wrist control trajectories with the objective of preserving operational semantics. Using only human hand manipulation videos, our system automatically configures system parameters for different tasks, balancing kinematic matching and dynamic optimization across dexterous hands, object categories, and tasks. Extensive experimental results demonstrate that our framework can automatically generate smooth and semantically correct dexterous hand manipulation that faithfully reproduces human intentions, achieving high efficiency and strong generalizability with an average transfer success rate of 73%, providing an easily implementable and scalable method for collecting robot dexterous manipulation data.

</details>


### [94] [Dynamic Reconfiguration of Robotic Swarms: Coordination and Control for Precise Shape Formation](https://arxiv.org/abs/2511.10989)
*Prab Prasertying,Paulo Garcia,Warisa Sritriratanarak*

Main category: cs.RO

TL;DR: 该算法实现了机器人集群的运动与构型协调，解决了机器人路径规划的难题。


<details>
  <summary>Details</summary>
Motivation: 协调机器人集群的运动与构型是一个挑战性问题，尤其是在物理系统存在误差和控制动态的限制下，如何为每个机器人确定最优路径仍然是一个待解决的问题。

Method: 利用几何方法，实现机器人集群从一个构型到另一个构型的无缝转换，并通过相应的控制、定位和映射技术将其映射到物理域。

Result: 实现了机器人集群从一个构型到另一个构型的无缝转换，并为机器人集群提供了更复杂的分布式行为能力。

Conclusion: 该算法能够实现机器人集群的运动与构型协调，为机器人集群的新型应用奠定了基础。

Abstract: Coordination of movement and configuration in robotic swarms is a challenging endeavor. Deciding when and where each individual robot must move is a computationally complex problem. The challenge is further exacerbated by difficulties inherent to physical systems, such as measurement error and control dynamics. Thus, how to best determine the optimal path for each robot, when moving from one configuration to another, and how to best perform such determination and effect corresponding motion remains an open problem. In this paper, we show an algorithm for such coordination of robotic swarms. Our methods allow seamless transition from one configuration to another, leveraging geometric formulations that are mapped to the physical domain through appropriate control, localization, and mapping techniques. This paves the way for novel applications of robotic swarms by enabling more sophisticated distributed behaviors.

</details>


### [95] [Latent-Space Autoregressive World Model for Efficient and Robust Image-Goal Navigation](https://arxiv.org/abs/2511.11011)
*Zhiwei Zhang,Hui Zhang,Xieyuanli Chen,Kaihong Huang,Chenghao Shi,Huimin Lu*

Main category: cs.RO

TL;DR: LS-NWM是一个轻量级的导航世界模型，完全在潜在空间中进行训练和操作，显著降低了计算成本并提高了导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统世界模型训练和推理的高计算成本问题，同时提高导航性能。

Method: 在潜在空间中进行训练和操作，预测未来潜在状态，并进行路径规划和决策。采用自回归多帧预测策略来捕捉时空依赖性。

Result: 与最先进的基线相比，训练时间减少了约3.2倍，规划时间减少了约447倍，成功率（SR）提高了35%，路径长度（SPL）提高了11%。

Conclusion: LS-NWM在保持高计算效率的同时，实现了先进的导航性能，并且在复杂场景下表现出色。

Abstract: Traditional navigation methods rely heavily on accurate localization and mapping. In contrast, world models that capture environmental dynamics in latent space have opened up new perspectives for navigation tasks, enabling systems to move beyond traditional multi-module pipelines. However, world model often suffers from high computational costs in both training and inference. To address this, we propose LS-NWM - a lightweight latent space navigation world model that is trained and operates entirely in latent space, compared to the state-of-the-art baseline, our method reduces training time by approximately 3.2x and planning time by about 447x,while further improving navigation performance with a 35% higher SR and an 11% higher SPL. The key idea is that accurate pixel-wise environmental prediction is unnecessary for navigation. Instead, the model predicts future latent states based on current observational features and action inputs, then performs path planning and decision-making within this compact representation, significantly improving computational efficiency. By incorporating an autoregressive multi-frame prediction strategy during training, the model effectively captures long-term spatiotemporal dependencies, thereby enhancing navigation performance in complex scenarios. Experimental results demonstrate that our method achieves state-of-the-art navigation performance while maintaining a substantial efficiency advantage over existing approaches.

</details>


### [96] [Miniature Testbed for Validating Multi-Agent Cooperative Autonomous Driving](https://arxiv.org/abs/2511.11022)
*Hyunchul Bae,Eunjae Lee,Jehyeop Han,Minhee Kang,Jaehyeon Kim,Junggeun Seo,Minkyun Noh,Heejin Ahn*

Main category: cs.RO

TL;DR: 本文设计并实现了一个名为CIVAT的1:15比例的微型测试平台，用于验证协同式自动驾驶。该测试平台集成了包括传感器、边缘计算和通信能力的智能路侧基础设施。通过V2V和V2I通信以及ROS2框架，实现了车辆与基础设施之间的信息交换，并进行了基于基础设施的感知和交叉口管理实验。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶测试平台缺乏配备传感、边缘计算和通信能力的智能基础设施。为了解决这一差距，需要一个能够验证协同式自动驾驶的测试平台。

Method: 设计并实现了一个1:15比例的微型测试平台CIVAT，该平台包含一个等比例缩小的城市地图、配备车载传感器的自动驾驶车辆以及智能路侧基础设施。测试平台集成了V2V和V2I通信，并通过共享Wi-Fi和ROS2框架使用发布-订阅模式，以实现车辆与基础设施之间的信息交换，从而支持协同驾驶功能。

Result: 通过基础设施感知识别和交叉口管理实验，验证了CIVAT测试平台的有效性。

Conclusion: CIVAT测试平台成功实现了协同式自动驾驶功能的验证，证明了其在研究和开发中的实用性。

Abstract: Cooperative autonomous driving, which extends vehicle autonomy by enabling real-time collaboration between vehicles and smart roadside infrastructure, remains a challenging yet essential problem. However, none of the existing testbeds employ smart infrastructure equipped with sensing, edge computing, and communication capabilities. To address this gap, we design and implement a 1:15-scale miniature testbed, CIVAT, for validating cooperative autonomous driving, consisting of a scaled urban map, autonomous vehicles with onboard sensors, and smart infrastructure. The proposed testbed integrates V2V and V2I communication with the publish-subscribe pattern through a shared Wi-Fi and ROS2 framework, enabling information exchange between vehicles and infrastructure to realize cooperative driving functionality. As a case study, we validate the system through infrastructure-based perception and intersection management experiments.

</details>


### [97] [AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation](https://arxiv.org/abs/2511.11052)
*Jinxuan Zhu,Chenrui Tie,Xinyi Cao,Yuran Wang,Jingxiang Guo,Zixuan Chen,Haonan Chen,Junting Chen,Yangyu Xiao,Ruihai Wu,Lin Shao*

Main category: cs.RO

TL;DR: 该研究提出了一个名为ApaptPNP的框架，它结合了视觉-语言模型（VLM）和任务与运动规划，以实现非抓取（NP）和抓取（P）操作的结合，用于机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作中，非抓取（NP）操作（如推动、戳动或滑动）能扩展机器人的能力，尤其是在抓取不可行或不足时。然而，开发一个能统一处理不同任务、物体和环境，并无缝集成NP和P操作的通用框架仍然是一个挑战。机器人需要判断何时使用NP技能，为特定情境选择合适的原始操作，并将P和NP策略组合成健壮的多步计划。

Method: 该方法利用VLM来解析视觉场景观察和文本任务描述，生成一个高级计划框架，该框架规定了P和NP操作的序列和协调。通过一个基于数字孪生（digital-twin）的面向对象的中间层来预测期望的物体姿态，从而能够对操纵序列进行主动的心理演练。最后，一个控制模块合成低级机器人命令，并通过持续的执行反馈实现VLM的在线任务计划精炼和自适应重规划。

Result: 在模拟和真实环境中，ApaptPNP在代表性的P&NP混合操作任务上进行了评估，结果表明了混合P&NP操作在实现通用、达到人类水平的机器人操作能力方面具有重要潜力。

Conclusion: ApaptPNP框架通过结合VLM和任务与运动规划，有效地实现了非抓取（NP）和抓取（P）操作的混合，为机器人提供了更强大的操作能力，并为实现更通用的机器人操作迈出了重要一步。

Abstract: Non-prehensile (NP) manipulation, in which robots alter object states without forming stable grasps (for example, pushing, poking, or sliding), significantly broadens robotic manipulation capabilities when grasping is infeasible or insufficient. However, enabling a unified framework that generalizes across different tasks, objects, and environments while seamlessly integrating non-prehensile and prehensile (P) actions remains challenging: robots must determine when to invoke NP skills, select the appropriate primitive for each context, and compose P and NP strategies into robust, multi-step plans. We introduce ApaptPNP, a vision-language model (VLM)-empowered task and motion planning framework that systematically selects and combines P and NP skills to accomplish diverse manipulation objectives. Our approach leverages a VLM to interpret visual scene observations and textual task descriptions, generating a high-level plan skeleton that prescribes the sequence and coordination of P and NP actions. A digital-twin based object-centric intermediate layer predicts desired object poses, enabling proactive mental rehearsal of manipulation sequences. Finally, a control module synthesizes low-level robot commands, with continuous execution feedback enabling online task plan refinement and adaptive replanning through the VLM. We evaluate ApaptPNP across representative P&NP hybrid manipulation tasks in both simulation and real-world environments. These results underscore the potential of hybrid P&NP manipulation as a crucial step toward general-purpose, human-level robotic manipulation capabilities. Project Website: https://sites.google.com/view/adaptpnp/home

</details>


### [98] [Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2511.11218)
*Chenhao Liu,Leyun Jiang,Yibo Wang,Kairan Yao,Jinchen Fu,Xiaoyu Ren*

Main category: cs.RO

TL;DR: 通过强化学习训练人形机器人进行羽毛球比赛，实现了协调的步法和击球动作。


<details>
  <summary>Details</summary>
Motivation: 现实世界具有动态性，需要机器人具备更强的动态交互能力，而不仅仅是静态交互。

Method: 采用三阶段强化学习课程：先是步法学习，然后是精确的挥拍生成，最后是专注于比赛任务的优化。引入了扩展卡尔曼滤波器（EKF）来估计和预测羽毛球的轨迹，并提出了一种无需EKF和轨迹预测的变体。

Result: 在模拟环境中，机器人能够连续对打21拍。在真实世界的测试中，机器人可以达到10米/秒的出球速度，并且能够准确地将球打到3.5米远的地方。

Conclusion: 所提出的人形机器人羽毛球控制器能够实现高动态和高精度的目标击球，并且该框架可以应用于其他需要动态交互的领域。

Abstract: Humanoid robots have demonstrated strong capability for interacting with deterministic scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, quasi-static interactions are insufficient to cope with the various environmental conditions. As a step toward more dynamic interaction scenario, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without any motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiment in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both the prediction and controller module exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 10 m/s with a mean return landing distance of 3.5 m. These experiment results show that our humanoid robot can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamism critical domains.

</details>


### [99] [Sashimi-Bot: Autonomous Tri-manual Advanced Manipulation and Cutting of Deformable Objects](https://arxiv.org/abs/2511.11223)
*Sverre Herland,Amit Parag,Elling Ruud Øye,Fangyi Zhang,Fouad Makiyeh,Aleksander Lillienskiold,Abhaya Pal Singh,Edward H. Adelson,Francois Chaumette,Alexandre Krupa,Peter Corke,Ekrem Misimi*

Main category: cs.RO

TL;DR: Sashimi-Bot是一个多机器人系统，可以自主地抓取、稳定和切割三文鱼片，解决了处理可变形、体积大的物体的机器人技术挑战。


<details>
  <summary>Details</summary>
Motivation: 处理具有弹塑性、易损性、可变性和不确定性的可变形、体积大的物体是机器人操作中的一个重大挑战。

Method: Sashimi-Bot结合了深度强化学习、手部工具形状操纵、手部工具切割以及视觉和触觉反馈，以实现对任务中固有的各种情况的鲁棒性。具体来说，三个机器人协同工作，由一个机器人拉直鱼片，一个机器人抓握和操纵刀具，另一个机器人进行切割。

Result: 该系统能够可靠地操纵三文鱼，并将其切成薄片，证明了机器人处理可变形、体积大物体的能力。

Conclusion: Sashimi-Bot在机器人操作可变形、体积大物体方面取得了里程碑式的进展，有望在其他现实世界应用中发挥作用。

Abstract: Advanced robotic manipulation of deformable, volumetric objects remains one of the greatest challenges due to their pliancy, frailness, variability, and uncertainties during interaction. Motivated by these challenges, this article introduces Sashimi-Bot, an autonomous multi-robotic system for advanced manipulation and cutting, specifically the preparation of sashimi. The objects that we manipulate, salmon loins, are natural in origin and vary in size and shape, they are limp and deformable with poorly characterized elastoplastic parameters, while also being slippery and hard to hold. The three robots straighten the loin; grasp and hold the knife; cut with the knife in a slicing motion while cooperatively stabilizing the loin during cutting; and pick up the thin slices from the cutting board or knife blade. Our system combines deep reinforcement learning with in-hand tool shape manipulation, in-hand tool cutting, and feedback of visual and tactile information to achieve robustness to the variabilities inherent in this task. This work represents a milestone in robotic manipulation of deformable, volumetric objects that may inspire and enable a wide range of other real-world applications.

</details>


### [100] [Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation](https://arxiv.org/abs/2511.11298)
*Yihao Zhang,Yuankai Qi,Xi Zheng*

Main category: cs.RO

TL;DR: 在此论文中，我们对四种代表性的视觉-语言-动作（VLA）模型（ACT、OpenVLA-OFT、RDT-1B 和 π₀）在模拟和 ALOHA Mobile 平台上进行了实证评估，并建立了一个标准化的评估框架，涵盖了准确性、效率、适应性和语言指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有关于基础模型在机器人领域（特别是视觉-语言-动作模型）应用的实证研究和跨模型比较仍然稀少，而这些模型在实现通用操作方面展现出巨大潜力。

Method: 在一个标准化的评估框架下，在模拟和 ALOHA Mobile 平台上，对 ACT、OpenVLA-OFT、RDT-1B 和 π₀ 这四种 VLA 模型进行了性能评估。评估维度包括：1. 准确性和效率（成功率和达到成功所需时间）；2. 适应性（在分布内、空间分布外、实例加空间分布外场景下的表现）；3. 语言指令遵循准确性。

Result: π₀ 模型在分布外场景中展现出更强的适应性，而 ACT 模型在分布内场景中表现出最高的稳定性。研究还揭示了不同 VLA 模型在计算需求、数据扩展行为和常见失败模式（如抓取失误、过早释放和长时程状态漂移）方面的差异。

Conclusion: 这项研究通过实证分析揭示了 VLA 模型架构在平衡精度、泛化性和部署成本方面存在的实际权衡，为在现实世界的机器人操作任务中选择和部署 VLA 模型提供了可行的见解。

Abstract: Foundation models applied in robotics, particularly \textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \textbf{empirical experiences} from benchmarking four representative VLAs -- \textbf{ACT}, \textbf{OpenVLA--OFT}, \textbf{RDT-1B}, and \boldmath{$π_0$} -- across four manipulation tasks conducted in both simulation and on the \textbf{ALOHA Mobile} platform. We establish a \textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \textit{accuracy and efficiency} (success rate and time-to-success), (2) \textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \textit{language instruction-following accuracy}. Through this process, we observe that \boldmath{$π_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.

</details>


### [101] [Simulating an Autonomous System in CARLA using ROS 2](https://arxiv.org/abs/2511.11310)
*Joseph Abdo,Aditya Shibu,Moaiz Saeed,Abdul Maajid Aga,Apsara Sivaprazad,Mohamed Al-Musleh*

Main category: cs.RO

TL;DR: 本研究提出了一种在CARLA仿真环境中为FS-AI 2025比赛设计的自动驾驶赛车软件栈，利用多种传感器和ROS 2进行赛道边界检测、轨迹优化和车辆控制，并在仿真和真实硬件上进行了广泛验证。


<details>
  <summary>Details</summary>
Motivation: 在高速和不确定的环境中，为自动驾驶赛车提供可靠的感知、规划和控制能力，以在FS-AI 2025比赛中取得竞争性表现。

Method: 使用360°激光雷达、立体摄像头、GNSS和IMU传感器，通过ROS 2可靠地检测赛道边界锥体（最远35米）。考虑车辆动力学以及可见性和光照等环境因素，计算优化轨迹，以实现高效的赛道导航。完整的软硬件栈在CARLA仿真中进行了广泛验证，然后移植到Jetson AGX Orin、ZED2i立体摄像头、Robosense Helios 16P激光雷达和CHCNAV INS等实际硬件上。

Result: 在CARLA仿真环境中，该系统能够可靠地检测到距离长达35米的赛道边界锥体，并计算出考虑车辆动力学和环境因素的优化轨迹，实现了高效的赛道导航。

Conclusion: 该研究成功设计并验证了一个用于自动驾驶赛车的软件栈，该软件栈能够在CARLA仿真环境中实现有竞争力的驾驶性能，并已成功移植到实际硬件上，为FS-AI 2025比赛做好了准备。

Abstract: Autonomous racing offers a rigorous setting to stress test perception, planning, and control under high speed and uncertainty. This paper proposes an approach to design and evaluate a software stack for an autonomous race car in CARLA: Car Learning to Act simulator, targeting competitive driving performance in the Formula Student UK Driverless (FS-AI) 2025 competition. By utilizing a 360° light detection and ranging (LiDAR), stereo camera, global navigation satellite system (GNSS), and inertial measurement unit (IMU) sensor via ROS 2 (Robot Operating System), the system reliably detects the cones marking the track boundaries at distances of up to 35 m. Optimized trajectories are computed considering vehicle dynamics and simulated environmental factors such as visibility and lighting to navigate the track efficiently. The complete autonomous stack is implemented in ROS 2 and validated extensively in CARLA on a dedicated vehicle (ADS-DV) before being ported to the actual hardware, which includes the Jetson AGX Orin 64GB, ZED2i Stereo Camera, Robosense Helios 16P LiDAR, and CHCNAV Inertial Navigation System (INS).

</details>


### [102] [A Comparative Evaluation of Prominent Methods in Autonomous Vehicle Certification](https://arxiv.org/abs/2511.11484)
*Mustafa Erdem Kırmızıgül,Hasan Feyzi Doğruyol,Haluk Bayram*

Main category: cs.RO

TL;DR: "Vision Zero"政策旨在消除交通事故的伤亡，并设想利用自动驾驶汽车实现这一目标。然而，自动驾驶汽车的安全要求验证和认证方法仍不明确。本文旨在对自动驾驶汽车认证过程中采用的突出方法进行比较评估，并提出一个包含认证阶段、参与者和适用方法的认证流程。


<details>
  <summary>Details</summary>
Motivation: "Vision Zero"政策要求消除交通事故伤亡，而自动驾驶汽车的广泛应用是实现该目标的关键。但自动驾驶汽车的安全认证标准和方法尚不明确，因此需要对现有方法进行评估和梳理。

Method: 本文对自动驾驶汽车认证过程中的主要方法进行了比较评估，并构建了一个认证流程，明确了认证的阶段、参与者以及各种方法在认证过程中的应用。

Result: 通过对现有认证方法的比较评估，本文提出了一个自动驾驶汽车的认证流程，明确了各个阶段和参与者，并指出了不同方法的适用性。

Conclusion: 对自动驾驶汽车的认证方法进行系统性评估，并提出一个清晰的认证流程，是实现"Vision Zero"政策目标、确保自动驾驶汽车安全性的重要一步。

Abstract: The "Vision Zero" policy, introduced by the Swedish Parliament in 1997, aims to eliminate fatalities and serious injuries resulting from traffic accidents. To achieve this goal, the use of self-driving vehicles in traffic is envisioned and a roadmap for the certification of self-driving vehicles is aimed to be determined. However, it is still unclear how the basic safety requirements that autonomous vehicles must meet will be verified and certified, and which methods will be used. This paper focuses on the comparative evaluation of the prominent methods planned to be used in the certification process of autonomous vehicles. It examines the prominent methods used in the certification process, develops a pipeline for the certification process of autonomous vehicles, and determines the stages, actors, and areas where the addressed methods can be applied.

</details>


### [103] [Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities](https://arxiv.org/abs/2511.11512)
*Yiyun Zhou,Mingjing Xu,Jingwei Shi,Quanjiang Li,Jingyuan Chen*

Main category: cs.RO

TL;DR:  to address the lack of standardization in tactile sensors and improve cross-modal generalization, this paper proposes TLV-CoRe, a method that unifies tactile features, disentangles irrelevant information, and enhances tri-modal interaction. It also introduces the RSS evaluation framework for fair assessment.


<details>
  <summary>Details</summary>
Motivation: Existing tactile sensors lack standardization, leading to redundant features and hindering cross-sensor generalization. Existing methods also fail to fully integrate communication among tactile, language, and vision modalities.

Method: The paper proposes TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. It includes a Sensor-Aware Modulator to unify tactile features, tactile-irrelevant decoupled learning to disentangle irrelevant features, and a Unified Bridging Adapter to enhance tri-modal interaction. The RSS evaluation framework is also introduced.

Result: TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment.

Conclusion: TLV-CoRe offers a new direction for multimodal tactile representation by effectively unifying tactile features and enhancing tri-modal interaction, supported by the proposed RSS evaluation framework.

Abstract: Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.

</details>


### [104] [Scalable Coverage Trajectory Synthesis on GPUs as Statistical Inference](https://arxiv.org/abs/2511.11514)
*Max M. Sun,Jueun Kwon,Todd Murphey*

Main category: cs.RO

TL;DR: 该研究将覆盖运动规划问题构建为一种基于流匹配的统计推断问题，以提高计算效率和并行化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的覆盖运动规划方法在计算效率和并行化方面存在局限性，需要更优化的方法。

Method: 将覆盖运动规划问题转化为统计推断问题，并采用流匹配技术，结合了统计差异度量（如KL散度和Sinkhorn散度）和线性二次调节器问题，将覆盖轨迹梯度生成与非线性系统动力学下的控制合成分离开。

Result: 通过并行化（尤其是在GPU上）显著提高了计算速度，并展示了与传统基于航点跟踪方法的计算优势。

Conclusion: 提出的方法通过并行化提供了更好的可扩展性，在覆盖运动规划方面具有计算优势。

Abstract: Coverage motion planning is essential to a wide range of robotic tasks. Unlike conventional motion planning problems, which reason over temporal sequences of states, coverage motion planning requires reasoning over the spatial distribution of entire trajectories, making standard motion planning methods limited in computational efficiency and less amenable to modern parallelization frameworks. In this work, we formulate the coverage motion planning problem as a statistical inference problem from the perspective of flow matching, a generative modeling technique that has gained significant attention in recent years. The proposed formulation unifies commonly used statistical discrepancy measures, such as Kullback-Leibler divergence and Sinkhorn divergence, with a standard linear quadratic regulator problem. More importantly, it decouples the generation of trajectory gradients for coverage from the synthesis of control under nonlinear system dynamics, enabling significant acceleration through parallelization on modern computational architectures, particularly Graphics Processing Units (GPUs). This paper focuses on the advantages of this formulation in terms of scalability through parallelization, highlighting its computational benefits compared to conventional methods based on waypoint tracking.

</details>


### [105] [Terrain Costmap Generation via Scaled Preference Conditioning](https://arxiv.org/abs/2511.11529)
*Luisa Mao,Garret Warnell,Peter Stone,Joydeep Biswas*

Main category: cs.RO

TL;DR: SPACER是一种新的地形成本图生成方法，它利用合成数据进行训练以实现泛化，并通过调整用户指定的偏好上下文来快速适应测试时间成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时实现成本图的泛化和快速适应，而这种能力对于越野机器人导航至关重要。

Method: SPACER利用合成数据进行训练以实现泛化，并通过调整用户指定的偏好上下文来快速适应测试时间成本。

Result: 在五种环境的七种环境中，SPACER在全局路径规划中的表现优于其他方法，测得的惋惜度最低。

Conclusion: SPACER在越野机器人导航领域生成地形成本图方面取得了优于其他方法的效果，能够很好地泛化并快速适应测试时间成本。

Abstract: Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.

</details>

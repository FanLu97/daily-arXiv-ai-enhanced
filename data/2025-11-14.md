<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.RO](#cs.RO) [Total: 18]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation](https://arxiv.org/abs/2511.09611)
*Ye Tian,Ling Yang,Jiongfan Yang,Anran Wang,Yu Tian,Jiani Zheng,Haochen Wang,Zhiyang Teng,Zhuochen Wang,Yinjie Wang,Yunhai Tong,Mengdi Wang,Xiangtai Li*

Main category: cs.CV

TL;DR: 现有分步思考生成方法在复杂任务中可能因错误传播而适得其反，导致性能下降。为解决此问题，我们提出了ParaBench基准测试和MMaDA-Parallel模型。MMaDA-Parallel通过并行多模态扩散框架和ParaRL训练，实现了文本与图像的持续双向交互，提高了跨模态对齐和语义一致性，在ParaBench上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 识别出现有分步思考生成方法在复杂任务中可能因错误传播而适得其反，导致文本和图像输出性能下降的问题。

Method: 提出ParaBench基准测试来系统性地分析该问题。提出MMaDA-Parallel，一种并行多模态扩散框架，通过在整个去噪过程中实现文本和图像的持续双向交互来解决问题。使用监督微调进行训练，并通过新颖的并行强化学习（ParaRL）策略进行优化，该策略沿轨迹应用语义奖励以强制跨模态一致性。

Result: MMaDA-Parallel在ParaBench上将输出对齐度提高了6.9%，优于最先进的模型Bagel，实现了更稳健的思考感知图像生成范式。实验证明该模型显著提高了跨模态对齐和语义一致性。

Conclusion: 提出的并行多模态扩散框架MMaDA-Parallel和ParaRL训练策略能够有效解决分步思考生成中的错误传播问题，显著提高跨模态对齐和语义一致性，为思考感知图像合成提供了一个更鲁棒的范式。

Abstract: While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel

</details>


### [2] [PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild](https://arxiv.org/abs/2511.09675)
*Felix B. Mueller,Jan F. Meier,Timo Lueddecke,Richard Vogg,Roger L. Freixanet,Valentin Hassler,Tiffany Bosshard,Elif Karakoc,William J. O'Hearn,Sofia M. Pereira,Sandro Sehner,Kaja Wierucka,Judith Burkart,Claudia Fichtel,Julia Fischer,Alexander Gail,Catherine Hobaiter,Julia Ostner,Liran Samuni,Oliver Schülke,Neda Shahidi,Erin G. Wessling,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 通过构建大规模的灵长类动物为中心的视频预训练数据集PriVi，并在此数据集上预训练V-JEPA模型，然后在四个基准数据集上进行评估，证明了灵长类动物为中心的预训练能够显著提高数据效率和泛化能力，尤其在标签数据较少的情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有的计算机视觉方法在分析非人灵长类动物行为时，由于依赖于以人类为中心的预训练模型和单一数据集，存在泛化能力受限的问题。本研究旨在克服这一局限性。

Method: 提出了一种数据为中心的学习方法，构建了一个大规模的灵长类动物为中心的视频预训练数据集PriVi（包含424小时视频），并使用该数据集预训练V-JEPA模型。然后，使用一个轻量级的冻结分类器在ChimpACT、BaboonLand、PanAf500和ChimpBehave四个基准数据集上进行评估。

Result: 在四个基准数据集上，本研究提出的方法一致优于先前的工作，包括完全微调的基线模型，并且在标签数据较少的情况下具有良好的可扩展性。

Conclusion: 灵长类动物为中心的预训练极大地提高了数据效率和泛化能力，为标签数据稀疏的应用提供了有前景的方法。

Abstract: Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.

</details>


### [3] [Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression](https://arxiv.org/abs/2511.09702)
*Katie Matton,Purvaja Balaji,Hamzeh Ghasemzadeh,Jameson C. Cooper,Daryush D. Mehta,Jarrad H. Van Stan,Robert E. Hillman,Rosalind Picard,John Guttag,S. Mazdak Abulnaga*

Main category: cs.CV

TL;DR: 我们提出了一种新的软序数回归方法，用于从声带图像自动分类发声创伤的严重程度。


<details>
  <summary>Details</summary>
Motivation: 发声创伤的严重程度评估依赖于临床专家的判断，这种方法成本高昂且可靠性差异大。

Method: 采用序数回归框架，并提出一种新颖的软标签序数回归损失函数。

Result: 我们的方法在预测性能上接近临床专家，并能提供校准良好的不确定性估计。

Conclusion: 该自动化工具可以实现大规模的发声创伤研究，最终改善临床理解和患者护理。

Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.

</details>


### [4] [SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control](https://arxiv.org/abs/2511.09715)
*Arman Zarei,Samyadeep Basu,Mobina Pournemat,Sayan Nag,Ryan Rossi,Soheil Feizi*

Main category: cs.CV

TL;DR: SliderEdit是一个用于连续图像编辑的框架，它通过解耦多指令提示中的各个指令，并为每个指令提供可调节的滑块，实现了对图像编辑精细化、可解释的控制。该框架通过学习一组低秩适配矩阵，能够泛化到各种编辑、属性和组合指令，实现在保持空间局部性和全局语义一致性的同时，沿单个编辑维度进行连续插值。SliderEdit已被应用于FLUX-Kontext和Qwen-Image-Edit等模型，显著提升了编辑的可控性、视觉一致性和用户可操纵性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于指令的图像编辑模型无法精确、连续地控制单个指令的强度，限制了用户的精细化编辑能力。

Method: SliderEdit框架解耦多指令提示中的各个指令，并将每个指令暴露为一个全局训练的滑块，允许用户平滑地调整其强度。该方法学习一组低秩适配矩阵，能够泛化到各种编辑、属性和组合指令，从而在保持空间局部性和全局语义一致性的同时，沿单个编辑维度进行连续插值。

Result: 在FLUX-Kontext和Qwen-Image-Edit等模型上应用SliderEdit后，观察到编辑可控性、视觉一致性和用户可操纵性得到了显著提升。

Conclusion: SliderEdit首次探索并提出了用于指令驱动图像编辑的连续、精细化指令控制框架，为实现具有连续和组合控制的交互式指令驱动图像操纵铺平了道路。

Abstract: Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.

</details>


### [5] [Density Estimation and Crowd Counting](https://arxiv.org/abs/2511.09723)
*Balachandra Devarangadi Sunil,Rakshith Venkatesh,Shantanu Todmal*

Main category: cs.CV

TL;DR: 本研究提出了一种结合扩散模型和事件驱动采样技术的视频人群密度估计新方法，通过生成高质量密度图并选取关键帧来提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了改进现有的基于图像的 crowd density 估计算法，使其适用于视频分析，并解决视频分析中的时间挑战，从而提高实时人群监控的可扩展性和效率。

Method: 提出了一种结合去噪概率扩散模型、窄高斯核、多密度图输出、回归分支和相似度分数合并机制的算法。引入了基于 Farneback 光流算法的事件驱动采样技术，以选择性地捕捉包含显著人群移动的帧。

Result: 该模型能够有效地捕捉密集和稀疏场景下的人群动态，在定性和定量评估中表现良好（包括叠加图和平均绝对误差 MAE）。事件驱动采样技术能够减少帧数，同时保留关键的人群事件。

Conclusion: 该研究提供了一个可扩展且高效的视频人群密度估计框架，能够有效处理人群动态，并可应用于公共安全、灾难响应和活动管理等领域。

Abstract: This study enhances a crowd density estimation algorithm originally designed for image-based analysis by adapting it for video-based scenarios. The proposed method integrates a denoising probabilistic model that utilizes diffusion processes to generate high-quality crowd density maps. To improve accuracy, narrow Gaussian kernels are employed, and multiple density map outputs are generated. A regression branch is incorporated into the model for precise feature extraction, while a consolidation mechanism combines these maps based on similarity scores to produce a robust final result. An event-driven sampling technique, utilizing the Farneback optical flow algorithm, is introduced to selectively capture frames showing significant crowd movements, reducing computational load and storage by focusing on critical crowd dynamics. Through qualitative and quantitative evaluations, including overlay plots and Mean Absolute Error (MAE), the model demonstrates its ability to effectively capture crowd dynamics in both dense and sparse settings. The efficiency of the sampling method is further assessed, showcasing its capability to decrease frame counts while maintaining essential crowd events. By addressing the temporal challenges unique to video analysis, this work offers a scalable and efficient framework for real-time crowd monitoring in applications such as public safety, disaster response, and event management.

</details>


### [6] [PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model](https://arxiv.org/abs/2511.09724)
*Yunqian Cheng,Benjamin Princen,Roberto Manduchi*

Main category: cs.CV

TL;DR: PALMS+是一种基于图像的室内定位系统，无需额外基础设施，通过3D点云重建和几何布局匹配来提高准确性，并在各种数据集和轨迹上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在GPS受限的室内环境中，如紧急响应和辅助导航，进行室内定位至关重要。现有的基于视觉的方法（如PALMS）虽然无需额外基础设施，但受限于智能手机LiDAR的短距离和室内布局的歧义性。

Method: PALMS+系统通过基础单目深度估计模型（Depth Pro）从姿态RGB图像重建尺度对齐的3D点云，然后通过与地板图进行卷积来进行几何布局匹配。

Result: PALMS+在Structured3D数据集和包含80个观测值的定制校园数据集上，在静态定位准确性方面优于PALMS和F3Loc。在用于顺序定位时，PALMS+实现的定位误差较低，证明了其在无摄像头跟踪和无基础设施应用方面的鲁棒性。

Conclusion: PALMS+通过其模块化、基于图像的方法，克服了现有室内定位技术的局限性，并在静态和顺序定位任务中均表现出优越的性能，为无基础设施的室内定位应用提供了有前景的解决方案。

Abstract: Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones

</details>


### [7] [Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.09735)
*Ahmed Alia,Mohcine Chraibi,Armin Seyfried*

Main category: cs.CV

TL;DR: 本文提出了一种新的深度学习模型，增强了Social LSTM，并引入了一个动态占据空间损失函数，以解决人群中行人的轨迹预测问题。该模型通过考虑行人的物理空间和引入场景密度及个体空间占据敏感的碰撞惩罚，有效降低了碰撞率并提高了位移预测精度。


<details>
  <summary>Details</summary>
Motivation: 在动态和拥挤的环境中，由于人类运动的复杂性和个体之间的相互影响， realistic pedestrian trajectory prediction 仍然是一个挑战。现有的深度学习模型虽然取得了 promising results，但大多数将行人视为点实体，忽略了每个人所占据的物理空间。

Method: 提出了一种新的深度学习模型，通过引入一个动态占据空间（Dynamic Occupied Space）损失函数来增强Social LSTM。该损失函数结合了平均位移误差（average displacement error）和一个新的碰撞惩罚（collision penalty），该惩罚对场景密度和个体空间占据空间敏感。

Result: 所提出的模型不仅降低了碰撞率，而且在所有五个生成的数据集（来自法国里昂2022年灯光节的真实行人轨迹）上都提高了位移预测精度。具体而言，与基线模型相比，该模型平均将碰撞率降低了高达31%，平均位移误差降低了5%，最终位移误差降低了6%。此外，在大多数测试集上，该模型性能优于几种最先进的深度学习模型。

Conclusion: 所提出的动态占据空间损失函数能够有效指导Social LSTM在不增加位移误差的情况下，学习避免现实世界中的碰撞，并且在不同的人群密度（从低到高）和场景（均匀和不均匀密度）下都能取得良好效果。

Abstract: In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.

</details>


### [8] [Soiling detection for Advanced Driver Assistance Systems](https://arxiv.org/abs/2511.09740)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: The paper addresses soiling detection for automotive cameras by treating it as a semantic segmentation problem. It compares segmentation methods, identifies issues with the Woodscape dataset (data leakage and imprecise annotations), and proposes a smaller, revised dataset for more efficient training while achieving comparable results.


<details>
  <summary>Details</summary>
Motivation: Soiling detection for automotive cameras is crucial for the robustness of advanced driver assistance systems, especially under adverse conditions like weather and dust.

Method: The paper treats soiling detection as a semantic segmentation problem, compares popular segmentation methods against tile-level classification approaches, and creates a revised, smaller dataset to address data leakage and annotation issues found in the original Woodscape dataset.

Result: Semantic segmentation methods show superiority over tile-level classification. The revised dataset, despite its smaller size, enables segmentation methods to achieve comparable results more efficiently.

Conclusion: The paper validates the effectiveness of semantic segmentation for soiling detection and introduces a revised dataset that resolves issues with the original Woodscape dataset, leading to more efficient and accurate detection under challenging conditions.

Abstract: Soiling detection for automotive cameras is a crucial part of advanced driver assistance systems to make them more robust to external conditions like weather, dust, etc. In this paper, we regard the soiling detection as a semantic segmentation problem. We provide a comprehensive comparison of popular segmentation methods and show their superiority in performance while comparing them to tile-level classification approaches. Moreover, we present an extensive analysis of the Woodscape dataset showing that the original dataset contains a data-leakage and imprecise annotations. To address these problems, we create a new data subset, which, despite being much smaller, provides enough information for the segmentation method to reach comparable results in a much shorter time. All our codes and dataset splits are available at https://github.com/filipberanek/woodscape_revision.

</details>


### [9] [Feature Quality and Adaptability of Medical Foundation Models: A Comparative Evaluation for Radiographic Classification and Segmentation](https://arxiv.org/abs/2511.09742)
*Frank Li,Theo Dapamede,Mohammadreza Chavoshi,Young Seok Jeon,Bardia Khosravi,Abdulhameed Dere,Beatrice Brown-Mulry,Rohan Satya Isaac,Aawez Mansuri,Chiratidzo Sanyika,Janice Newsome,Saptarshi Purkayastha,Imon Banerjee,Hari Trivedi,Judy Gichoya*

Main category: cs.CV

TL;DR: 医学领域的基金模型在医学影像中的泛化能力参差不齐，本研究评估了八个医学和通用领域基金模型的视觉编码器在胸部X光分析中的表现，比较了线性探测和微调下的分类和分割任务。与通用领域模型相比，医学领域预训练模型在特征质量上表现出明显优势，特别是在全局分类和显著解剖结构分割任务上。然而，在分割复杂、细微病变（如气胸）方面，所有基金模型在未经大量微调的情况下表现均不佳。研究还发现基金模型在分类任务中存在利用混淆捷径（如使用胸管区分气胸）的问题，这种策略在精确分割任务中失效。此外，昂贵的文本-图像对齐并非必需，仅图像（RAD-DINO）或标签监督（Ark+）的基金模型表现优异。值得注意的是，一个有监督的端到端基线模型在分割任务上表现与最佳基金模型相当甚至更优。结论是，医学预训练有益，但架构选择（如多尺度）至关重要，预训练特征并非在所有任务上都有效，特别是在需要精确定位的复杂任务上，有监督模型仍然是强有力的替代方案。


<details>
  <summary>Details</summary>
Motivation: 评估不同预训练领域（医学 vs. 通用）、范式（如文本引导）和架构对医学影像嵌入质量的影响，以期为放射学任务选择最佳编码器。

Method: 评估来自八个医学和通用领域基金模型的视觉编码器在胸部X光分类（气胸、心增大）和分割（气胸、心脏边界）任务上的表现，采用线性探测和微调两种方法进行基准测试。

Result: 1. 特定领域的预训练提供了显著优势；在医学领域预训练的基金模型在线性探测任务中始终优于通用领域模型，显示出更优的初始特征质量。
2. 特征的效用高度依赖于具体任务。预训练的嵌入在全局分类和显著解剖结构（如心脏）的分割任务上表现良好。
3. 对于需要精确分割细微病变（如气胸）的任务，所有基金模型在未经大量微调的情况下表现均不佳，表明在精确定位细微病变方面存在关键差距。
4. 子组分析显示，基金模型在分类任务中会利用混淆捷径（例如，使用胸管来识别气胸），但这种策略在精确分割任务中会失效。
5. 昂贵的文本-图像对齐并非必要；仅图像（RAD-DINO）和标签监督（Ark+）的基金模型均位列表现最佳的模型之列。
6. 一个有监督的、端到端的基线模型在分割任务上表现出高度竞争力，其表现与最佳基金模型相当或更优。

Conclusion: 尽管医学领域预训练对基金模型有益，但架构选择（例如，多尺度）至关重要。预训练的特征并非对所有任务都有效，特别是在需要精确定位的复杂任务上，有监督模型仍然是强有力的替代方案。

Abstract: Foundation models (FMs) promise to generalize medical imaging, but their effectiveness varies. It remains unclear how pre-training domain (medical vs. general), paradigm (e.g., text-guided), and architecture influence embedding quality, hindering the selection of optimal encoders for specific radiology tasks. To address this, we evaluate vision encoders from eight medical and general-domain FMs for chest X-ray analysis. We benchmark classification (pneumothorax, cardiomegaly) and segmentation (pneumothorax, cardiac boundary) using linear probing and fine-tuning. Our results show that domain-specific pre-training provides a significant advantage; medical FMs consistently outperformed general-domain models in linear probing, establishing superior initial feature quality. However, feature utility is highly task-dependent. Pre-trained embeddings were strong for global classification and segmenting salient anatomy (e.g., heart). In contrast, for segmenting complex, subtle pathologies (e.g., pneumothorax), all FMs performed poorly without significant fine-tuning, revealing a critical gap in localizing subtle disease. Subgroup analysis showed FMs use confounding shortcuts (e.g., chest tubes for pneumothorax) for classification, a strategy that fails for precise segmentation. We also found that expensive text-image alignment is not a prerequisite; image-only (RAD-DINO) and label-supervised (Ark+) FMs were among top performers. Notably, a supervised, end-to-end baseline remained highly competitive, matching or exceeding the best FMs on segmentation tasks. These findings show that while medical pre-training is beneficial, architectural choices (e.g., multi-scale) are critical, and pre-trained features are not universally effective, especially for complex localization tasks where supervised models remain a strong alternative.

</details>


### [10] [STORM: Segment, Track, and Object Re-Localization from a Single 3D Model](https://arxiv.org/abs/2511.09771)
*Yu Deng,Teng Cao,Hikaru Shindo,Jiahong Xue,Quentin Delfosse,Kristian Kersting*

Main category: cs.CV

TL;DR: STORM是一个无需人工标注的6D姿态估计系统，通过结合视觉-语言理解和自监督特征匹配，实现了高精度和实时性，并能从遮挡或快速运动中恢复。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法依赖手动标注的分割掩码，劳动密集且在遮挡或快速运动时性能下降。

Method: STORM采用一个三阶段流程：利用上下文对象描述进行定位，使用自交叉注意力机制识别候选区域，并通过分割模型生成精确掩码进行姿态估计。此外，还包含一个自动重新注册机制，通过监控特征相似性来检测跟踪失败并从中恢复。

Result: STORM在具有多对象遮挡、高速运动和光照变化的工业数据集上实现了最先进的精度，同时保持实时运行速度，且无需额外训练。

Conclusion: STORM通过其无标注方法显著降低了部署成本，为灵活制造和智能质量控制等现代应用提供了实用的解决方案。

Abstract: Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with self-supervised feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and a segmentation model produces precise masks for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.

</details>


### [11] [PANDA - Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning](https://arxiv.org/abs/2511.09791)
*Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: EFCL在没有历史数据的情况下会出现灾难性遗忘，而现有的基于预训练模型（PTMs）的方法未能充分考虑真实世界数据分布的不平衡性。本文提出的PANDA框架通过增强低频类别和自适应平衡策略来解决这些问题，提高了准确性并减少了灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决现有PTM-based EFCL方法忽略真实世界数据分布双重不平衡性（数据集级别和任务内部的极端或反向偏斜）的问题，以应对学习和泛化方面的挑战。

Method: 提出PANDA（Patch-and-Distribution-Aware Augmentation）框架，包括：1. 使用CLIP编码器识别代表性区域并将其移植到高频类别样本中，以增强低频类别。2. 采用自适应平衡策略，利用先前任务的分布来平滑任务间不平衡，缩小任务间平均样本的差距。

Result: PANDA能够与现有的PTM-based CL方法无缝集成，通过实验证明了其提高准确性和减少灾难性遗忘的能力。消融研究进一步验证了其有效性。

Conclusion: PANDA是一个有效的框架，可以与现有PTM-based CL方法结合使用，以解决真实世界数据不平衡问题，从而提高持续学习的性能。

Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.

</details>


### [12] [Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2511.09809)
*Konstantinos M. Dafnis,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: STS是一种轻量级的视觉-语言模型（VLM）测试时自适应框架，通过提取文本嵌入的频谱子空间来定义主要的语义方向，并以感知频谱的方式学习在潜在空间中引导潜在表示，而无需反向传播或修改冻结的编码器。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型（VLM）在零样本推理方面表现出色，但在测试时领域转移下性能会下降。虽然现有的适应策略（例如，测试时提示调整）已被证明是有效的，但它们通常需要通过大型编码器权重进行反向传播或修改核心模型组件。

Method: STS框架通过提取文本嵌入的频谱子空间来定义主要的语义方向，并学习以感知频谱的方式引导潜在表示，通过调整少量每样本的移位参数来最小化增强视图之间的熵。STS完全在推理时在潜在空间中运行，无需反向传播或修改冻结的编码器。

Result: 在标准评估协议的基础上，STS在很大程度上超越或媲美了最先进的测试时自适应方法，同时仅引入少量额y外参数，推理速度比传统的测试时提示调整快8倍，内存占用小12倍。

Conclusion: STS是一种轻量级、高效的VLM测试时自适应框架，它在不修改或反向传播冻结编码器的情况下，通过在潜在空间中进行操作来有效处理领域转移问题。

Abstract: Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.

</details>


### [13] [AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting](https://arxiv.org/abs/2511.09827)
*Aymen Mir,Jian Wang,Riza Alp Guler,Chuan Guo,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: 我们提出了一个新颖的框架，利用3D高斯泼溅（3DGS）在3D场景中为人类制作动画。3DGS是一种神经场景表示，最近在新的视图合成方面取得了最先进的照片级效果，但在人类-场景动画和交互方面的探索尚不充分。我们的方法将3DGS作为3D表示引入到场景中为人类制作动画的问题中，从而实现与3D场景交互的人类的几何一致的自由视点渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的动画流程使用网格或点云作为底层的3D表示，但对于人类-场景动画和交互的探索尚不充分。我们的方法引入3DGS作为3D表示来解决这个问题。

Method: 我们的核心是将渲染与运动合成分离，允许每个子问题独立解决。我们使用基于不透明度的线索和投影的高斯结构来指导人类的位置和姿势对齐，从而合成运动。我们还提出了一种人类-场景高斯细化优化，以实现逼真的接触和导航。

Result: 我们在Scannet++和SuperSplat库的场景以及从稀疏和密集的多视角人类捕获重建的化身上评估了我们的方法。我们展示了我们的框架可以实现新的应用程序，例如对具有新动画人类的编辑单目RGB视频进行几何一致的自由视点渲染。

Conclusion: 我们的框架利用3DGS在3D场景中为人类制作动画，实现了几何一致的自由视点渲染，并为单目视频驱动的人类动画带来了新的可能性。

Abstract: We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.

</details>


### [14] [CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage](https://arxiv.org/abs/2511.09834)
*Xuntao Lyu,Ching-Chi Lin,Abdullah Al Arafat,Georg von der Brüggen,Jian-Jia Chen,Zhishan Guo*

Main category: cs.CV

TL;DR: CertMask是一种高效且可证明的鲁棒防御方法，通过生成一组二元掩码来消除对抗性斑块攻击对图像的影响，其推理成本为O(n)，而现有方法PatchCleanser的成本为O(n^2)。


<details>
  <summary>Details</summary>
Motivation: 对抗性斑块攻击能够物理部署并对现实世界的应用构成严重威胁，因此需要开发鲁棒的防御机制。

Method: 提出CertMask，一种利用数学上严谨的覆盖策略生成一组二元掩码的防御方法，以确保每个可能的斑块位置至少被覆盖k次，从而实现高效和鲁棒性。

Result: CertMask在ImageNet、ImageNette和CIFAR-10数据集上进行了实验，结果显示，在保持与原始模型几乎相同的准确率的同时，CertMask将认证鲁棒准确率在某些情况下提高了13.4%，优于PatchCleanser。

Conclusion: CertMask提供了一种可证明的鲁棒防御方法，能够有效地抵御对抗性斑块攻击，在效率和鲁棒性方面均优于现有技术。

Abstract: Adversarial patch attacks inject localized perturbations into images to mislead deep vision models. These attacks can be physically deployed, posing serious risks to real-world applications. In this paper, we propose CertMask, a certifiably robust defense that constructs a provably sufficient set of binary masks to neutralize patch effects with strong theoretical guarantees. While the state-of-the-art approach (PatchCleanser) requires two rounds of masking and incurs $O(n^2)$ inference cost, CertMask performs only a single round of masking with $O(n)$ time complexity, where $n$ is the cardinality of the mask set to cover an input image. Our proposed mask set is computed using a mathematically rigorous coverage strategy that ensures each possible patch location is covered at least $k$ times, providing both efficiency and robustness. We offer a theoretical analysis of the coverage condition and prove its sufficiency for certification. Experiments on ImageNet, ImageNette, and CIFAR-10 show that CertMask improves certified robust accuracy by up to +13.4\% over PatchCleanser, while maintaining clean accuracy nearly identical to the vanilla model.

</details>


### [15] [VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2511.10203)
*Stephane Da Silva Martins,Emanuel Aldea,Sylvie Le Hégarat-Mascle*

Main category: cs.CV

TL;DR: VISTA是一个用于多智能体轨迹预测的循环、以目标为导向的Transformer模型。它通过结合长期意图、过去运动、社交代币注意力和成对注意力图来改进预测，从而实现更真实的未来轨迹，并减少碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同时捕捉智能体的长期目标和细粒度的社交互动方面存在不足，导致预测的未来不切实际。

Method: 提出了一种名为VISTA的递归、以目标为导向的Transformer模型。该模型包含一个交叉注意融合模块，用于整合长期意图和过去运动；一个社交代币注意机制，用于对智能体之间的互动进行灵活建模；以及成对注意力图，用于在推理时解释社交影响模式。

Result: 在MADRAS和SDD数据集上，VISTA在准确性和减少碰撞方面均取得了最先进的成果。它将MADRAS数据集上的平均碰撞率从2.14%大幅降低至0.03%，并在SDD数据集上实现了零碰撞，同时提高了ADE、FDE和minFDE指标。

Conclusion: VISTA能够生成符合社交规范、目标明确且可解释的轨迹，对于安全关键的自主系统具有重要应用前景。

Abstract: Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.

</details>


### [16] [MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation](https://arxiv.org/abs/2511.10376)
*Xun Huang,Shijia Zhao,Yunxiang Wang,Xin Lu,Wanfa Zhang,Rongsheng Qu,Weixin Li,Yunhong Wang,Chenglu Wen*

Main category: cs.CV

TL;DR: MSGNav是一个创新的实体导航系统，通过引入多模态3D场景图（M3DSG）解决了现有零样本方法的局限性，实现了开放词汇泛化和低训练开销。


<details>
  <summary>Details</summary>
Motivation: 现有零样本实体导航方法在处理开放词汇泛化和低训练开销方面存在不足，主要表现在将丰富的视觉信息压缩成纯文本关系，导致高昂的构建成本、视觉信息丢失以及受限的词汇。现有方法需要领域特定的强化学习训练，不适用于真实世界的部署。

Method: 提出多模态3D场景图（M3DSG），用动态图像替换文本关系边，保留视觉线索。在此基础上，开发了一种名为MSGNav的零样本导航系统，包含关键子图选择模块（用于高效推理）、自适应词汇更新模块（支持开放词汇）和闭环推理模块（用于准确的探索推理）。此外，还提出了基于可见性的视点决策模块，以解决零样本导航中的“最后一英里”问题（确定可行的目标位置和合适的最终视点）。

Result: MSGNav在GOAT-Bench和HM3D-OVON数据集上取得了最先进的性能。

Conclusion: MSGNav通过引入M3DSG和一系列创新模块，有效地解决了现有零样本导航方法的痛点，在开放词汇泛化、低训练开销和导航精度方面取得了显著的进步，并在基准测试中展现了优越的性能。

Abstract: Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.

</details>


### [17] [IPCD: Intrinsic Point-Cloud Decomposition](https://arxiv.org/abs/2511.09866)
*Shogo Sato,Takuhiro Kaneko,Shoichiro Takeda,Tomoyasu Shimada,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida,Akisato Kimura*

Main category: cs.CV

TL;DR: 该论文提出了一种名为IPCD的新方法，用于将彩色点云分解为反照率和阴影，以解决AR和机器人等领域中的真实感可视化问题。


<details>
  <summary>Details</summary>
Motivation: 点云数据的真实感可视化需要分离反照率和阴影，但点云的非网格结构和现有模型未明确考虑全局光照方向带来了挑战。

Method: 提出IPCD-Net，通过点式特征聚合处理非网格数据，并引入基于投影的亮度分布（PLD）及分层特征细化来捕捉全局光照信息。

Result: IPCD-Net能减少反照率中的投影阴影，提高阴影中的颜色准确性，并在纹理编辑、重新照明和点云配准等应用中表现出色，验证了其在真实世界中的应用潜力。

Conclusion: IPCD-Net成功实现了点云的反照率和阴影分解，解决了现有方法的局限性，并展示了其在多个实际应用中的有效性。

Abstract: Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.

</details>


### [18] [SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.10518)
*Wei Li,Renshan Zhang,Rui Shao,Zhijian Fang,Kaiwen Zhou,Zhuotao Tian,Liqiang Nie*

Main category: cs.CV

TL;DR: SemanticVLA通过语义引导的双重视觉修剪器（SD-Pruner）和语义互补分层融合器（SH-Fuser）解决了机器人操作中感知冗余和指令-视觉对齐肤浅的问题，并通过语义条件动作耦合器（SA-Coupler）增强了感知到动作的转换，从而在性能和效率上都取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 实际部署机器人操作的Vision-Language-Action（VLA）模型受到感知冗余（处理不相关的视觉输入效率低下）和肤浅指令-视觉对齐（阻碍语义基础的动作）的限制。

Method: 提出了一种名为SemanticVLA的新型VLA框架，采用语义对齐的稀疏化和增强技术进行高效机器人操作。具体包括：1）通过指令驱动的修剪器（ID-Pruner）和空间聚合修剪器（SA-Pruner）来实现语义引导的双重视觉修剪（SD-Pruner），以去除冗余感知并保持语义对齐。2）通过语义互补分层融合器（SH-Fuser）融合来自SigLIP和DINOv2的密集斑块和稀疏标记，以利用稀疏化特征并整合语义与空间几何。3）通过语义条件动作耦合器（SA-Coupler）取代传统的观察到自由度（DoF）的方法，以增强从感知到动作的转换，从而实现更高效、可解释的操作任务行为建模。

Result: 在模拟和真实世界的任务中进行了广泛的实验，结果表明SemanticVLA在性能和效率方面均达到了新的SOTA水平。在LIBERO基准测试中，SemanticVLA的成功率比OpenVLA高出21.1%，同时训练成本和推理延迟分别降低了3.0倍和2.7倍。

Conclusion: SemanticVLA通过其创新的SD-Pruner、SH-Fuser和SA-Coupler组件，成功解决了现有VLA模型在机器人操作中的关键挑战，实现了显著的性能提升和效率改进，并在实际应用中显示出巨大潜力。

Abstract: Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA

</details>


### [19] [Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies](https://arxiv.org/abs/2511.09868)
*Peng Gao,Yujian Lee,Xiaofeng Zhang,Zailong Chen,Hui Zhang*

Main category: cs.CV

TL;DR: ROPE在长序列建模中存在注意力衰减问题。提出T-DRS（包括SD-DRS、DC-DRS、reRD-DRS）在推理时解决此问题，通过内容感知、距离感知和重强化机制来恢复被抑制的长距离依赖，同时保留局部归纳偏置。


<details>
  <summary>Details</summary>
Motivation: ROPE在长序列建模中存在注意力衰减问题，导致模型难以记住全局上下文。

Method: 提出推理时即可使用的T-DRS（Three-step Decay Resilience Strategies），包括：1. SD-DRS（Semantic-Driven DRS）：放大有意义的远距离信号。2. DC-DRS（Distance-aware Control DRS）：根据距离平滑调制注意力权重。3. reRD-DRS（re-Reinforce Distant DRS）：巩固剩余的远距离依赖。

Result: 在VQA基准测试中，T-DRS在不损害局部归纳偏置的情况下，一致地提高了性能。

Conclusion: T-DRS是一种有效的、无需训练的策略，可以解决LVLM中的长距离依赖建模问题，提高VQA性能。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive performance across a wide range of multimodal tasks. However, they still face critical challenges in modeling long-range dependencies under the usage of Rotary Positional Encoding (ROPE). Although it can facilitate precise modeling of token positions, it induces progressive attention decay as token distance increases, especially with progressive attention decay over distant token pairs, which severely impairs the model's ability to remember global context. To alleviate this issue, we propose inference-only Three-step Decay Resilience Strategies (T-DRS), comprising (1) Semantic-Driven DRS (SD-DRS), amplifying semantically meaningful but distant signals via content-aware residuals, (2) Distance-aware Control DRS (DC-DRS), which can purify attention by smoothly modulating weights based on positional distances, suppressing noise while preserving locality, and (3) re-Reinforce Distant DRS (reRD-DRS), consolidating the remaining informative remote dependencies to maintain global coherence. Together, the T-DRS recover suppressed long-range token pairs without harming local inductive biases. Extensive experiments on Vision Question Answering (VQA) benchmarks demonstrate that T-DRS can consistently improve performance in a training-free manner. The code can be accessed in https://github.com/labixiaoq-qq/Remember-me

</details>


### [20] [SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection](https://arxiv.org/abs/2511.09870)
*Jia Lin,Xiaofei Zhou,Jiyuan Liu,Runmin Cong,Guodao Zhang,Zhi Liu,Jiyong Zhang*

Main category: cs.CV

TL;DR: SAM-DAQ通过结合深度和时间线索，解决了SAM在RGB-D视频显著性目标检测中的局限性，实现了更优的性能。


<details>
  <summary>Details</summary>
Motivation: 直接将SAM应用于RGB-D视频显著性目标检测会遇到手动提示依赖、序列适配器内存消耗高和内存注意力计算负担重的问题，需要新的方法来解决这些挑战。

Method: 提出SAM-DAQ方法，包含并行适配器多模态图像编码器（PAMIE）和查询驱动时序内存（QTM）模块。PAMIE利用深度引导的并行适配器（DPA）融合多模态特征，并在无提示条件下微调冻结的SAM编码器。QTM模块统一内存库和提示嵌入，同时利用帧级和视频级查询来提取时序一致性特征并迭代更新查询的时序表示。

Result: 在三个RGB-D VSOD数据集上的广泛实验表明，SAM-DAQ在所有评估指标上始终优于最先进的方法。

Conclusion: SAM-DAQ成功地适应了SAM模型，通过无缝集成深度和时间线索，在RGB-D视频显著性目标检测任务中有效解决了现有方法的局限性，并取得了优越的性能。

Abstract: Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.

</details>


### [21] [HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models](https://arxiv.org/abs/2511.09883)
*Liheng Zhang,Jin Wang,Hui Li,Bingfeng Zhang,Weifeng Liu*

Main category: cs.CV

TL;DR: HCC-3D通过全局结构压缩（GSC）和自适应细节挖掘（ADM）模块，在大幅降低3D-VLMs计算开销（约98%压缩率）的同时，保持了关键信息的完整性，并在效率和性能上取得了新的SOTA成果。


<details>
  <summary>Details</summary>
Motivation: 目前的3D-VLMs直接将3D点云嵌入3D标记，计算成本高，LLM部分是瓶颈。如何减少3D标记的计算开销并保留关键信息是需要解决的问题。

Method: 提出了一种名为HCC-3D（Hierarchical Compensatory Compression for 3D）的方法。该方法首先通过全局结构压缩（GSC）模块，利用全局查询将所有3D标记压缩成少数几个关键标记，保留整体结构信息。然后，通过自适应细节挖掘（ADM）模块，选择性地重新压缩有信息损失但重要的特征，以弥补GSC造成的损失。

Result: HCC-3D实现了约98%的极端压缩率，同时在效率和性能上均取得了新的SOTA表现。

Conclusion: HCC-3D在效率和性能上都取得了显著的提升，证明了该方法在3D-VLMs领域的有效性。

Abstract: 3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.

</details>


### [22] [Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2511.09891)
*Jinfu Li,Yuqi Huang,Hong Song,Ting Wang,Jianghan Xia,Yucong Lin,Jingfan Fan,Jian Yang*

Main category: cs.CV

TL;DR: 现有的目标检测器在检测小目标时性能不佳，主要因为小目标特征信息有限且在训练中受不公平的回归损失惩罚。本文提出了尺度感知中继层（SARL）和尺度自适应损失（SAL）来解决这些问题。SARL通过跨尺度空间通道注意力来增强特征表示和跨层特征共享。SAL则调整损失函数，降低大目标权重，使训练更侧重于小目标。实验证明，该方法在YOLOv5和YOLOx等框架上能显著提升小目标检测精度，在AI-TOD、DOTA-v2.0和VisDrone2019等数据集上表现优异，并在AI-TOD-v2.0数据集上取得了29.0%的AP。


<details>
  <summary>Details</summary>
Motivation: 现有的目标检测器在检测小目标时性能不佳，主要因为小目标特征信息有限且在训练中受不公平的回归损失惩罚。

Method: 提出尺度感知中继层（SARL）和尺度自适应损失（SAL）。SARL采用跨尺度空间通道注意力增强特征表示和跨层特征共享。SAL调整损失函数，降低大目标权重。

Result: 在AI-TOD、DOTA-v2.0和VisDrone2019数据集上，将SARL和SAL嵌入YOLOv5和YOLOx框架后，平均精度（AP）提升了5.5%。在AI-TOD-v2.0数据集上，AP达到了29.0%。

Conclusion: 所提出的SARL和SAL方法能够有效提升小目标检测的性能和泛化能力。

Abstract: Recently, despite the remarkable advancements in object detection, modern detectors still struggle to detect tiny objects in aerial images. One key reason is that tiny objects carry limited features that are inevitably degraded or lost during long-distance network propagation. Another is that smaller objects receive disproportionately greater regression penalties than larger ones during training. To tackle these issues, we propose a Scale-Aware Relay Layer (SARL) and a Scale-Adaptive Loss (SAL) for tiny object detection, both of which are seamlessly compatible with the top-performing frameworks. Specifically, SARL employs a cross-scale spatial-channel attention to progressively enrich the meaningful features of each layer and strengthen the cross-layer feature sharing. SAL reshapes the vanilla IoU-based losses so as to dynamically assign lower weights to larger objects. This loss is able to focus training on tiny objects while reducing the influence on large objects. Extensive experiments are conducted on three benchmarks (\textit{i.e.,} AI-TOD, DOTA-v2.0 and VisDrone2019), and the results demonstrate that the proposed method boosts the generalization ability by 5.5\% Average Precision (AP) when embedded in YOLOv5 (anchor-based) and YOLOx (anchor-free) baselines. Moreover, it also promotes the robust performance with 29.0\% AP on the real-world noisy dataset (\textit{i.e.,} AI-TOD-v2.0).

</details>


### [23] [Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning](https://arxiv.org/abs/2511.09893)
*Zubia Naz,Farhan Asghar,Muhammad Ishfaq Hussain,Yahya Hadadi,Muhammad Aasim Rafique,Wookjin Choi,Moongu Jeon*

Main category: cs.CV

TL;DR: Swin-BART编码器-解码器系统结合区域注意力模块，在ROCO数据集上实现了最先进的医学图像描述生成，提高了ROUGE和BERTScore分数，并能可视化解释。


<details>
  <summary>Details</summary>
Motivation: 开发能够诊断性地解释复杂放射学图像的自动化医学图像描述生成系统，以支持报告工作流程。

Method: 使用Swin-BART编码器-解码器系统，并加入一个轻量级区域注意力模块，该模块在交叉注意力之前增强了诊断上的显著区域。模型在ROCO数据集上进行了训练和评估。

Result: 在ROCO数据集上，该模型在ROUGE（0.603）和BERTScore（0.807）方面取得了最先进的性能，优于基线模型。同时，在BLEU、CIDEr和METEOR方面也具有竞争力。提供了消融研究、按模态分析、显著性检验和可视化热图。

Conclusion: 所提出的方法能够生成准确、临床措辞的医学图像描述，并提供透明的区域归因，支持在有人工参与的循环中安全使用。

Abstract: Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\_repeat\_ngram\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.

</details>


### [24] [MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding](https://arxiv.org/abs/2511.09919)
*Ketong Chen,Yuhao Chen,Yang Xue*

Main category: cs.CV

TL;DR: 现有的视觉语言模型（VLM）基准测试在评估复杂文档布局和多语言理解方面存在不足。为解决此问题，我们提出了DocWeaver，一个利用大型语言模型自动生成新基准测试的多智能体流程。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估视觉丰富文档理解（VRDU）模型，尤其是在处理复杂布局和多语言文档时。

Method: 利用大型语言模型（LLM）通过DocWeaver多智能体流程自动生成名为MosaicDoc的新基准测试。

Result: MosaicDoc是一个大规模、双语（中英）的VRDU基准测试，包含72K图像和600K以上问答对，覆盖多样化的布局和多任务（OCR、VQA、阅读顺序、定位），并评估了现有模型的局限性。

Conclusion: MosaicDoc为VRDU领域提供了一个更全面的基准，并指明了未来研究方向，以应对真实世界文档的复杂性。

Abstract: Despite the rapid progress of Vision-Language Models (VLMs), their capabilities are inadequately assessed by existing benchmarks, which are predominantly English-centric, feature simplistic layouts, and support limited tasks. Consequently, they fail to evaluate model performance for Visually Rich Document Understanding (VRDU), a critical challenge involving complex layouts and dense text. To address this, we introduce DocWeaver, a novel multi-agent pipeline that leverages Large Language Models to automatically generate a new benchmark. The result is MosaicDoc, a large-scale, bilingual (Chinese and English) resource designed to push the boundaries of VRDU. Sourced from newspapers and magazines, MosaicDoc features diverse and complex layouts (including multi-column and non-Manhattan), rich stylistic variety from 196 publishers, and comprehensive multi-task annotations (OCR, VQA, reading order, and localization). With 72K images and over 600K QA pairs, MosaicDoc serves as a definitive benchmark for the field. Our extensive evaluation of state-of-the-art models on this benchmark reveals their current limitations in handling real-world document complexity and charts a clear path for future research.

</details>


### [25] [Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers](https://arxiv.org/abs/2511.09926)
*Xuan Rao,Simian Xu,Zheng Li,Bo Zhao,Derong Liu,Mingming Ha,Cesare Alippi*

Main category: cs.CV

TL;DR: SeqFT在类别增量学习（CIL）中虽然有效，但容易受到由共享骨干参数顺序优化引起的分布漂移影响。本文提出了“具有漂移补偿的顺序学习”（SLDC）来解决这个问题，其通过引入潜在空间转换算子来对齐任务间的特征分布。SLDC包括一个线性变体和一个弱非线性变体。此外，还结合了知识蒸馏（KD）来进一步减少表示漂移。实验表明，SLDC显著提高了SeqFT的性能，并且结合KD后，SeqFT的性能可与联合训练相媲美。


<details>
  <summary>Details</summary>
Motivation: SeqFT方法在类别增量学习（CIL）中虽然有效，但共享骨干参数的顺序优化会导致分布漂移，从而降低分类器性能。

Method: 提出了一种潜在空间转换算子，并设计了“具有漂移补偿的顺序学习”（SLDC）方法。SLDC包含一个线性变体（通过求解正则化最小二乘问题学习映射）和一个弱非线性变体（使用可学习的弱非线性映射）。此外，在两种变体中都应用了知识蒸馏（KD）来减少表示漂移。

Result: 在标准的CIL基准测试上的广泛实验表明，SLDC显著提高了SeqFT的性能。特别地，通过结合KD和SLDC，SeqFT在所有评估的数据集上的性能都可与联合训练相媲美。

Conclusion: SLDC通过对齐任务间的特征分布，有效缓解了SeqFT中的分布漂移问题，并结合KD进一步减少了表示漂移，从而在类别增量学习任务中取得了与联合训练相当的性能。

Abstract: Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.

</details>


### [26] [Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification](https://arxiv.org/abs/2511.09933)
*Yuhang Zhou,Yanxiang Zhao,Zhongyun Hua,Zhipu Liu,Zhaoquan Gu,Qing Liao,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 深度学习ReID模型易受对抗攻击，现有防御方法存在不足。本文提出一种去偏置双不变性防御框架，通过数据重采样缓解模型偏置，并采用新颖的度量对抗训练和对抗性增强自元机制来提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型易受对抗攻击，且现有防御方法未能解决ReID特有的挑战，如模型偏置和复合泛化要求。

Method: 提出一个包含数据平衡和双对抗自元防御两个阶段的框架。数据平衡阶段使用基于扩散模型的数据重采样策略来缓解模型偏置。双对抗自元防御阶段引入度量对抗训练（包含最远负例扩展软化）和对抗性增强自元机制，以应对无分类器导致的鲁棒性下降，并实现对未知身份和未知攻击类型的双泛化。

Result: 实验证明该方法显著优于现有的最先进防御方法。

Conclusion: 所提出的去偏置双不变性防御框架能有效应对ReID中的模型偏置和复合泛化挑战，显著提高ReID模型的对抗鲁棒性。

Abstract: Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.

</details>


### [27] [AdaptViG: Adaptive Vision GNN with Exponential Decay Gating](https://arxiv.org/abs/2511.09942)
*Mustafa Munir,Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: AdaptViG是一种高效的混合视觉GNN，通过自适应图卷积（结合静态轴向支架和动态指数衰减门控）解决了ViG的计算挑战，并在早期使用门控机制，后期使用全局注意力，实现了准确性和效率的最优平衡。


<details>
  <summary>Details</summary>
Motivation: 解决Vision Graph Neural Networks（ViGs）在图构建阶段面临substantial计算挑战，进而影响效率的问题。

Method: 提出了一种名为自适应图卷积（Adaptive Graph Convolution）的新颖图构建机制，它结合了一个高效的静态轴向支架（static axial scaffold）和一个动态的、感知内容的门控策略（Exponential Decay Gating），该门控机制根据特征相似性选择性地加权长距离连接。AdaptViG采用了混合策略，在早期阶段使用高效的门控机制，在最后阶段使用全局注意力（Global Attention）块以实现最大的特征聚合。

Result: AdaptViG在准确性和效率方面取得了新的state-of-the-art trade-off。例如，AdaptViG-M达到了82.6%的top-1准确率，比ViG-B高0.3%，但参数数量减少了80%，GMACs减少了84%。在下游任务中，AdaptViG-M获得了45.8 mIoU、44.8 APbox和41.1 APmask，优于EfficientFormer-L7，但参数数量减少了78%。

Conclusion: AdaptViG通过其新颖的自适应图卷积机制，在保持或提高准确性的同时，显著提高了Vision GNN的效率，并在准确性和效率之间取得了优于现有方法的平衡。

Abstract: Vision Graph Neural Networks (ViGs) offer a new direction for advancements in vision architectures. While powerful, ViGs often face substantial computational challenges stemming from their graph construction phase, which can hinder their efficiency. To address this issue we propose AdaptViG, an efficient and powerful hybrid Vision GNN that introduces a novel graph construction mechanism called Adaptive Graph Convolution. This mechanism builds upon a highly efficient static axial scaffold and a dynamic, content-aware gating strategy called Exponential Decay Gating. This gating mechanism selectively weighs long-range connections based on feature similarity. Furthermore, AdaptViG employs a hybrid strategy, utilizing our efficient gating mechanism in the early stages and a full Global Attention block in the final stage for maximum feature aggregation. Our method achieves a new state-of-the-art trade-off between accuracy and efficiency among Vision GNNs. For instance, our AdaptViG-M achieves 82.6% top-1 accuracy, outperforming ViG-B by 0.3% while using 80% fewer parameters and 84% fewer GMACs. On downstream tasks, AdaptViG-M obtains 45.8 mIoU, 44.8 APbox, and 41.1 APmask, surpassing the much larger EfficientFormer-L7 by 0.7 mIoU, 2.2 APbox, and 2.1 APmask, respectively, with 78% fewer parameters.

</details>


### [28] [TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.09944)
*Zhiyuan Xu,Nan Min,Yuhang Guo,Tong Wei*

Main category: cs.CV

TL;DR: 3D高斯放射（3D Gaussian Splatting）在处理半透明表面时存在局限，因为大多数方法假设每个像素只有一个深度，这在有多个表面可见时会失效。本文提出了TSPE-GS（Transparent Surface Probabilistic Extraction for Gaussian Splatting），通过均匀采样透射率来对每个像素的不透明度和深度进行多模态分布建模，取代了先前单一峰值的假设，并解决了跨表面深度模糊的问题。通过逐步融合截断符号距离函数，TSPE-GS在统一的框架内分别重建外部和内部表面。该方法无需额外的训练开销即可推广到其他基于高斯的方法。在公共和自收集的半透明及不透明数据集上的广泛实验表明，TSPE-GS在保持不透明场景性能的同时，显著提高了半透明几何重建的质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯放射方法在处理半透明表面时存在困难，主要原因是其像素深度单一的假设无法处理多表面可见的情况。

Method: TSPE-GS通过均匀采样透射率来建模像素的深度和不透明度，采用多模态分布假设，并结合截断符号距离函数逐步融合，以分别重建内外表面。

Result: TSPE-GS方法在半透明数据集上显著提高了几何重建质量，同时在不透明场景上的性能保持不变。

Conclusion: TSPE-GS能够有效解决半透明表面的重建问题，并在不透明场景上保持良好性能，且易于集成到现有框架中。

Abstract: 3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.

</details>


### [29] [Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment](https://arxiv.org/abs/2511.09948)
*Zhicheng Liao,Dongxu Wu,Zhenshan Shi,Sijie Mai,Hanwei Zhu,Lingyu Zhu,Yuncheng Jiang,Baoliang Chen*

Main category: cs.CV

TL;DR: CLIP模型可用于图像质量评估，但其基于余弦相似度的语义匹配忽略了图像特征的幅度信息。本文提出一种自适应融合框架，结合了余弦相似度和幅度感知质量线索，通过Box-Cox变换进行统计归一化，并使用置信度引导的融合策略，在多个IQA数据集上取得了优于SOTA方法的性能，且无需任务特定训练。


<details>
  <summary>Details</summary>
Motivation: 利用CLIP模型进行图像质量评估时，其语义相似度忽略了图像特征的幅度信息，而该信息与感知质量高度相关。

Method: 提出一种自适应融合框架，结合了基于余弦相似度的语义线索和基于图像特征幅度的质量线索。具体方法包括：1. 提取绝对CLIP图像特征；2. 应用Box-Cox变换对特征进行统计归一化；3. 设计置信度引导的融合策略，自适应地加权两个线索。

Result: 在多个IQA数据集上，所提出的方法在没有任务特定训练的情况下，持续优于标准的基于CLIP的IQA方法和最先进的基线方法。

Conclusion: 本文提出的自适应融合框架通过结合语义线索和幅度感知质量线索，有效提升了基于CLIP的图像质量评估性能，展示了幅度信息在IQA中的重要性。

Abstract: Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.

</details>


### [30] [Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching](https://arxiv.org/abs/2511.09955)
*Uday Bhaskar,Rishabh Bhattacharya,Avinash Patel,Sarthak Khoche,Praveen Anil Kulkarni,Naresh Manwani*

Main category: cs.CV

TL;DR: 利用VLMs生成伪标签，并通过改进的Co-teaching训练策略来训练高效的实时目标检测器，以解决自动驾驶领域中的标注成本高和模型推理慢的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域手动标注成本高，且现有VLM零样本检测存在延迟和幻觉问题，不适用于直接部署。

Method: 提出一个新颖的流水线，利用VLMs自动生成伪标签，并采用一种新颖的逐对象Co-teaching训练策略来解决VLM生成标签中的噪声问题。该策略通过比较两个YOLO模型在每个小批量中逐对象的损失值来协同过滤不可靠的边界框，而不是过滤整个图像。

Result: 在KITTI数据集上，该方法将mAP@0.5从31.12%提升至46.61%，同时保持了实时检测的低延迟。使用10%的真实标签进行补充训练，mAP@0.5进一步提升至57.97%。在ACDC和BDD100k数据集上也有类似性能提升。

Conclusion: 该流水线提供了一种高效、鲁棒且可扩展的方法，用于训练高性能的自动驾驶目标检测器，显著降低了对昂贵的人工标注的依赖。

Abstract: Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\%$ to $46.61\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\%$) leads to further performance gains, reaching $57.97\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.

</details>


### [31] [Equivariant Sampling for Improving Diffusion Model-based Image Restoration](https://arxiv.org/abs/2511.09965)
*Chenxu Wu,Qingpeng Kong,Peiang Zhao,Wendi Yang,Wenxin Ma,Fenghe Tang,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 最近的生成模型（尤其是扩散模型）在图像恢复（IR）方面取得了显著进展。然而，现有的与问题无关的基于扩散模型的图像恢复（DMIR）方法在充分利用扩散先验方面存在挑战，导致性能不佳。本文通过分析现有与问题无关的DMIR方法的采样过程并提供有效的解决方案，来解决这些方法的局限性。我们引入了EquS，一种通过双重采样轨迹施加等变信息的DMIR方法。为了进一步增强EquS，我们提出了时间步感知调度（TAS），并引入了EquS$^+$。TAS优先考虑确定性步骤，以增强确定性和采样效率。在基准测试上的大量实验表明，我们的方法与以前的与问题无关的DMIR方法兼容，并且在不增加计算成本的情况下显著提高了它们的性能。代码可在https://github.com/FouierL/EquS获取。


<details>
  <summary>Details</summary>
Motivation: 解决现有与问题无关的基于扩散模型的图像恢复（DMIR）方法在充分利用扩散先验方面存在挑战，导致性能不佳的问题。

Method: 提出EquS，一种通过双重采样轨迹施加等变信息的DMIR方法，并提出时间步感知调度（TAS）来增强EquS，引入EquS$^+$。

Result: 在基准测试上的大量实验表明，EquS$^+$与以前的与问题无关的DMIR方法兼容，并且在不增加计算成本的情况下显著提高了它们的性能。

Conclusion: EquS$^+$是一种有效的方法，可以提高与问题无关的DMIR方法的性能。

Abstract: Recent advances in generative models, especially diffusion models, have significantly improved image restoration (IR) performance. However, existing problem-agnostic diffusion model-based image restoration (DMIR) methods face challenges in fully leveraging diffusion priors, resulting in suboptimal performance. In this paper, we address the limitations of current problem-agnostic DMIR methods by analyzing their sampling process and providing effective solutions. We introduce EquS, a DMIR method that imposes equivariant information through dual sampling trajectories. To further boost EquS, we propose the Timestep-Aware Schedule (TAS) and introduce EquS$^+$. TAS prioritizes deterministic steps to enhance certainty and sampling efficiency. Extensive experiments on benchmarks demonstrate that our method is compatible with previous problem-agnostic DMIR methods and significantly boosts their performance without increasing computational costs. Our code is available at https://github.com/FouierL/EquS.

</details>


### [32] [Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models](https://arxiv.org/abs/2511.09973)
*Satoshi Suzuki,Shin'ya Yamaguchi,Shoichiro Takeda,Taiga Yamane,Naoki Makishima,Naotaka Kawata,Mana Ihori,Tomohiro Tanaka,Shota Orihashi,Ryo Masumura*

Main category: cs.CV

TL;DR: DiVE通过约束差分向量来在微调过程中保持视觉-语言模型的嵌入几何结构，从而在ID、OOD和零样本设置下都取得优异的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的鲁棒微调方法在微调时复用对比学习，会破坏嵌入的几何结构，从而限制了OOD和零样本性能。

Method: 提出差分向量均衡（DiVE），通过约束差分向量（微调前后模型对同一数据样本的嵌入之差）来在微调过程中保持几何结构。引入平均向量损失（AVL）和成对向量损失（PVL）来实现。

Result: DiVE能够有效保持几何结构，并在ID、OOD和零样本指标上都取得了强有力的结果。

Conclusion: DiVE能够有效保持嵌入的几何结构，显著提升了视觉-语言模型在ID、OOD和零样本场景下的性能。

Abstract: Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.

</details>


### [33] [STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data](https://arxiv.org/abs/2511.09977)
*Yongdeuk Seo,Hyun-seok Min,Sungchul Choi*

Main category: cs.CV

TL;DR: STELLAR是一个用于低资源语言和真实世界数据的场景文本编辑器，解决了现有方法的局限性，并提出了新的数据集STIPLAR和评估指标TAS。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的场景文本编辑方法在低资源语言、合成数据与真实数据之间的域差距以及文本风格保持评估指标方面存在不足。

Method: STELLAR采用语言自适应字形编码器和多阶段训练策略（先在合成数据上预训练，然后在真实图像上微调），并提出了新的数据集STIPLAR和评估指标TAS（文本外观相似度）。

Result: STELLAR在视觉一致性和识别准确性方面优于现有方法，在跨语言的平均TAS得分上比基线提高了2.2%。

Conclusion: STELLAR能够可靠地进行多语言编辑，并且TAS指标能够有效地评估在没有真实标签的情况下文本风格的保持情况。

Abstract: Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.

</details>


### [34] [MOBA: A Material-Oriented Backdoor Attack against LiDAR-based 3D Object Detection Systems](https://arxiv.org/abs/2511.09999)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Pan He,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MOBA的新型框架，用于解决激光雷达3D目标检测中的后门攻击的物理实现问题，通过模拟真实世界触发器的材料特性来弥合数字-物理鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击因忽视激光雷达回波的材料依赖性而缺乏物理可实现性，MOBA旨在解决数字-物理域的差距，并设计更有效、可物理实现的后门触发器。

Method: MOBA框架通过系统化地选择鲁棒的触发器材料（选择二氧化钛）和开发新的模拟管线（包括角度无关的Oren-Nayar BRDF模型近似和距离感知缩放机制）来确保物理触发器的行为能够被数字模拟准确模仿。

Result: MOBA在最先进的激光雷达和激光雷达-摄像头融合模型上实现了93.50%的攻击成功率，相比先前的方法提高了41%以上。

Conclusion: MOBA揭示了一类新的物理可实现威胁，并强调了防御措施需要考虑现实环境中材料层面的属性的紧迫性。

Abstract: LiDAR-based 3D object detection is widely used in safety-critical systems. However, these systems remain vulnerable to backdoor attacks that embed hidden malicious behaviors during training. A key limitation of existing backdoor attacks is their lack of physical realizability, primarily due to the digital-to-physical domain gap. Digital triggers often fail in real-world settings because they overlook material-dependent LiDAR reflection properties. On the other hand, physically constructed triggers are often unoptimized, leading to low effectiveness or easy detectability.This paper introduces Material-Oriented Backdoor Attack (MOBA), a novel framework that bridges the digital-physical gap by explicitly modeling the material properties of real-world triggers. MOBA tackles two key challenges in physical backdoor design: 1) robustness of the trigger material under diverse environmental conditions, 2) alignment between the physical trigger's behavior and its digital simulation. First, we propose a systematic approach to selecting robust trigger materials, identifying titanium dioxide (TiO_2) for its high diffuse reflectivity and environmental resilience. Second, to ensure the digital trigger accurately mimics the physical behavior of the material-based trigger, we develop a novel simulation pipeline that features: (1) an angle-independent approximation of the Oren-Nayar BRDF model to generate realistic LiDAR intensities, and (2) a distance-aware scaling mechanism to maintain spatial consistency across varying depths. We conduct extensive experiments on state-of-the-art LiDAR-based and Camera-LiDAR fusion models, showing that MOBA achieves a 93.50% attack success rate, outperforming prior methods by over 41%. Our work reveals a new class of physically realizable threats and underscores the urgent need for defenses that account for material-level properties in real-world environments.

</details>


### [35] [LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers](https://arxiv.org/abs/2511.10004)
*Minjun Kim,Jaeri Lee,Jongjin Kim,Jeongin Yun,Yongmo Kwon,U Kang*

Main category: cs.CV

TL;DR: LampQ通过层级量化、类型感知Fisher度量以及整数线性规划优化比特分配，解决了现有ViT混合精度量化方法的局限性，实现了最先进的ViT量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViT量化方法依赖统一精度，忽略了不同组件对量化的敏感度差异。虽然MPQ有前景，但现有ViT MPQ方法存在粒度粗糙、度量尺度不匹配和量化感知比特分配缺失等问题。

Method: LampQ（Layer-wise Mixed Precision Quantization for Vision Transformers）提出了一种层级量化方法，结合类型感知Fisher度量来评估敏感度，并使用整数线性规划优化比特分配，最后进行迭代更新。

Result: LampQ在图像分类、目标检测和零样本量化等多种任务上预训练的ViT模型量化方面，取得了最先进的性能。

Conclusion: LampQ是一种精确的、基于度量的混合精度量化方法，克服了现有ViT MPQ方法的局限性，实现了对ViT模型的高效量化。

Abstract: How can we accurately quantize a pre-trained Vision Transformer model? Quantization algorithms compress Vision Transformers (ViTs) into low-bit formats, reducing memory and computation demands with minimal accuracy degradation. However, existing methods rely on uniform precision, ignoring the diverse sensitivity of ViT components to quantization. Metric-based Mixed Precision Quantization (MPQ) is a promising alternative, but previous MPQ methods for ViTs suffer from three major limitations: 1) coarse granularity, 2) mismatch in metric scale across component types, and 3) quantization-unaware bit allocation. In this paper, we propose LampQ (Layer-wise Mixed Precision Quantization for Vision Transformers), an accurate metric-based MPQ method for ViTs to overcome these limitations. LampQ performs layer-wise quantization to achieve both fine-grained control and efficient acceleration, incorporating a type-aware Fisher-based metric to measure sensitivity. Then, LampQ assigns bit-widths optimally through integer linear programming and further updates them iteratively. Extensive experiments show that LampQ provides the state-of-the-art performance in quantizing ViTs pre-trained on various tasks such as image classification, object detection, and zero-shot quantization.

</details>


### [36] [AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models](https://arxiv.org/abs/2511.10017)
*Xinyi Wang,Xun Yang,Yanlong Xu,Yuchen Wu,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一个名为AffordBot的新框架，用于解决精细的3D具身推理任务。该任务要求智能体根据指令预测3D场景中每个可操作元素的空间位置、运动类型和运动轴。AffordBot通过将多模态大语言模型（MLLMs）与定制的思维链（CoT）推理相结合，并利用渲染的环绕视图图像来弥合3D输入和2D MLLMs之间的差距，在SceneFun3D数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在物理环境中进行人机协作时，往往只处理物体级别的信息，或者孤立地处理细粒度的可操作性推理，缺乏与指令相关的连贯的定位和推理能力。

Method: 提出了一种名为AffordBot的新框架，该框架整合了多模态大语言模型（MLLMs）和定制的思维链（CoT）推理范式。为了解决3D输入和2D兼容的MLLMs之间的差距，论文渲染了场景的环绕视图图像，并将3D元素候选者投影到这些视图中，形成与场景几何对齐的丰富视觉表示。CoT流程首先进行主动感知，提示MLLM根据指令选择信息量最大的视点，然后进行逐步推理，以定位可操作元素并推断合理的交互动作。

Result: 在SceneFun3D数据集上的评估结果显示，AffordBot取得了最先进的性能，仅使用3D点云输入和MLLMs就展现了强大的泛化能力和基于物理的推理能力。

Conclusion: AffordBot通过结合MLLMs和CoT推理，解决了精细3D具身推理任务，实现了对物体可操作性元素的精确定位和交互动作推断，并在基准数据集上取得了优异表现。

Abstract: Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.

</details>


### [37] [Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation](https://arxiv.org/abs/2511.10020)
*Yuxin Jiang,Wei Luo,Hui Zhang,Qiyu Chen,Haiming Yao,Weiming Shen,Yunkang Cao*

Main category: cs.CV

TL;DR: Anomagic是一种零样本异常生成方法，通过跨模态提示编码统一视觉和文本线索，利用了丰富的上下文信息来指导基于修复的生成流程，并采用对比度细化策略来精确对齐生成的异常及其掩码，从而提高了异常检测的准确性。该方法还引入了一个包含12,987个异常-掩码-标题三元组的数据集AnomVerse。


<details>
  <summary>Details</summary>
Motivation: 目前的方法在生成语义上连贯的异常方面存在挑战，特别是需要示例异常。

Method: Anomagic通过跨模态提示编码统一视觉和文本线索，利用上下文信息来指导基于修复的生成流程，并采用对比度细化策略来精确对齐生成的异常及其掩码。此外，还构建了一个名为AnomVerse的数据集，用于训练。

Result: Anomagic在AnomVerse上训练后，能够生成比以前的方法更真实、更多样化的异常，并在下游异常检测任务中取得更好的效果。

Conclusion: Anomagic是一种有效的零样本异常生成方法，能够生成高质量的异常，并提高下游异常检测的性能。它还可以根据用户定义的提示为任何正常类别的图像生成异常，成为一个通用的异常生成基础模型。

Abstract: We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.

</details>


### [38] [DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection](https://arxiv.org/abs/2511.10035)
*Feiyang Jia,Caiyan Jia,Ailin Liu,Shaoqing Xu,Qiming Xia,Lin Liu,Lei Yang,Yan Gong,Ziying Song*

Main category: cs.CV

TL;DR: DGFusion addresses the challenge of detecting hard instances (distant, small, occluded objects) in autonomous driving by introducing a Dual-guided paradigm that combines Point-guide-Image and Image-guide-Point approaches. Its core component, the Difficulty-aware Instance Pair Matcher (DIPM), matches instances based on difficulty to create easy and hard instance pairs, enabling effective multi-modal feature fusion. DGFusion shows improved performance on nuScenes and robust gains in hard instance detection.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal 3D object detection methods struggle with detecting hard instances (distant, small, occluded objects) due to a single-guided paradigm that fails to account for differing information density between modalities. This compromises the safety of autonomous driving systems.

Method: The proposed DGFusion method utilizes a Dual-guided paradigm, integrating both the Point-guide-Image and Image-guide-Point paradigms. The core component, the Difficulty-aware Instance Pair Matcher (DIPM), matches instances based on difficulty to generate easy and hard instance pairs. Dual-guided Modules then leverage these pairs for effective multi-modal feature fusion.

Result: DGFusion outperforms baseline methods on nuScenes, achieving +1.0% mAP, +0.8% NDS, and +1.3% average recall. Extensive experiments demonstrate consistent robustness gains in detecting hard instances across various scenarios, including different ego-distances, object sizes, visibility conditions, and small-scale training data.

Conclusion: DGFusion, with its Dual-guided paradigm and DIPM, effectively addresses the limitations of single-guided methods in multi-modal 3D object detection, particularly for hard instances. The method demonstrates superior performance and robustness, contributing to safer autonomous driving systems.

Abstract: As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.

</details>


### [39] [LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning](https://arxiv.org/abs/2511.10040)
*Xinran Yang,Shuichang Lai,Jiangjing Lyu,Hongjie Li,Bowen Pan,Yuanqi Li,Jie Guo,Zhou Zhengkang,Yanwen Guo*

Main category: cs.CV

TL;DR: We introduce a novel 3D VAE framework using unsigned distance fields (UDFs) and a local-to-global (LoG) architecture with UBlocks, enabling high-fidelity 3D content generation up to 2048^3 resolution, outperforming existing methods in accuracy and flexibility.


<details>
  <summary>Details</summary>
Motivation: Generating high-fidelity 3D content is challenging due to complex topologies and limitations of existing methods like SDFs (costly preprocessing, non-manifold issues) and point clouds (sampling artifacts, discontinuities).

Method: We propose a 3D VAE framework using unsigned distance fields (UDFs). Our core is a local-to-global (LoG) architecture that partitions UDFs into 'UBlocks' and uses 3D convolutions for local details and sparse transformers for global coherence. A Pad-Average strategy ensures smooth boundary transitions.

Result: Our method achieves state-of-the-art performance in reconstruction accuracy and generative quality, demonstrating superior surface smoothness and geometric flexibility. It scales to ultra-high resolutions (up to 2048^3), overcoming previous limitations of 3D VAEs.

Conclusion: Our UDF-based 3D VAE with the LoG architecture offers a more robust and efficient solution for generating high-fidelity 3D content, naturally handling complex geometries and achieving unprecedented resolutions.

Abstract: Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to 2048^3-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.

</details>


### [40] [FreDFT: Frequency Domain Fusion Transformer for Visible-Infrared Object Detection](https://arxiv.org/abs/2511.10046)
*Wencong Wu,Xiuwei Zhang,Hanlin Yin,Shun Dai,Hongxi Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 可见光-红外目标检测在复杂场景下存在模态间信息不平衡和现有方法忽视频域模态互补信息的问题。本文提出了一种名为FreDFT的频域融合Transformer，通过新颖的多模态频域注意力（MFDA）、频域前馈层（FDFFL）、跨模态全局建模及局部特征增强模块来解决这些问题，并在多个公共数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 可见光和红外模态在复杂场景下存在信息不平衡问题，导致跨模态融合不足，检测性能下降。现有方法大多在空间域利用Transformer，忽略了频域Transformer挖掘互补信息的优势。

Method: 提出一种名为FreDFT的频域融合Transformer，包含：1.多模态频域注意力（MFDA），挖掘模态间互补信息。2.频域前馈层（FDFFL），通过混合尺度频域特征融合策略增强模态特征。3.跨模态全局建模模块（CGMM），进行像素级、通道级、空间级的跨模态特征交互，消除信息不平衡。4.局部特征增强模块（LFEM），利用卷积和通道shuffle增强局部特征和模态融合。

Result: 所提FreDFT在多个公共数据集上实现了优于其他先进方法（state-of-the-art）的性能。

Conclusion: FreDFT在可见光-红外目标检测方面取得了优异的性能，有效解决了信息不平衡和模态互补信息挖掘的问题。

Abstract: Visible-infrared object detection has gained sufficient attention due to its detection performance in low light, fog, and rain conditions. However, visible and infrared modalities captured by different sensors exist the information imbalance problem in complex scenarios, which can cause inadequate cross-modal fusion, resulting in degraded detection performance. \textcolor{red}{Furthermore, most existing methods use transformers in the spatial domain to capture complementary features, ignoring the advantages of developing frequency domain transformers to mine complementary information.} To solve these weaknesses, we propose a frequency domain fusion transformer, called FreDFT, for visible-infrared object detection. The proposed approach employs a novel multimodal frequency domain attention (MFDA) to mine complementary information between modalities and a frequency domain feed-forward layer (FDFFL) via a mixed-scale frequency feature fusion strategy is designed to better enhance multimodal features. To eliminate the imbalance of multimodal information, a cross-modal global modeling module (CGMM) is constructed to perform pixel-wise inter-modal feature interaction in a spatial and channel manner. Moreover, a local feature enhancement module (LFEM) is developed to strengthen multimodal local feature representation and promote multimodal feature fusion by using various convolution layers and applying a channel shuffle. Extensive experimental results have verified that our proposed FreDFT achieves excellent performance on multiple public datasets compared with other state-of-the-art methods. The code of our FreDFT is linked at https://github.com/WenCongWu/FreDFT.

</details>


### [41] [Image Aesthetic Reasoning via HCM-GRPO: Empowering Compact Model for Superior Performance](https://arxiv.org/abs/2511.10055)
*Zhiyuan Hu,Zheng Sun,Yi Wei,Long Yu*

Main category: cs.CV

TL;DR: 该研究提出了一个全面的图像筛选解决方案，包括收集了超过128k个样本、约640k张图像的大型数据集，并引入了一种名为HCM-GRPO的新方法，以解决当前多模态大语言模型（MLLMs）在图像审美推理方面存在的数据不足和能力较弱的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在图像生成方面虽有进步，但在图像筛选任务上的表现不佳，主要原因是缺乏相关数据以及MLLMs在图像审美推理方面的能力较弱。

Method: 研究方法包括：1. 构建了一个包含128k+样本、640k+图像的数据集，涵盖外观变形、物理阴影、布局合理性、扩展合理性四个方面，并探索了手动、自动化、答案驱动等多种数据标注方法以获取高质量的思维链（CoT）数据。2. 提出了一种名为HCM-GRPO的新方法，将硬案例挖掘（HCM）策略和动态成比例精度（DPA）奖励相结合，并应用于Group Relative Policy Optimization（GRPO）框架，以增强模型的图像审美推理能力。

Result: 实验结果表明，即使是像GPT4o和Qwen-VL-Max这样的顶尖闭源MLLMs，在图像审美推理方面的表现也接近随机猜测。相比之下，使用HCM-GRPO方法，一个规模较小的模型就能够超越大型开源模型和领先的闭源模型的表现。

Conclusion: 研究成功构建了一个大规模图像筛选数据集，并提出了一种名为HCM-GRPO的新颖方法，该方法显著提高了模型的图像审美推理能力，甚至优于当前最先进的模型，证明了其在解决MLLMs在图像筛选方面挑战的有效性。

Abstract: The performance of image generation has been significantly improved in recent years. However, the study of image screening is rare and its performance with Multimodal Large Language Models (MLLMs) is unsatisfactory due to the lack of data and the weak image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive image screening dataset with over 128k samples, about 640k images. Each sample consists of an original image, four generated images. The dataset evaluates the image aesthetic reasoning ability under four aspects: appearance deformation, physical shadow, placement layout, and extension rationality. Regarding data annotation, we investigate multiple approaches, including purely manual, fully automated, and answer-driven annotations, to acquire high-quality chains of thought (CoT) data in the most cost-effective manner. Methodologically, we introduce a Hard Cases Mining (HCM) strategy with a Dynamic Proportional Accuracy (DPA) reward into the Group Relative Policy Optimization (GRPO) framework, called HCM-GRPO. This enhanced method demonstrates superior image aesthetic reasoning capabilities compared to the original GRPO. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the HCM-GRPO, we are able to surpass the scores of both large-scale open-source and leading closed-source models with a much smaller model.

</details>


### [42] [When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?](https://arxiv.org/abs/2511.10059)
*Qilang Ye,Wei Zeng,Meng Liu,Jie Zhang,Yupeng Hu,Zitong Yu,Yu Zhou*

Main category: cs.CV

TL;DR: MLLMs难以区分视觉上存在但听觉上不存在的混淆对象。本研究提出了AV-ConfuseBench基准和RL-CoMM模型，通过引入音频语言模型（LALM）作为参考，利用步进推理奖励和以答案为中心的置信度优化来提高多模态大语言模型的音频-视觉推理能力，相较于基线模型提高了10-30%的准确率。


<details>
  <summary>Details</summary>
Motivation: 目前的多模态大语言模型（MLLMs）在处理视觉上存在但听觉上缺失的混淆对象时存在困难。

Method: 提出AV-ConfuseBench基准来模拟“视听混淆”场景。设计了RL-CoMM模型，包括两个阶段：1.引入大型音频语言模型（LALM）作为参考模型生成纯音频推理，并设计步进推理奖励函数以提高MLLMs的视听推理能力。2.引入以答案为中心的置信度优化，以减少潜在异构推理差异的不确定性。

Result: 在视听问答和视听幻觉任务上，RL-CoMM相较于基线模型，在有限的训练数据下，准确率提高了10%~30%。

Conclusion: RL-CoMM通过引入音频参考和优化置信度，有效提升了MLLMs在处理视听混淆场景下的推理准确性。

Abstract: Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.

</details>


### [43] [Multivariate Gaussian Representation Learning for Medical Action Evaluation](https://arxiv.org/abs/2511.10060)
*Luming Yang,Haoxian Liu,Siqing Li,Alper Yilmaz*

Main category: cs.CV

TL;DR: GaussMedAct通过多变量高斯编码框架在医学动作分析中取得了92.1%的准确率，优于ST-GCN基线。


<details>
  <summary>Details</summary>
Motivation: 医学视觉中的细粒度动作评估面临数据集不足、精度要求高以及快速动作时空动态建模不清的挑战。

Method: 提出CPREval-6k数据集，包含6,372个专家标注的视频和22个临床标签。提出GaussMedAct框架，使用多变量高斯表示将联合运动投影到时间缩放的多维空间，并将动作分解为自适应3D高斯作为令牌，通过各向异性协方差建模来保留运动语义并对噪声保持鲁棒性。采用混合空间编码（笛卡尔和向量双流策略）利用骨骼信息（关节和骨骼特征）。

Result: 在CPREval-6k基准上达到92.1%的Top-1准确率，实时推理。与ST-GCN基线相比，准确率提高5.9%，FLOPs仅为其10%。跨数据集实验也验证了方法的鲁棒性。

Conclusion: GaussMedAct框架在医学动作分析方面表现出色，通过创新的表示学习和编码方法，有效解决了现有挑战，并展现出优越的性能和鲁棒性。

Abstract: Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming the ST-GCN baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.

</details>


### [44] [Perceive, Act and Correct: Confidence Is Not Enough for Hyperspectral Classification](https://arxiv.org/abs/2511.10068)
*Muzhou Yang,Wuzhou Quan,Mingqiang Wei*

Main category: cs.CV

TL;DR: CABIN是一个半监督框架，通过认知感知和行为启发式学习来解决高光谱图像分类中的置信度误导问题。它通过估计认知不确定性来识别潜在错误区域，并采用不确定性引导的双重采样策略来选择样本进行探索，同时锚定置信样本作为伪标签以减少偏差。此外，它还引入了细粒度的动态分配策略，将伪标签数据分为可靠、模糊和噪声子集，并应用定制损失来增强泛化能力。实验证明，CABIN可以提高各种最先进方法的标记效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的高光谱图像分类模型往往过度依赖置信度，导致在稀疏标注或类别不平衡的情况下出现确认偏差，即使模型预测分数很高也可能出错，并且泛化能力较差。因此，需要一种能够感知和纠正不确定性的方法。

Method: CABIN框架通过一个闭环的学习过程（感知、行动和纠正）来解决这个问题。首先，通过估计认知不确定性来发展感知意识，识别容易出错的模糊区域。然后，采用不确定性引导的双重采样策略，选择不确定的样本进行探索，同时将置信样本锚定为稳定的伪标签以减少偏差。最后，通过引入细粒度的动态分配策略，将伪标签数据分为可靠、模糊和噪声子集，并应用定制损失来增强泛化能力。

Result: 实验结果表明，CABIN框架的集成可以使各种最先进的方法受益，提高了标记效率和分类性能。

Conclusion: CABIN通过引入认知不确定性估计和细粒度的伪标签管理，有效解决了高光谱图像分类中的置信度误导问题，提高了模型的泛化能力和标记效率。

Abstract: Confidence alone is often misleading in hyperspectral image classification, as models tend to mistake high predictive scores for correctness while lacking awareness of uncertainty. This leads to confirmation bias, especially under sparse annotations or class imbalance, where models overfit confident errors and fail to generalize. We propose CABIN (Cognitive-Aware Behavior-Informed learNing), a semi-supervised framework that addresses this limitation through a closed-loop learning process of perception, action, and correction. CABIN first develops perceptual awareness by estimating epistemic uncertainty, identifying ambiguous regions where errors are likely to occur. It then acts by adopting an Uncertainty-Guided Dual Sampling Strategy, selecting uncertain samples for exploration while anchoring confident ones as stable pseudo-labels to reduce bias. To correct noisy supervision, CABIN introduces a Fine-Grained Dynamic Assignment Strategy that categorizes pseudo-labeled data into reliable, ambiguous, and noisy subsets, applying tailored losses to enhance generalization. Experimental results show that a wide range of state-of-the-art methods benefit from the integration of CABIN, with improved labeling efficiency and performance.

</details>


### [45] [VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System](https://arxiv.org/abs/2511.10074)
*Gwangyeon Ahn,Jiwan Seo,Joonhyuk Kang*

Main category: cs.CV

TL;DR: VLF-MSC使用单一的视觉-语言特征（VLF）来支持图像和文本生成，提高了频谱效率和鲁棒性，优于仅文本或仅图像的方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信处理单一模态，而VLF-MSC旨在通过单一紧凑的视觉-语言表示来支持图像和文本的生成，实现更高效的通信。

Method: 使用预训练的视觉-语言模型（VLM）将源图像编码为VLF，然后通过无线信道传输。接收端使用基于VLF的解码器语言模型生成文本，并使用基于VLF的扩散模型生成图像。

Result: VLF-MSC在低信噪比（SNR）下，与仅文本或仅图像的基线相比，在两种模态下都取得了更高的语义准确性，并显著减少了带宽消耗。

Conclusion: VLF-MSC通过统一的视觉-语言表示，克服了现有技术的局限性，提高了频谱效率、适应性和鲁棒性，在恶劣的信道条件下仍能有效支持多模态生成。

Abstract: We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.

</details>


### [46] [Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints](https://arxiv.org/abs/2511.10076)
*Xiangyue Zhang,Jianfang Li,Jianqiang Ren,Jiaxu Zhang*

Main category: cs.CV

TL;DR: GlobalDiff是一种创新的基于扩散的生成模型，它直接在全局关节旋转空间中操作，解决了现有方法中存在的累积误差问题，并通过引入多层次的约束方案来增强结构一致性，最终在语音驱动的动作生成任务上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于局部关节旋转的生成方法存在累积误差，导致运动不稳定和不合理。

Method: 提出了一种直接在全局关节旋转空间中操作的基于扩散的框架（GlobalDiff），并引入了多层次的约束方案（关节结构约束、骨骼结构约束、时间结构约束）来弥补全局旋转空间中结构先验的缺失。

Result: GlobalDiff生成了平滑且准确的运动，在多个说话人身份的基准测试中，相比当前最先进的方法，性能提高了46.0%。

Conclusion: GlobalDiff通过直接在全局旋转空间进行预测并结合多层次约束，有效解决了语音驱动动作生成中的误差累积问题，并取得了优于现有方法的性能。

Abstract: Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints. Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure. This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors. In this work, we propose GlobalDiff, a diffusion-based framework that operates directly in the space of global joint rotations for the first time, fundamentally decoupling each joint's prediction from upstream dependencies and alleviating hierarchical error accumulation. To compensate for the absence of structural priors in global rotation space, we introduce a multi-level constraint scheme. Specifically, a joint structure constraint introduces virtual anchor points around each joint to better capture fine-grained orientation. A skeleton structure constraint enforces angular consistency across bones to maintain structural integrity. A temporal structure constraint utilizes a multi-scale variational encoder to align the generated motion with ground-truth temporal patterns. These constraints jointly regularize the global diffusion process and reinforce structural awareness. Extensive evaluations on standard co-speech benchmarks show that GlobalDiff generates smooth and accurate motions, improving the performance by 46.0 % compared to the current SOTA under multiple speaker identities.

</details>


### [47] [GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2511.10081)
*Yuxiang Duan,Ao Li,Yingqin Li,Luyu Li,Pengwei Wang*

Main category: cs.CV

TL;DR: 通过引入“先粗略选择，再精细关注”的策略，GridPrune方法在保持高准确率的同时，显著减少了视觉令牌的数量，从而提高了多模态大语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉令牌修剪方法主要关注“选择什么”，而忽视了人类视觉的“在哪里看”的双阶段策略，这导致了低效的空间分配和不相关令牌的保留。

Method: GridPrune提出了一种“全局引导、局部选择”区域选择系统，首先利用文本条件引导动态分配令牌预算到不同的空间区域，然后在每个预算区域内进行局部选择，取代了全局Top-K机制。

Result: GridPrune在LLaVA-NeXT-7B上取得了卓越的性能，以11.1%的令牌使用量保留了96.98%的完整性能，并且在相同的修剪率下比性能最佳的基线提高了2.34%。

Conclusion: GridPrune通过模仿人类视觉的两阶段注意力分配策略，有效地解决了现有方法在效率和性能上的不足，为多模态大语言模型的视觉令牌修剪提供了新的有效途径。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities in a wide range of vision-language tasks. However, the large number of visual tokens introduces significant computational overhead. To address this issue, visual token pruning has emerged as a key technique for enhancing the efficiency of MLLMs. In cognitive science, humans tend to first determine which regions of a scene to attend to ("where to look") before deciding which specific elements within those regions to process in detail ("what to select"). This two-stage strategy enables the visual system to efficiently allocate attention at a coarse spatial level before performing fine-grained selection. However, existing pruning methods primarily focus on directly optimizing "what to select", typically using attention scores or similarity metrics. They rarely consider "where to look", which has been shown to lead to inefficient spatial allocation, positional bias, and the retention of irrelevant or redundant tokens. In this paper, we propose GridPrune, a method that replaces the global Top-K mechanism with a "guide-globally, select-locally" zonal selection system. GridPrune splits the pruning process into two steps: first, it uses text-conditional guidance to dynamically allocate a token budget across spatial zones; and then, it performs local selection within each budgeted zone. Experimental results demonstrate that GridPrune achieves superior performance across various MLLM architectures. On LLaVA-NeXT-7B, GridPrune retains 96.98% of the full performance while using 11.1% of the tokens, outperforming the best-performing baseline by 2.34% at the same pruning rate.

</details>


### [48] [SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition](https://arxiv.org/abs/2511.10091)
*Qilang Ye,Yu Zhou,Lian He,Jie Zhang,Xuanming Guo,Jiayu Zhang,Mingkui Tan,Weicheng Xie,Yue Sun,Tao Tan,Xiaochen Yuan,Ghada Khoriba,Zitong Yu*

Main category: cs.CV

TL;DR: SUGAR是一个结合大型语言模型（LLMs）和人体骨骼进行动作分类和描述的新范式。它通过视频模型生成视觉-运动信息来监督骨骼学习，解决LLMs理解骨骼和区分动作的问题。


<details>
  <summary>Details</summary>
Motivation: 解决如何让LLMs理解人体骨骼信息并进行动作分类和描述的问题。

Method: 1. 使用现成的视频模型生成与动作相关的视觉和运动信息。 2. 利用这些先验知识监督骨骼学习，生成离散表示。 3. 使用未经修改的LLM理解这些表示并生成动作目标和描述。 4. 引入时间查询投影（TQP）模块来处理长序列的骨骼信号。

Result: 在多个基于骨骼的动作分类基准测试中，SUGAR均表现出有效性，并在零样本场景下比基于线性模型的方法更具通用性。

Conclusion: SUGAR是一种有效且通用的方法，能够利用LLMs进行基于骨骼的动作识别和描述。

Abstract: Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.

</details>


### [49] [RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo](https://arxiv.org/abs/2511.10107)
*Jueun Ko,Hyewon Park,Hyesong Choi,Dongbo Min*

Main category: cs.CV

TL;DR: RobIA是一个新颖的鲁棒、实例感知框架，用于立体深度估计中的连续测试时自适应（CTTA）。


<details>
  <summary>Details</summary>
Motivation: 现实世界环境中的立体深度估计因动态域偏移、稀疏或不可靠的监督以及密集地面真实标签的高昂成本而带来重大挑战。虽然最近的测试时自适应（TTA）方法提供了有希望的解决方案，但大多数依赖于静态目标域假设和输入不变的自适应策略，这限制了它们在持续偏移下的有效性。

Method: RobIA集成了一个Attend-and-Excite Mixture-of-Experts（AttEx-MoE）模块，该模块通过轻量级自注意力机制动态地将输入路由到冻结的专家。此外，还使用了一个基于PEFT的Robust AdaptBN Teacher模型，通过补充稀疏的手工标签来提供密集的伪监督。

Result: RobIA在动态目标域中实现了卓越的自适应性能，同时保持了计算效率。

Conclusion: RobIA通过其Attend-and-Excite Mixture-of-Experts和Robust AdaptBN Teacher组件，为立体深度估计中的持续测试时自适应提供了一种有效且计算效率高的方法，能够更好地处理动态域偏移。

Abstract: Stereo Depth Estimation in real-world environments poses significant challenges due to dynamic domain shifts, sparse or unreliable supervision, and the high cost of acquiring dense ground-truth labels. While recent Test-Time Adaptation (TTA) methods offer promising solutions, most rely on static target domain assumptions and input-invariant adaptation strategies, limiting their effectiveness under continual shifts. In this paper, we propose RobIA, a novel Robust, Instance-Aware framework for Continual Test-Time Adaptation (CTTA) in stereo depth estimation. RobIA integrates two key components: (1) Attend-and-Excite Mixture-of-Experts (AttEx-MoE), a parameter-efficient module that dynamically routes input to frozen experts via lightweight self-attention mechanism tailored to epipolar geometry, and (2) Robust AdaptBN Teacher, a PEFT-based teacher model that provides dense pseudo-supervision by complementing sparse handcrafted labels. This strategy enables input-specific flexibility, broad supervision coverage, improving generalization under domain shift. Extensive experiments demonstrate that RobIA achieves superior adaptation performance across dynamic target domains while maintaining computational efficiency.

</details>


### [50] [Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction](https://arxiv.org/abs/2511.10134)
*Mingda Jia,Weiliang Meng,Zenghuang Fu,Yiheng Li,Qi Zeng,Yifan Zhang,Ju Xin,Rongtao Xu,Jiguang Zhang,Xiaopeng Zhang*

Main category: cs.CV

TL;DR: CACMI是一个显式的时间-语义建模框架，通过跨模态的帧聚合和上下文感知的特征增强来定位和描述非修剪视频中的显著事件，在ActivityNet Captions和YouCook2数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频字幕生成方法主要依赖于隐式建模，使用帧级或片段化的视频特征，这使得它们无法捕捉事件序列之间的时间连贯性以及视觉上下文中的全面语义。

Method: CACMI框架包含两个核心组件：1. 跨模态帧聚合：通过跨模态检索来聚合相关帧，提取具有时间连贯性的、与事件对齐的文本特征。2. 上下文感知特征增强：利用查询引导的注意力机制，将视觉动态与伪事件语义相结合。

Result: 在ActivityNet Captions和YouCook2数据集上进行的大量实验表明，CACMI在密集视频字幕生成任务上取得了最先进的性能。

Conclusion: CACMI通过显式地建模时间和语义信息，克服了现有方法的局限性，从而在密集视频字幕生成方面取得了优越的表现。

Abstract: Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.

</details>


### [51] [Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation](https://arxiv.org/abs/2511.10136)
*Mayank Vatsa,Aparna Bharati,Richa Singh*

Main category: cs.CV

TL;DR: 当前主流文生图模型的根本缺陷在于无法处理逻辑组合。本研究调查了否定、计数和空间关系这三个核心基元在此问题上的表现。分析显示，模型在单独处理基元时表现良好，但组合使用时性能急剧下降，并出现严重干扰。这一失败归因于三个关键因素：1. 训练数据中几乎没有明确的否定示例；2. 连续注意力架构不适用于离散逻辑；3. 评估指标偏重视觉效果而非约束满足。通过分析最新的基准和方法，我们发现现有解决方案和简单扩展无法弥补这一差距。要实现真正的组合性，需要对表示和推理进行根本性改进，而非对现有架构进行渐进式调整。


<details>
  <summary>Details</summary>
Motivation: 当前主流文生图模型在处理逻辑组合方面存在根本缺陷，具体表现在否定、计数和空间关系等核心基元的组合上。

Method: 分析了否定、计数和空间关系三个核心基元在文生图中处理逻辑组合的缺陷，并追溯了失败的三个关键因素：训练数据缺乏显式否定、连续注意力架构不适用于离散逻辑、以及评估指标偏重视觉保真度而非约束满足。

Result: 模型在处理组合基元时性能急剧下降，并出现严重干扰。现有解决方案和简单扩展无法弥补这一差距。

Conclusion: 实现文生图的真正组合性需要对表示和推理进行根本性改进，而非对现有架构进行渐进式调整。

Abstract: The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.

</details>


### [52] [Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space](https://arxiv.org/abs/2511.10142)
*Zhicheng Cai,Hao Zhu,Linsen Chen,Qiu Shen,Xun Cao*

Main category: cs.CV

TL;DR: INR模型的表示能力受限于MLP的低维特征空间，通过引入split-layer，在不增加过多计算成本的情况下，有效扩展特征空间，提升了INR在多任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MLP架构的低维特征空间限制了INR的表示能力，而加宽MLP会带来计算和内存成本的二次方增长。

Method: 提出split-layer结构，将MLP的层划分为多个并行分支，并通过Hadamard积整合输出，构建高次多项式空间，从而提升INR的表示能力。

Result: 实验表明，split-layer显著提升了INR的性能，在2D图像拟合、2D CT重建、3D形状表示和5D新视角合成等任务上均优于现有方法。

Conclusion: Split-layer是一种有效提升INR表示能力且计算成本可控的新方法，在多项任务上展现出优越性能。

Abstract: Implicit neural representation (INR) models signals as continuous functions using neural networks, offering efficient and differentiable optimization for inverse problems across diverse disciplines. However, the representational capacity of INR defined by the range of functions the neural network can characterize, is inherently limited by the low-dimensional feature space in conventional multilayer perceptron (MLP) architectures. While widening the MLP can linearly increase feature space dimensionality, it also leads to a quadratic growth in computational and memory costs. To address this limitation, we propose the split-layer, a novel reformulation of MLP construction. The split-layer divides each layer into multiple parallel branches and integrates their outputs via Hadamard product, effectively constructing a high-degree polynomial space. This approach significantly enhances INR's representational capacity by expanding the feature space dimensionality without incurring prohibitive computational overhead. Extensive experiments demonstrate that the split-layer substantially improves INR performance, surpassing existing methods across multiple tasks, including 2D image fitting, 2D CT reconstruction, 3D shape representation, and 5D novel view synthesis.

</details>


### [53] [Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection](https://arxiv.org/abs/2511.10150)
*Feng Ding,Wenhui Yi,Yunpeng Zhou,Xinan He,Hong Rao,Shu Hu*

Main category: cs.CV

TL;DR: 该研究提出了一种新的双重机制协同优化框架，通过解耦结构公平性和全局分布对齐来提高深度伪造检测模型在不同人口统计群体中的公平性，同时保持检测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的深度伪造检测模型在公平性方面存在偏见，可能导致系统性误判，加剧数字鸿沟和社会不平等。现有方法在提高公平性的同时会牺牲检测准确性。

Method: 提出了一种双重机制协同优化框架，结合了结构公平解耦（在模型架构层面解耦对人口统计群体敏感的通道）和全局分布对齐（在特征层面减小整体样本分布与每个人口统计群体分布之间的距离）。

Result: 实验结果表明，与现有方法相比，该框架在跨域范围内提高了组间和组内公平性，同时保持了整体检测准确性。

Conclusion: 该研究提出的双重机制协同优化框架能够有效改善深度伪造检测模型的公平性，同时不损失检测准确性，解决了现有方法在该领域面临的挑战。

Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.

</details>


### [54] [GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2511.10154)
*Hao Zou,Runqing Zhang,Xue Zhou,Jianxiao Zou*

Main category: cs.CV

TL;DR: 该研究提出了一种名为GEA（Generation-Enhanced Alignment）的方法，通过生成式方法来解决文本到图像的行人检索（TIPR）问题，以提高文本描述与图像内容之间的跨模态对齐精度，并克服数据集有限和模态间隙问题。


<details>
  <summary>Details</summary>
Motivation: 现有的TIPR方法在处理文本描述无法准确反映图像内容、数据集有限以及文本与图像模态间差距等问题时存在局限性，导致跨模态对齐不佳和过拟合。

Method: GEA包含两个并行模块：1. 文本引导令牌增强（TGTE），利用扩散模型生成的图像作为中间语义表示，以弥合文本与视觉模式之间的差距。2. 生成式中间融合（GIF），通过生成图像、原始图像和文本特征之间的交叉注意力来融合信息，并使用三元组对齐损失进行优化。

Result: 在CUHK-PEDES、RSTPReid和ICFG-PEDES三个公共TIPR数据集上进行的大量实验证明了GEA的有效性。

Conclusion: GEA通过引入生成式方法，利用扩散模型生成的图像作为中间语义表示，增强了文本和视觉特征的对齐，有效解决了TIPR中的挑战，并在多个数据集上取得了优越的性能。

Abstract: Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at https://github.com/sugelamyd123/Sup-for-GEA.

</details>


### [55] [Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands](https://arxiv.org/abs/2511.10177)
*Tishya Chhabra,Manisha Bajpai,Walter Zesk,Skylar Tibbits*

Main category: cs.CV

TL;DR: Prithvi-EO-2.0 geospatial foundation model shows strong performance in shoreline delineation of small sandy islands, even with limited training data, demonstrating significant transfer learning capabilities for coastal monitoring in data-poor regions.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on the task of shoreline delineation for small sandy islands using satellite imagery.

Method: Curated and labeled a dataset of 225 multispectral images of two Maldivian islands. Fine-tuned both 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Evaluated performance using F1 and IoU metrics.

Result: Even with as few as 5 training images, the Prithvi models achieved high performance, with an F1 score of 0.94 and an IoU of 0.79.

Conclusion: Prithvi-EO-2.0 models exhibit strong transfer learning capabilities, highlighting their potential for effective coastal monitoring in regions with limited data availability.

Abstract: We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.

</details>


### [56] [HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction](https://arxiv.org/abs/2511.10211)
*Yueran Zhao,Zhang Zhang,Chao Sun,Tianze Wang,Chao Yue,Nuoran Li*

Main category: cs.CV

TL;DR: HeatV2X是一个可扩展的V2X协同感知框架，通过局部异构微调和全局协同微调来解决多模态异构体和可扩展性挑战，从而在最小的训练成本下实现优越的感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有的V2X协同感知框架在处理多模态异构体和扩展到大量参与者时面临挑战。其一，参与的智能体本质上是多模态和异构的，需要有效的跨智能体特征对齐来解决异构性损失。其二，协同框架必须具有可扩展性以适应新的参与者，这使得全参数训练不切实际。

Method: 提出了一种名为HeatV2X的可扩展协同框架。首先，基于异构图注意力训练一个高性能的智能体作为协同学习的基础。然后，设计了局部异构微调（Local Heterogeneous Fine-Tuning）和全局协同微调（Global Collaborative Fine-Tuning）。局部微调利用Hetero-Aware Adapters提取特定于模态的差异，而全局微调则使用Multi-Cognitive Adapter来增强跨智能体协作。

Result: 在OPV2V-H和DAIR-V2X数据集上的实验结果表明，该方法实现的协同框架具有卓越的感知性能，同时大大降低了训练成本，并且优于现有的最先进方法。

Conclusion: HeatV2X通过局部和全局微调策略，有效解决了V2X协同感知中的异构性和可扩展性问题，以最小的训练成本实现了显著的性能提升。

Abstract: Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.

</details>


### [57] [Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization](https://arxiv.org/abs/2511.10212)
*Ashutosh Anshul,Shreyas Gopal,Deepu Rajan,Eng Siong Chng*

Main category: cs.CV

TL;DR: Recent deepfake detection methods struggle with generalization due to limitations in single-stage training and over-reliance on audio-visual inconsistencies. This paper proposes a single-stage framework using next-frame prediction for both uni-modal and cross-modal features, along with a window-level attention mechanism. This approach enhances generalization by focusing on intra-modal artifacts and improves detection of fully manipulated videos and localization of deepfakes in partially spoofed samples, demonstrating strong performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Single-stage supervised training for deepfake detection struggles to generalize to unseen manipulations and datasets. Existing methods often require pretraining on real samples and primarily focus on audio-visual inconsistencies, failing to detect manipulations that preserve audio-visual alignment. There’s a need for a single-stage training framework that improves generalization and addresses intra-modal artifacts.

Method: The proposed framework incorporates next-frame prediction for both uni-modal and cross-modal features within a single-stage training process. It also introduces a window-level attention mechanism to detect discrepancies between predicted and actual frames, enabling the capture of local artifacts around each frame.

Result: The model demonstrates strong generalization capabilities and precise temporal localization when evaluated on multiple benchmark datasets. It is effective in classifying fully manipulated videos and localizing deepfake segments in partially spoofed samples.

Conclusion: The proposed single-stage training framework, leveraging next-frame prediction and a window-level attention mechanism, effectively addresses the generalization limitations of existing deepfake detection methods. It improves the detection of intra-modal artifacts, leading to better performance in classifying fully manipulated videos and localizing deepfake segments in partially manipulated content.

Abstract: Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.

</details>


### [58] [TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding](https://arxiv.org/abs/2511.10241)
*Jinxuan Li,Yi Zhang,Jian-Fang Hu,Chaolei Tan,Tianming Liang,Beihao Xia*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 TubeRMC 的框架，用于解决时空视频精确定位（STVG）任务，特别是在弱监督设置下。TubeRMC 通过生成文本条件候选时空管，并利用三种重建策略（时间、空间、时空）进行精炼，以提高管与文本描述的匹配度和跟踪一致性。实验结果表明，TubeRMC 在 VidSTG 和 HCSTVG 基准测试中优于现有方法，并有效减少了目标识别错误和跟踪不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督时空视频精确定位（STVG）方法通常采用简单的后期融合，导致生成的时空管与文本描述无关，从而出现目标识别失败和跟踪不一致的问题。

Method: 提出 TubeRMC 框架，该框架首先利用预训练的视觉精确定位模型生成文本条件的候选时空管，然后通过时空约束下的管条件重建进行精炼。具体包括三种重建策略：时间、空间和时空重建，每种策略都使用时空管作为条件来重建查询中的关键线索。此外，还引入了空间和时间建议之间的互斥约束来提高重建质量。

Result: TubeRMC 在 VidSTG 和 HCSTVG 两个公共基准测试中均优于现有方法。可视化结果表明，TubeRMC 有效地缓解了目标识别错误和跟踪不一致的问题。

Conclusion: TubeRMC 框架通过引入管条件重建和互斥约束，有效解决了弱监督 STVG 任务中的挑战，能够更准确地定位时空管并保持跟踪的一致性。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.

</details>


### [59] [FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment](https://arxiv.org/abs/2511.10250)
*Yongji Zhang,Siqi Li,Yue Gao,Yu Jiang*

Main category: cs.CV

TL;DR: 该研究提出了JudgeMind，一个用于空中滑雪动作质量评估（AQA）的新方法和数据集，通过模拟裁判评分过程，并包含细粒度的评分和扣分项注释，以提高评估的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有AQA方法主要基于提取的整个视频特征进行评分，可解释性和可靠性有限，且现有数据集缺乏细粒度的动作评分注释，尤其是在扣分项和子分数方面。

Method: 提出JudgeMind方法，通过将动作视频分割为不同阶段并对每个阶段进行评分来提高准确性；提出一个阶段感知特征增强和融合模块，以增强对特定阶段关键区域的感知能力，并提高对频繁视角切换引起的视觉变化的鲁棒性；提出一个基于知识的、与评分相关的解码器，将可能的扣分项纳入先验知识，以预测更准确可靠的分数。

Result: 实验结果表明，JudgeMind在AQA任务上取得了最先进的性能。

Conclusion: JudgeMind方法通过模拟裁判的评分思维，并结合细粒度的阶段性评分和扣分项知识，显著提高了空中滑雪动作质量评估的性能和可靠性，并构建了一个新的AQA数据集作为基准。

Abstract: Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.

</details>


### [60] [Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis](https://arxiv.org/abs/2511.10254)
*Jiulong Wu,Yucheng Shen,Lingyong Yan,Haixin Sun,Deguo Xia,Jizhou Huang,Min Cao*

Main category: cs.CV

TL;DR: Facial-R1是一个新框架，通过指令微调、强化学习和数据合成来改进面部情感分析（FEA），解决VLM的幻觉推理和情感识别与推理不匹配的问题，并在FEA-20K数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前面部情感分析（FEA）方法利用视觉语言模型（VLMs）时存在的两个关键问题：1）VLM由于缺乏特定情感知识而产生幻觉推理；2）情感推理与识别之间存在不匹配，因为观察到的面部特征与最终标签之间的联系不完整。

Method: 提出了一种名为Facial-R1的三阶段对齐框架。第一阶段：使用指令微调来建立基本的情感推理能力。第二阶段：引入以情感和动作单元（AU）标签作为奖励信号的强化训练，以明确推理过程与预测情感的一致性。第三阶段：设计一个数据合成流程，以迭代利用前两个阶段的成果来扩展训练数据集，从而实现可扩展的自我改进。

Result: Facial-R1在八个标准基准上进行了广泛的实验，在FEA方面取得了最先进的性能，表现出强大的泛化能力和鲁棒的可解释性。此外，还引入了FEA-20K数据集，包含17,737个训练样本和1,688个测试样本，并带有细粒度情感分析注释。

Conclusion: Facial-R1框架通过结合指令微调、强化训练和数据合成，成功解决了现有FEA方法中的幻觉推理和识别-推理不匹配问题，并在多个基准测试中取得了最先进的成果，证明了其有效性和泛化能力。

Abstract: Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.

</details>


### [61] [H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification](https://arxiv.org/abs/2511.10260)
*Yongji Zhang,Siqi Li,Kuiyang Huang,Yue Gao,Yu Jiang*

Main category: cs.CV

TL;DR: H3Former通过利用高阶语义关系来聚合局部细粒度表示和结构化区域级别的建模，从而解决细粒度视觉分类中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉区分性线索和处理类别无关的冗余方面存在不足。

Method: 提出了一种名为H3Former的新型token到region框架，该框架利用高阶语义关系来聚合局部细粒度表示和结构化区域级别的建模。引入了语义感知聚合模块（SAAM）和双曲线分层对比损失（HHCL）。SAAM利用多尺度上下文线索在token之间动态构建加权超图，并通过超图卷积捕获高阶语义依赖性，将token特征逐步聚合为紧凑的区域级表示。HHCL在非欧几何嵌入空间中强制执行分层语义约束，以增强类间可分离性和类内一致性，同时保留细粒度类别之间的内在分层关系。

Result: 在四个标准的FGVC基准测试中，H3Former框架在消融研究和与其他方法的比较中都取得了优越的性能。

Conclusion: H3Former通过整合高阶语义关系和结构化区域建模，有效地解决了细粒度视觉分类的挑战，并在多个基准测试中取得了最先进的成果。

Abstract: Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations. Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis. However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy. To address these limitations, we propose H3Former, a novel token-to-region framework that leverages high-order semantic relations to aggregate local fine-grained representations with structured region-level modeling. Specifically, we propose the Semantic-Aware Aggregation Module (SAAM), which exploits multi-scale contextual cues to dynamically construct a weighted hypergraph among tokens. By applying hypergraph convolution, SAAM captures high-order semantic dependencies and progressively aggregates token features into compact region-level representations. Furthermore, we introduce the Hyperbolic Hierarchical Contrastive Loss (HHCL), which enforces hierarchical semantic constraints in a non-Euclidean embedding space. The HHCL enhances inter-class separability and intra-class consistency while preserving the intrinsic hierarchical relationships among fine-grained categories. Comprehensive experiments conducted on four standard FGVC benchmarks validate the superiority of our H3Former framework.

</details>


### [62] [PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.10279)
*Yanbei Jiang,Chao Lei,Yihao Ding,Krista Ehinger,Jey Han Lau*

Main category: cs.CV

TL;DR: PROPA是一个新框架，它结合了MCTS和GRPO，通过生成过程级别的奖励来优化视觉推理中的多步依赖问题，解决了SFT标注成本高和RLVR反馈稀疏的问题，并在多项基准测试中取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在处理复杂视觉推理任务时存在挑战，多步依赖关系可能导致早期错误累积。现有的微调方法（SFT）需要昂贵的逐级标注，而基于可验证奖励的强化学习（RLVR）方法（如GRPO）只提供稀疏的最终结果反馈，难以稳定优化。

Method: PROPA框架整合了蒙特卡洛树搜索（MCTS）和GRPO，以生成密集的、过程级别的奖励，从而在没有人工标注的情况下优化每一步的推理。为了解决冷启动问题，PROPA将GRPO更新与SFT交错进行，使模型能够从成功的和失败的推理轨迹中学习。此外，还训练了一个过程奖励模型（PRM）来指导推理时搜索 GRPO 优化。

Result: PROPA 在七个基准测试和四个 VLM 主干上，在数据域内任务上相比现有方法取得了高达 17.0% 的提升，在数据域外任务上取得了高达 21.0% 的提升。实验证明了 PROPA 在视觉推理任务上的强大推理和泛化能力。

Conclusion: PROPA 通过结合 MCTS 和 GRPO，并引入交错 SFT 和 PRM 指导的推理时搜索，有效解决了视觉推理中多步依赖导致的错误级联问题，并在多项基准测试中超越了现有 SFT 和 RLVR 方法，展现了优越的推理和泛化能力。

Abstract: Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.

</details>


### [63] [Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models](https://arxiv.org/abs/2511.10292)
*Zhengtao Zou,Ya Gao,Jiarui Guan,Bin Li,Pekka Marttinen*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLM）在生成与视觉输入不符的文本时会出现对象幻觉问题。现有的推理干预方法虽然有效，但计算开销大。RUDDER 是一个低开销框架，通过上下文激活残差方向（CARD）向量和自适应门控机制，将 LVLM 引向视觉基础生成，从而解决这个问题。实验表明，RUDDER 在 POPE 和 CHAIR 等基准测试中取得了与最先进方法相当的性能，同时计算延迟可忽略不计。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型（LVLM）在生成文本时出现的对象幻觉问题，并克服现有方法计算开销大的缺点，以提高其在现实世界低延迟场景下的实用性。

Method: 提出了一种名为 Residual-Update Directed DEcoding Regulation (RUDDER) 的低开销框架，该框架包含两个关键创新：1. 上下文激活残差方向（CARD）向量，从自注意力层的残差更新中提取，作为每样本的视觉证据向量。2. 借鉴贝叶斯思想的自适应门控机制，对模型输出进行逐个 token 的调整，其强度根据模型与视觉上下文的偏差进行条件化。

Result: 在 POPE 和 CHAIR 等基准测试中，RUDDER 取得了与最先进方法相当的性能，同时计算延迟可忽略不计。

Conclusion: RUDDER 是一种实用且有效的方法，可以在不显著影响效率的情况下提高 LVLM 的可靠性，解决了 LVLM 的对象幻觉问题和现有干预方法的效率瓶颈。

Abstract: Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.

</details>


### [64] [Rethinking Visual Information Processing in Multimodal LLMs](https://arxiv.org/abs/2511.10301)
*Dongwan Kim,Viresh Ranjan,Takashi Nagata,Arnab Dhua,Amit Kumar K C*

Main category: cs.CV

TL;DR: LLaViT enables LLMs to act as vision encoders by learning separate QKV projections for vision, enabling bidirectional attention on visual tokens, and incorporating both global and local visual representations, outperforming LLaVA.


<details>
  <summary>Details</summary>
Motivation: Address the inherent mismatch between text and vision modalities in current vision-language architectures like LLaVA, where visual features are not effectively integrated.

Method: Introduce LLaViT, which modifies the LLM to function as a vision encoder through three key changes: 1. Learning separate QKV projections for the vision modality. 2. Enabling bidirectional attention on visual tokens. 3. Incorporating both global and local visual representations.

Result: LLaViT significantly outperforms the baseline LLaVA method on multiple benchmarks, even exceeding the performance of models with twice its parameter count.

Conclusion: LLaViT establishes a more effective approach to vision-language modeling by enabling LLMs to serve as powerful vision encoders.

Abstract: Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.

</details>


### [65] [Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision](https://arxiv.org/abs/2511.10316)
*Yu Deng,Baozhu Zhao,Junyan Su,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TL;DR: A new framework for 3D reconstruction tackles extreme depth variations by combining depth-of-field supervision and multi-view consistency, improving depth accuracy and structural integrity, especially in challenging urban environments.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods struggle with scenes that have extreme depth variations because of inconsistent supervisory signals between near-field and far-field regions, leading to inaccurate depth estimation in distant areas and structural degradation in close-range regions.

Method: The proposed framework integrates two main components: 1) Depth-of-field Supervision, which uses a monocular depth estimator and defocus convolution to create physically accurate defocused images and enforces geometric consistency with a novel depth-of-field loss. 2) Multi-View Consistency Supervision, which uses LoFTR-based feature matching to minimize cross-view geometric errors and enforces depth consistency through least squares optimization.

Result: The method achieves superior depth fidelity, showing a 0.8 dB PSNR improvement over the state-of-the-art on the Waymo Open Dataset, by unifying defocus physics with multi-view geometric constraints.

Conclusion: The framework successfully bridges physical imaging principles and learning-based depth regularization, providing a scalable solution for complex depth stratification in urban environments.

Abstract: Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.

</details>


### [66] [Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment](https://arxiv.org/abs/2511.10334)
*Wenti Yin,Huaxin Zhang,Xiang Wang,Yuqing Lu,Yicheng Zhang,Bingquan Gong,Jialong Zuo,Li Yu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DSANet通过解耦粗粒度和细粒度表示，明确分离异常和正常特征，以提高细粒度视频异常检测和分类的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于多模态基础模型的弱监督视频异常检测方法倾向于检测最显著的异常片段，忽略了区分正常模式，并且容易因外观相似而混淆类别，导致细粒度分类效果不佳。

Method: DSANet在粗粒度层面引入自指导性常态建模分支，通过学习到的正常原型来重建输入视频特征，以挖掘视频固有的常态线索；在细粒度层面，提出解耦对比语义对齐机制，通过时间分解事件中心和背景中心组件，并应用视觉-语言对比学习来增强类别判别性表示。

Result: DSANet在XD-Violence和UCF-Crime两个标准基准测试中，性能优于现有的最先进方法。

Conclusion: DSANet能够显着提高弱监督视频异常检测的细粒度分类能力。

Abstract: Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.

</details>


### [67] [FOUND: Fourier-based von Mises Distribution for Robust Single Domain Generalization in Object Detection](https://arxiv.org/abs/2511.10352)
*Mengzhu Wang,Changyuan Deng,Shanshan Wang,Nan Yin,Long Lan,Liang Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖的框架，通过结合 von Mises-Fisher (vMF) 分布和傅里叶变换来增强单域泛化 (SDG) 目标检测的性能，同时利用 CLIP 进行语义引导。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单域泛化目标检测中，虽然利用了像 CLIP 这样的语义增强方法，但忽略了特征分布的内在结构和频率域特性，这些特性对于提高模型的鲁棒性至关重要。

Method: 该框架将 vMF 分布和傅里叶变换集成到一个 CLIP 引导的流程中。vMF 分布用于模拟物体表征的定向特征，以捕捉嵌入空间中不随域变化的语义结构。傅里叶变换则被用作一种数据增强策略，通过扰动幅度相位分量来模拟频率域中的域偏移，从而提高特征的鲁棒性。

Result: 该框架在多样的天气驾驶基准上进行了广泛实验，结果表明该方法在保持 CLIP 的语义对齐优势的同时，还丰富了跨域特征的多样性和结构一致性，并在性能上超越了现有的最先进方法。

Conclusion: 本研究提出的结合 vMF 分布和傅里叶变换的 CLIP 引导框架，能够有效提升单域泛化目标检测模型的鲁棒性和泛化能力，解决了现有方法忽略特征结构和频率域特性的问题，并在实验中取得了优于现有方法的性能。

Abstract: Single Domain Generalization (SDG) for object detection aims to train a model on a single source domain that can generalize effectively to unseen target domains. While recent methods like CLIP-based semantic augmentation have shown promise, they often overlook the underlying structure of feature distributions and frequency-domain characteristics that are critical for robustness. In this paper, we propose a novel framework that enhances SDG object detection by integrating the von Mises-Fisher (vMF) distribution and Fourier transformation into a CLIP-guided pipeline. Specifically, we model the directional features of object representations using vMF to better capture domain-invariant semantic structures in the embedding space. Additionally, we introduce a Fourier-based augmentation strategy that perturbs amplitude and phase components to simulate domain shifts in the frequency domain, further improving feature robustness. Our method not only preserves the semantic alignment benefits of CLIP but also enriches feature diversity and structural consistency across domains. Extensive experiments on the diverse weather-driving benchmark demonstrate that our approach outperforms the existing state-of-the-art method.

</details>


### [68] [DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile](https://arxiv.org/abs/2511.10367)
*Thales Bezerra,Emanoel Thyago,Kelvin Cunha,Rodrigo Abreu,Fábio Papais,Francisco Mauro,Natália Lopes,Érico Medeiros,Jéssica Guido,Shirley Cruz,Paulo Borba,Tsang Ing Ren*

Main category: cs.CV

TL;DR: DermAI是一个基于智能手机的应用程序，用于实时捕获、注释和分类皮肤病变，解决了AI在皮肤病学中应用受限于数据集偏差、图像质量和验证不足的问题。


<details>
  <summary>Details</summary>
Motivation: AI在皮肤病学中的应用受到数据集偏差、图像质量差和验证不足的限制，需要一个能够克服这些挑战的解决方案。

Method: DermAI是一个轻量级的、基于智能手机的应用程序，可以在常规咨询中实时捕获、注释和分类皮肤病变。它还在设备上执行质量检查和本地模型适应。DermAI临床数据集包含了各种肤色、种族和设备来源的样本。

Result: 在初步实验中，使用公共数据集训练的模型在新数据上泛化能力不足，而使用本地数据进行微调后性能有所提高。这表明模型在公共数据集上训练后，对其在新数据上的实际应用效果不佳，但在本地数据上进行微调后，能够更好地适应新的数据。

Conclusion: 标准化、多样化且符合医疗需求的数据收集对于机器学习的开发至关重要，DermAI的实验结果强调了这一点。

Abstract: AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The DermAI clinical dataset, encompasses a wide range of skin tones, ethinicity and source devices. In preliminary experiments, models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance. These results highlight the importance of standardized, diverse data collection aligned with healthcare needs and oriented to machine learning development.

</details>


### [69] [SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation](https://arxiv.org/abs/2511.10370)
*Kai-Hendrik Cohrs,Zuzanna Osika,Maria Gonzalez-Calabuig,Vishal Nedungadi,Ruben Cartuyvels,Steffen Knoblauch,Joppe Massant,Shruti Nath,Patrick Ebel,Vasileios Sitokonstantinou*

Main category: cs.CV

TL;DR: SHRUG-FM是一个用于地球观测的地理空间基础模型（GFM）的可靠性框架，通过结合输入空间和嵌入空间的OOD检测以及预测不确定性，来提高模型在欠代表环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 地球观测的GFM在欠代表环境中表现不可靠。

Method: SHRUG-FM框架整合了三种互补信号：输入空间OOD检测、嵌入空间OOD检测和特定任务的预测不确定性。

Result: 在火烧疤痕分割任务中，SHRUG-FM表明OOD分数与特定环境条件下的性能下降相关，而不确定性标志有助于剔除大量性能不佳的预测。将这些标志与HydroATLAS的土地覆盖属性关联后发现，模型失效并非随机，而是集中在某些地理区域，如低海拔区域和大型河流区域，这可能是由于预训练数据不足所致。

Conclusion: SHRUG-FM为GFM在气候敏感应用中的安全、可解释部署提供了途径，有助于弥合基准性能与实际可靠性之间的差距。

Abstract: Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.

</details>


### [70] [Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation](https://arxiv.org/abs/2511.10382)
*Zhen Chen,Yi Zhang,Xiangyu Yin,Chengxuan Qin,Xingyu Zhao,Xiaowei Huang,Wenjie Ruan*

Main category: cs.CV

TL;DR: 个性化AI应用（如DreamBooth）能生成定制内容，但也带来隐私风险，特别是面部身份泄露。现有的防御机制（如Anti-DreamBooth）通过注入对抗性扰动来防止个性化，但存在明显的人工痕迹和易被轻易去除的问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制的对抗样本存在明显的人工痕迹，且易被传统图像滤波或对抗性净化去除，因此需要新的评估框架来系统评估现有防御在现实净化威胁下的保护效果。

Method: 提出了一种名为AntiDB_Purify的新型评估框架，用于系统性地评估现有防御机制在面对传统图像滤波器和对抗性净化等现实净化威胁时的有效性。

Result: 现有防御机制在面对上述净化威胁时，都无法有效保持其保护效果。

Conclusion: 目前的防御机制提供了一种虚假的安全感，迫切需要更不可感知和更鲁棒的保护措施来保障个性化生成中的用户身份安全。

Abstract: Personalized AI applications such as DreamBooth enable the generation of customized content from user images, but also raise significant privacy concerns, particularly the risk of facial identity leakage. Recent defense mechanisms like Anti-DreamBooth attempt to mitigate this risk by injecting adversarial perturbations into user photos to prevent successful personalization. However, we identify two critical yet overlooked limitations of these methods. First, the adversarial examples often exhibit perceptible artifacts such as conspicuous patterns or stripes, making them easily detectable as manipulated content. Second, the perturbations are highly fragile, as even a simple, non-learned filter can effectively remove them, thereby restoring the model's ability to memorize and reproduce user identity. To investigate this vulnerability, we propose a novel evaluation framework, AntiDB_Purify, to systematically evaluate existing defenses under realistic purification threats, including both traditional image filters and adversarial purification. Results reveal that none of the current methods maintains their protective effectiveness under such threats. These findings highlight that current defenses offer a false sense of security and underscore the urgent need for more imperceptible and robust protections to safeguard user identity in personalized generation.

</details>


### [71] [SAMIRO: Spatial Attention Mutual Information Regularization with a Pre-trained Model as Oracle for Lane Detection](https://arxiv.org/abs/2511.10385)
*Hyunjong Lee,Jangho Lee,Jaekoo Lee*

Main category: cs.CV

TL;DR: SAMIRO是一种用于车道检测的即插即用模块，通过从预训练模型转移知识并保留空间信息，在各种数据集上一致地提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的车道检测面临背景混乱、光照变化和遮挡等挑战，需要利用上下文和全局信息来克服数据驱动方法的局限性。

Method: 提出了一种空间注意力互信息正则化（SAMIRO）方法，该方法利用预训练模型作为 Oracle，通过传递知识和保留领域不可知的空间信息来增强车道检测。

Result: 在CULane、Tusimple和LLAMAS等主要基准数据集上进行的大量实验表明，SAMIRO在不同模型和数据集上始终能提高性能。

Conclusion: SAMIRO是一种有效的即插即用模块，可以增强各种先进车道检测方法的性能，同时保留重要的空间信息。

Abstract: Lane detection is an important topic in the future mobility solutions. Real-world environmental challenges such as background clutter, varying illumination, and occlusions pose significant obstacles to effective lane detection, particularly when relying on data-driven approaches that require substantial effort and cost for data collection and annotation. To address these issues, lane detection methods must leverage contextual and global information from surrounding lanes and objects. In this paper, we propose a Spatial Attention Mutual Information Regularization with a pre-trained model as an Oracle, called SAMIRO. SAMIRO enhances lane detection performance by transferring knowledge from a pretrained model while preserving domain-agnostic spatial information. Leveraging SAMIRO's plug-and-play characteristic, we integrate it into various state-of-the-art lane detection approaches and conduct extensive experiments on major benchmarks such as CULane, Tusimple, and LLAMAS. The results demonstrate that SAMIRO consistently improves performance across different models and datasets. The code will be made available upon publication.

</details>


### [72] [MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns](https://arxiv.org/abs/2511.10390)
*Jiarui Zhang,Yuliang Liu,Zijun Wu,Guosheng Pang,Zhili Ye,Yupei Zhong,Junteng Ma,Tao Wei,Haiyang Xu,Weikai Chen,Zeen Wang,Qiangjun Ji,Fanxi Zhou,Qi Zhang,Yuanrui Hu,Jiahao Liu,Zhang Li,Ziyang Zhang,Qiang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: MonkeyOCR v1.5是一个统一的视觉-语言框架，通过两阶段解析流程增强了布局理解和内容识别能力，解决了复杂文档布局（如多级表格、嵌入式图像/公式、跨页结构）的挑战，并在OmniDocBench v1.5上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统难以处理现实世界中具有复杂布局（如多级表格、嵌入式图像/公式、跨页结构）的文档，因此需要一个能增强布局理解和内容识别能力的统一框架。

Method: MonkeyOCR v1.5采用两阶段解析流程：第一阶段使用大型多模态模型联合预测文档布局和阅读顺序；第二阶段对检测区域内的文本、公式和表格进行本地化识别。引入了基于视觉一致性的强化学习方案来处理复杂表格结构，并增加了图像解耦表格解析和类型引导表格合并模块来处理包含嵌入图像的表格以及跨页/跨列的表格。

Result: MonkeyOCR v1.5在OmniDocBench v1.5上实现了最先进的性能，优于PPOCR-VL和MinerU 2.5，并且在视觉复杂文档场景下表现出卓越的鲁棒性。

Conclusion: MonkeyOCR v1.5通过其新颖的两阶段解析流程、视觉一致性强化学习方案以及专门的表格处理模块，有效解决了复杂文档解析的挑战，并在多个方面取得了显著的性能提升。

Abstract: Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.

</details>


### [73] [LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components](https://arxiv.org/abs/2511.10394)
*Yaru Li,Yanxue Wang,Meng Li,Xinming Li,Jianbo Feng*

Main category: cs.CV

TL;DR: 该研究提出了一种结合YOLOMS和大型语言模型（LLM）的智能风力涡轮机故障分析和诊断框架。


<details>
  <summary>Details</summary>
Motivation: 现有故障检测方法主要局限于视觉识别，缺乏语义可解释性，无法支持维护决策。本研究旨在解决这些局限性。

Method: 该框架结合了YOLOMS和LLM。YOLOMS利用多尺度检测和滑动窗口裁剪来提取故障特征。一个轻量级键值（KV）映射模块将YOLOMS的检测结果转换为结构化文本表示，包含定性和定量属性。然后，经过领域微调的LLM进行语义推理，生成可解释的故障分析和维护建议。

Result: 实验结果表明，该框架在故障检测准确率上达到了90.6%，生成的维护报告准确率平均为89%。

Conclusion: 所提出的框架提高了风力涡轮机故障诊断结果的可解释性，并为风力涡轮机的运行和维护提供了实用的决策支持。

Abstract: The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\% and generates maintenance reports with an average accuracy of 89\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.

</details>


### [74] [3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound](https://arxiv.org/abs/2511.10412)
*Alomar Antonia,Rubio Ricardo,Albaiges Gerard,Salort-Benejam Laura,Caminal Julia,Prat Maria,Rueda Carolina,Cortes Berta,Piella Gemma,Sukno Federico*

Main category: cs.CV

TL;DR: Acquiring standard facial planes in fetal ultrasounds is difficult. We developed GT++ and 3DFETUS, which use algorithms and deep learning to automatically estimate these planes from 3D US volumes, achieving high accuracy and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Fetal movement, orientation variability, and operator dependence make acquiring standard facial planes during routine fetal ultrasound examinations challenging, leading to inconsistencies, increased examination time, and potential diagnostic bias.

Method: The study presents GT++ (an algorithm using annotated anatomical landmarks to estimate standard facial planes from 3D US volumes) and 3DFETUS (a deep learning model for automated localization of these planes in 3D fetal US volumes).

Result: The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming state-of-the-art methods. Clinical assessments confirmed statistically significant improvements in plane estimation accuracy.

Conclusion: GT++ and 3DFETUS effectively automate and standardize the localization of standard facial planes in 3D fetal US volumes, addressing the challenges of fetal movement and operator variability and improving accuracy compared to existing methods.

Abstract: Acquiring standard facial planes during routine fetal ultrasound (US) examinations is often challenging due to fetal movement, variability in orientation, and operator-dependent expertise. These factors contribute to inconsistencies, increased examination time, and potential diagnostic bias.
  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.
  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.

</details>


### [75] [RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation](https://arxiv.org/abs/2511.10431)
*Daniele Perlo,Vladimir Despotovic,Selma Boudissa,Sang-Yoon Kim,Petr Nazarov,Yanrong Zhang,Max Wintermark,Olivier Keunen*

Main category: cs.CV

TL;DR: 我们提出了一个用于自动检测癫痫发作的实验室啮齿动物视频数据集，其中包含10,101个正常样本和2,952个癫痫发作样本。


<details>
  <summary>Details</summary>
Motivation: 为癫痫研究中的非侵入式视频监测提供一个标准化的、易于获取的数据集，以支持对癫痫发作的自动检测。

Method: 收集并标注了包含正常活动和癫痫发作的啮齿动物视频片段，并使用TimeSformer模型进行基线实验，采用五折交叉验证和严格的被试者分区以避免数据泄露。

Result: 基于TimeSformer的模型在区分癫痫发作和正常活动方面取得了97%的平均F1分数。

Conclusion: 该数据集和基线代码的公开提供，将促进可复现的研究，推动无创视频监测技术在临床前癫痫研究中的应用。

Abstract: We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357

</details>


### [76] [Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations](https://arxiv.org/abs/2511.10432)
*Willem Bonnaffé,Yang Hu,Andrea Chatrian,Mengran Fan,Stefano Malacrino,Sandy Figiel,CRUK ICGC Prostate Group,Srinivasa R. Rao,Richard Colling,Richard J. Bryant,Freddie C. Hamdy,Dan J. Woodcock,Ian G. Mills,Clare Verrill,Jens Rittscher*

Main category: cs.CV

TL;DR: HIT uses semantic segmentation to extract glands from WSIs for MIL, improving accuracy and interpretability by focusing on biologically meaningful structures.


<details>
  <summary>Details</summary>
Motivation: Digital pathology pipelines often ignore tissue architecture by relying on grid-based tiling, which introduces irrelevant information and limits interpretability. This paper introduces a histology-informed tiling (HIT) approach to address this limitation.

Method: HIT uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. The model was trained on 137 samples from the ProMPT cohort and then applied to 760 WSIs across ICGC-C and TCGA-PRAD cohorts.

Result: HIT achieved a gland-level Dice score of 0.83 +/- 0.17. It improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC. HIT also revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. HIT streamlined computations by focusing on biologically meaningful structures.

Conclusion: HIT improved the accuracy and interpretability of MIL predictions in digital pathology by using histology-informed tiling, which focuses on biologically relevant structures like glands, thereby enhancing computational efficiency and analytical insights.

Abstract: Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.

</details>


### [77] [OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data](https://arxiv.org/abs/2511.10461)
*Simon Donike,Cesar Aybar,Julio Contreras,Luis Gómez-Chova*

Main category: cs.CV

TL;DR: OpenSR-SRGAN是一个开源的、模块化的单图超分辨率框架，专为地球观测多光谱卫星数据（如Sentinel-2）设计。它通过配置文件简化了SRGAN模型的使用、配置和扩展，降低了研究者和实践者在超分辨率领域的实验门槛。


<details>
  <summary>Details</summary>
Motivation: 提供一个易于配置、扩展和应用于多光谱卫星数据的单图超分辨率框架，特别是针对SRGAN模型，降低研究和实践的门槛。

Method: 通过简洁的配置文件实现生成器、判别器、损失函数和训练计划的统一化，无需修改模型代码即可切换架构、比例因子和波段设置。。

Result: 实现了一个配置驱动的GAN超分辨率工作流程，内置了日志记录、验证和大场景推理功能，并提供适用于常见遥感场景的即用型配置和合理的默认设置。

Conclusion: OpenSR-SRGAN通过将GAN超分辨率转化为一种配置驱动的工作流程，使得研究人员和实践者能够更轻松地进行实验、可复现地比较模型，并在不同的地球观测数据集上部署超分辨率流程。

Abstract: We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.

</details>


### [78] [Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes](https://arxiv.org/abs/2511.10484)
*Tejas Sudharshan Mathai,Anisa V. Prasad,Xinya Wang,Praveen T. S. Balamuralikrishna,Yan Zhuang,Abhinav Suri,Jianfei Liu,Perry J. Pickhardt,Ronald M. Summers*

Main category: cs.CV

TL;DR: 该研究提出了一种全自动方法来分割胰腺，并利用CT图像生物标志物（特别是胰腺表面小叶化（PSL））来筛查2型糖尿病（T2DM）。结果显示，T2DM患者的PSL高于非糖尿病患者，并且所提出的模型能够以0.90的AUC预测T2DM。


<details>
  <summary>Details</summary>
Motivation: T2DM的早期检测至关重要，因为它会通过形态学改变和异位脂肪增加来影响胰腺功能，最终导致器官损伤。虽然已有研究关注胰腺体积和脂肪含量与T2DM的关联，但胰腺表面小叶化（PSL）在T2DM患者中的作用尚未得到充分研究。

Method: 研究人员开发了一种全自动方法，利用四个基于深度学习的模型来分割584名患者（297名男性，437名非糖尿病患者，年龄：45±15岁）的胰腺。然后，自动检测PSL，并将其与糖尿病状态相关联。此外，还训练了一个多变量模型，利用CT生物标志物来预测T2DM。

Result: 研究发现，T2DM患者的PSL（4.26±8.32）显著高于非糖尿病患者（3.19±3.62）（p=0.01）。在胰腺分割方面，PancAP模型取得了最高的Dice分数（0.79±0.17）和最低的ASSD误差（1.94±2.63 mm）（p<0.05）。在T2DM预测方面，基于CT生物标志物的多变量模型达到了0.90的AUC、66.7%的灵敏度和91.9%的特异性。

Conclusion: 研究结果表明，PSL可用于T2DM筛查，并有潜力预测T2DM的早期发病。

Abstract: Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\pm$ 8.32 compared to 3.19 $\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\pm$ 0.17 and lowest ASSD error of 1.94 $\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\% sensitivity, and 91.9\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.

</details>


### [79] [SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers](https://arxiv.org/abs/2511.10488)
*Oded Schlesinger,Amirhossein Farzam,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: Vision Transformers (ViTs)计算量大，容易二次方增长。提出SParsification with attentiOn dynamics via Token relevance (SPOT)框架，通过分析token嵌入、交互和跨层注意力动态来推断token重要性，实现早期冗余token检测和消除，提高计算效率且不牺牲性能。SPOT的预测器轻量级，可集成到不同ViT架构中，实现性能可调。实验表明SPOT效率提升高达40%，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: ViT的计算需求随token数量二次方增长，需要更有效的计算方法。

Method: SPOT框架利用token嵌入、交互和跨层注意力动态来推断token重要性，并通过轻量级预测器实现token稀疏化。

Result: SPOT框架在效率上相较于标准ViT有显著提升（高达40%），同时保持或提高了准确性。

Conclusion: SPOT框架能够有效地减少ViT的计算量，提高效率，同时不牺牲模型性能。

Abstract: While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .

</details>


### [80] [Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising](https://arxiv.org/abs/2511.10500)
*Yusuf Talha Basak,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: LTV结合了展开式TV求解器和数据驱动的Lambda Mapping Network（LambdaNet），通过预测每像素正则图，实现了对TV的改进，在CT图像重建中表现优于传统 métodos。


<details>
  <summary>Details</summary>
Motivation: TV在去噪和边缘保持方面表现良好，但其对lambda参数的依赖性限制了其效率和有效性。

Method: 提出了一种可学习的总变分（LTV）框架，该框架将展开式TV求解器与数据驱动的Lambda Mapping Network（LambdaNet）相结合，LambdaNet预测每像素正则图。该流程端到端训练，联合优化重建和正则化，实现了空间自适应平滑。

Result: +2.9 dB PSNR和+6% SSIM的平均增益，优于经典TV和FBP+U-Net。

Conclusion: LTV提供了一种可解释的替代黑色盒子CNN的方法，并为3D和数据一致性驱动的重建奠定了基础。

Abstract: Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: https://github.com/itu-biai/deep_tv_for_ldct

</details>


### [81] [Dynamic Avatar-Scene Rendering from Human-centric Context](https://arxiv.org/abs/2511.10539)
*Wenqing Wang,Haosen Yang,Josef Kittler,Xiatian Zhu*

Main category: cs.CV

TL;DR: 利用单独的映射机制在动态场景中重建人类，以解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建与真实环境交互的动态人类时，要么整体建模场景，要么将场景和背景分开建模，忽视了人类和场景之间独特运动特征和信息交换，导致重建不完整或出现空间不一致和视觉伪影。

Method: 提出了一种名为'Separate-then-Map'（StM）的策略，通过共享的变换函数统一单独建模的组件，以实现人类和周围环境在空间和视觉上的一致性。

Result: 在单眼视频数据集上的大量实验表明，StM在视觉质量和渲染精度方面显著优于现有最先进的方法，尤其是在具有挑战性的人体-场景交互边界处。

Conclusion: StM策略通过引入信息映射机制，有效解决了现有动态人类重建方法的局限性，提高了重建的准确性和视觉效果。

Abstract: Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.

</details>


### [82] [Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation](https://arxiv.org/abs/2511.10547)
*Isabela Albuquerque,Ira Ktena,Olivia Wiles,Ivana Kajić,Amal Rannen-Triki,Cristina Vasconcelos,Aida Nematzadeh*

Main category: cs.CV

TL;DR: Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of diversity in current text-to-image (T2I) models, which often generate homogeneous outputs despite advances in generation quality. The paper aims to provide a robust framework for evaluating and improving the diversity of T2I models.

Method: The framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. It includes a novel human evaluation template, a curated prompt set with identified factors of variation (e.g., prompt: An image of an apple, factor of variation: color), and a methodology for comparing models using human annotations via binomial tests. Various image embeddings are also compared for diversity measurement.

Result: The study enables ranking of T2I models by diversity and identifies specific categories where they struggle. This provides insights into the diversity performance of different models.

Conclusion: This research offers a robust methodology and insights for evaluating and improving T2I model diversity. It paves the way for advancements in T2I model diversity and the development of relevant metrics.

Abstract: Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.
  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.

</details>


### [83] [A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space](https://arxiv.org/abs/2511.10555)
*Huijie Liu,Shuhao Cui,Haoxiang Cao,Shuai Ma,Kai Wu,Guoliang Kang*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的“代码到风格”图像生成任务，仅使用数值风格代码来生成具有新颖且一致视觉风格的图像，解决了现有方法在风格一致性、创造性和复杂风格表示方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法在风格一致性、创造性和复杂风格表示方面存在挑战，并且行业（如 Midjourney）虽然进行了探索，但学术界缺乏开放研究。

Method: 提出了一种名为 CoTyle 的新颖的、开源的“代码到风格”图像生成方法。首先，从图像集合中训练一个离散的风格代码本以提取风格嵌入。然后，将这些嵌入作为文本到图像扩散模型（T2I-DM）的条件来生成风格化图像。最后，在离散风格嵌入上训练一个自回归风格生成器来模拟其分布，从而合成新颖的风格嵌入。在推理时，风格生成器将数值风格代码映射到唯一的风格嵌入，然后该嵌入指导 T2I-DM 生成相应风格的图像。

Result: CoTyle 能够有效地将数值代码转换为风格控制器，生成新颖且一致的视觉风格，证明了“代码即风格”的概念。

Conclusion: CoTyle 是第一个开源的“代码到风格”图像生成方法，它通过数值风格代码实现了简单、多样且可复现的风格生成，有效解决了现有方法的局限性。

Abstract: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.

</details>


### [84] [OmniVGGT: Omni-Modality Driven Visual Geometry Grounded](https://arxiv.org/abs/2511.10560)
*Haosong Peng,Hao Li,Yalun Dai,Yushi Lan,Yihang Luo,Tianyu Qi,Zhengshen Zhang,Yufeng Zhan,Junfei Zhang,Wenchao Xu,Ziwei Liu*

Main category: cs.CV

TL;DR: OmniVGGT是一个新框架，它融合了RGB信息和几何线索（如深度图、相机内外参），并通过GeoAdapter和随机多模态融合策略进行优化。该框架在多种3D视觉任务上均表现出色，甚至优于仅使用RGB输入的模型，并能有效提升视觉-语言-动作（VLA）模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D视觉基础模型大多仅依赖RGB输入，忽略了可用的几何信息，限制了其性能。

Method: 提出GeoAdapter来编码深度和相机参数，并使用零初始化卷积稳定地将几何信息注入模型。同时，采用随机多模态融合策略，允许在训练时随机采样模态子集，从而使模型能够处理任意数量的模态输入，并学习鲁棒的空间表示。

Result: OmniVGGT在单目/多目深度估计、多目立体重建和相机位姿估计等任务上超越了现有方法，即使在仅使用RGB输入时也达到了最先进的水平。此外，集成OmniVGGT的VLA模型在机器人任务上取得了显著的性能提升。

Conclusion: OmniVGGT能够有效利用任意数量的几何模态信息，提升3D视觉任务和VLA模型的性能，并展现了其在实际应用中的潜力。

Abstract: General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.

</details>


### [85] [Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping](https://arxiv.org/abs/2511.10604)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Kaylee Xiao,Motasem Alkayid,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: Sentinel-2 LULC 分类面临空间异质性、上下文信息和光谱模糊性等挑战。本文提出了一种名为 MSOM 的新模型，结合了基于对象的图像分析 (OBIA) 和 Mamba 架构，以改进 Sentinel-2 图像的 LULC 分类。


<details>
  <summary>Details</summary>
Motivation: 提高 Sentinel-2 图像的 LULC 分类精度，克服数据挑战。

Method: 提出了一种名为 MSOM 的新模型，该模型结合了：1. OBIA-Mamba 模型，使用超像素作为 Mamba 标记以减少计算冗余并保留细节。2. GLocal 双分支 CNN-Mamba 架构，用于同时处理局部空间细节和全局上下文信息。3. 多任务优化框架，使用双损失函数来平衡局部精度和全局一致性。

Result: 所提出的方法在加拿大艾伯塔省的 Sentinel-2 影像上进行了测试，与几种先进的分类方法进行了比较，结果表明所提出的方法实现了更高的分类精度和比其他最先进方法更精细的细节。

Conclusion: MSOM 模型能够有效提高 Sentinel-2 图像的 LULC 分类精度，并生成更精细的分类结果。

Abstract: Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.

</details>


### [86] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR:  大型视觉语言模型（VLMs）在视频描述方面表现出色，但模型大小对可访问性描述质量有影响。本文评估了不同参数量的SmolVLM2变体在两个数据集上的表现，并提出了两个新的评估框架（多上下文BLV框架和导航辅助框架）来评估模型的可访问性。此外，还评估了不同的提示设计策略，并在智能手机上测试了模型的实际性能。


<details>
  <summary>Details</summary>
Motivation: 研究模型大小对可访问性描述质量的影响，特别是为了满足盲人和低视力（BLV）用户的需求。

Method: 评估了SmolVLM2 500M和2.2B参数的变体在AVCaps和Charades数据集上的表现。提出了多上下文BLV框架和导航辅助框架。评估了四种不同的提示设计策略。在智能手机上部署了FP32和INT8精度的模型。

Result: 初步结果表明，更大的模型在某些方面可能提供更详细的描述，但这也带来了更高的计算和内存需求。提出的评估框架能够更全面地评估模型的可访问性。

Conclusion: 模型大小对视频描述的可访问性有重要影响。需要进一步研究以在模型性能和可访问性之间取得平衡，特别是对于资源受限的设备和特定用户群体。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [87] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: Latent Upscaler Adapter (LUA) is a new module for diffusion models that allows for high-resolution image synthesis by upscaling directly in the latent space before VAE decoding, reducing artifacts and improving speed compared to post-hoc super-resolution.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face challenges in generating high-resolution images due to slow and costly direct sampling and the introduction of artifacts and latency by post-hoc super-resolution.

Method: LUA integrates a lightweight module into the generator's latent code, using a Swin-style backbone with scale-specific pixel-shuffle heads for 2x and 4x upscaling. It operates in the latent space before VAE decoding, requiring no changes to the base model or additional diffusion stages.

Result: LUA enables high-resolution synthesis via a single feed-forward pass in latent space, achieving comparable perceptual quality to image-space SR baselines but with significantly lower decoding and upscaling time (3x faster). It also demonstrates strong generalization across different VAEs.

Conclusion: LUA provides a practical and efficient solution for scalable, high-fidelity image synthesis in diffusion models by performing super-resolution in the latent space, closely matching native high-resolution generation quality while improving speed and reducing artifacts.

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [88] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: DA3是一个能够从任意数量的视觉输入（无论是否有已知的相机姿态）预测空间一致几何的3D模型。它展示了单一的Transformer骨干和单一的深度射线预测目标足以实现高性能，并且通过师生训练范式，其细节和泛化能力与DA2相当。DA3在新的视觉几何基准测试中取得了先进的性能，并在所有任务上超越了之前的SOTA VGGT，尤其在相机姿态和几何精度方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索一种能够从任意数量的视觉输入（无论是否有已知的相机姿态）预测空间一致几何的3D模型，并追求最简化的模型设计。

Method: 使用单一的普通Transformer（如vanilla DINO encoder）作为骨干，无需进行架构专门化。采用单一的深度射线预测目标，避免了复杂的多任务学习。通过师生训练范式进行模型训练。

Result: DA3在细节和泛化能力上达到了与DA2相当的水平。在新的视觉几何基准测试中，DA3在相机姿态估计、任意视角几何和视觉渲染等所有任务上均设定了新的SOTA，平均相机姿态精度提高了44.3%，几何精度提高了25.1%。此外，DA3在单目深度估计方面也优于DA2。所有模型仅使用公开的学术数据集进行训练。

Conclusion: DA3模型通过简化的架构和创新的训练方法，在视觉几何任务上取得了显著的性能提升，并在新的基准测试中设定了新的SOTA，证明了其在三维视觉几何预测方面的有效性和潜力。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [89] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: SCS通过引入视觉扰动和轨迹重采样来解决多模态大模型在多项选择题中因不忠实推理而获得的奖励相同的问题，从而提高模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在多项选择题设置中，未忠实的推理轨迹（错误推理但猜中正确答案）与真实的推理轨迹获得相同的奖励，这是一个缺陷。

Method: SCS通过（i）引入小的视觉扰动和（ii）对初始轨迹进行重复截断和重采样。结果轨迹之间的一致性产生了一个可微分的一致性分数，该分数在策略更新期间会降低不可靠轨迹的权重。

Result: SCS可以提高多模态大模型的准确性，在Qwen2.5-VL-7B-Instruct等模型上，准确性最多可提高7.7个百分点，并且在Qwen2.5-VL-3B-Instruct和InternVL3-8B上也有显著提升，同时计算成本可忽略不计。

Conclusion: SCS是一种简单且通用的方法，可以解决多模态大模型中结果奖励强化学习的缺陷。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [90] [A Shared-Autonomy Construction Robotic System for Overhead Works](https://arxiv.org/abs/2511.09695)
*David Minkwan Kim,K. M. Brian Lee,Yong Hyeok Seo,Nikola Raicevic,Runfa Blark Li,Kehan Long,Chan Seon Yoon,Dong Min Kang,Byeong Jo Lim,Young Pyoung Kim,Nikolay Atanasov,Truong Nguyen,Se Woong Jun,Young Wook Kim*

Main category: cs.RO

TL;DR: 开发了一个用于天花板钻孔等 overhead 作业的机器人系统，该系统结合了移动底座、升降装置、双臂躯干和定制钻孔末端执行器。


<details>
  <summary>Details</summary>
Motivation: 支持在能见度有限的动态环境中进行远程操作。

Method: 使用高斯泼溅法进行在线 3D 重建，并引入运动参数来模拟移动物体，同时采用神经配置空间障碍物方法进行规划和控制，以确保在动态障碍物周围安全操作。

Result: 初步可行性研究证明了该硬件在钻孔、螺栓固定和锚固方面的能力，以及该软件在动态环境中安全远程操作方面的能力。

Conclusion: 所开发的机器人系统在 overhead 作业和动态环境下的安全远程操作方面具有可行性。

Abstract: We present the ongoing development of a robotic system for overhead work such as ceiling drilling. The hardware platform comprises a mobile base with a two-stage lift, on which a bimanual torso is mounted with a custom-designed drilling end effector and RGB-D cameras. To support teleoperation in dynamic environments with limited visibility, we use Gaussian splatting for online 3D reconstruction and introduce motion parameters to model moving objects. For safe operation around dynamic obstacles, we developed a neural configuration-space barrier approach for planning and control. Initial feasibility studies demonstrate the capability of the hardware in drilling, bolting, and anchoring, and the software in safe teleoperation in a dynamic environment.

</details>


### [91] [Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard](https://arxiv.org/abs/2511.09727)
*Stelios Zarifis,Ioannis Chalkiadakis,Artemis Chardouveli,Vasiliki Moutzouri,Aggelos Sotirchos,Katerina Papadimitriou,Panagiotis Filntisis,Niki Efthymiou,Petros Maragos,Katerina Pastra*

Main category: cs.RO

TL;DR: Inspired by infant development, this paper proposes a Reinforcement Learning (RL) framework for autonomous self-exploration in a robotic agent. The agent learns self-touch and hand regard behaviors through intrinsic rewards, mimicking infant curiosity. For self-touch, tactile inputs are transformed for efficient learning, and curriculum learning encourages body coverage and balance. For hand regard, visual features are learned through motor babbling, with intrinsic rewards driving novel motions and gaze following. A curriculum progresses from single-hand to dual-hand training for visual-motor coordination. The results show that curiosity-driven signals can lead to coordinated multimodal learning, imitating infant progression from babbling to purposeful behaviors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an autonomous self-exploration framework for robotic agents inspired by infant development, utilizing intrinsic rewards to mimic curiosity-driven learning of self-touch and hand regard behaviors.

Method: The framework uses Reinforcement Learning (RL) with intrinsic rewards. For self-touch, high-dimensional tactile inputs are transformed, and curriculum learning is employed to encourage body coverage, balance, and generalization. For hand regard, visual features are learned through motor babbling, and intrinsic rewards promote novel hand motions and gaze following, with a curriculum progressing from single to dual-hand training.

Result: The results demonstrate that curiosity-based, unsupervised signals can drive coordinated multimodal learning in a robotic agent, successfully imitating an infant's developmental progression from random motor babbling to purposeful behaviors like self-touch and hand regard.

Conclusion: The paper concludes that purely curiosity-driven intrinsic rewards can effectively guide autonomous self-exploration and coordinated multimodal learning in robotic agents, mirroring the developmental stages observed in human infants.

Abstract: Inspired by infant development, we propose a Reinforcement Learning (RL) framework for autonomous self-exploration in a robotic agent, Baby Sophia, using the BabyBench simulation environment. The agent learns self-touch and hand regard behaviors through intrinsic rewards that mimic an infant's curiosity-driven exploration of its own body. For self-touch, high-dimensional tactile inputs are transformed into compact, meaningful representations, enabling efficient learning. The agent then discovers new tactile contacts through intrinsic rewards and curriculum learning that encourage broad body coverage, balance, and generalization. For hand regard, visual features of the hands, such as skin-color and shape, are learned through motor babbling. Then, intrinsic rewards encourage the agent to perform novel hand motions, and follow its hands with its gaze. A curriculum learning setup from single-hand to dual-hand training allows the agent to reach complex visual-motor coordination. The results of this work demonstrate that purely curiosity-based signals, with no external supervision, can drive coordinated multimodal learning, imitating an infant's progression from random motor babbling to purposeful behaviors.

</details>


### [92] [A Robust Task-Level Control Architecture for Learned Dynamical Systems](https://arxiv.org/abs/2511.09790)
*Eshika Pathak,Ahmed Aboudonia,Sandeep Banik,Naira Hovakimyan*

Main category: cs.RO

TL;DR: L1-DS是一种新颖的任务级鲁棒控制架构，用于解决基于动力学系统(DS)的学习演示(LfD)中常见的任务-执行不匹配问题，通过结合标称稳定控制器和L1自适应控制器来处理未建模动力学、持续干扰和系统延迟，并使用基于窗口的动态时间规整(DTW)的目标选择器来处理时间失准，以实现相位一致的跟踪，该架构在LASA和IROS手写数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于动力学系统(DS)的学习演示(LfD)生成的运动计划在实际执行中，由于未建模动力学、持续干扰和系统延迟等因素导致机器人实际任务空间状态偏离期望轨迹的任务-执行不匹配问题。

Method: 提出了一种新颖的任务级鲁棒控制架构——L1增强动力学系统(L1-DS)。该框架通过为一个DS-based LfD模型增加一个标称稳定控制器和一个L1自适应控制器来处理任务-执行不匹配。此外，引入了一个基于窗口的动态时间规整(DTW)的目标选择器，以处理时间失准，实现相位一致的跟踪。

Result: 成功地在LASA和IROS手写数据集上展示了L1-DS架构的有效性。

Conclusion: L1-DS架构可以有效地处理DS-based LfD中的任务-执行不匹配问题，实现鲁棒的任务空间轨迹跟踪。

Abstract: Dynamical system (DS)-based learning from demonstration (LfD) is a powerful tool for generating motion plans in the operation (`task') space of robotic systems. However, the realization of the generated motion plans is often compromised by a ''task-execution mismatch'', where unmodeled dynamics, persistent disturbances, and system latency cause the robot's actual task-space state to diverge from the desired motion trajectory. We propose a novel task-level robust control architecture, L1-augmented Dynamical Systems (L1-DS), that explicitly handles the task-execution mismatch in tracking a nominal motion plan generated by any DS-based LfD scheme. Our framework augments any DS-based LfD model with a nominal stabilizing controller and an L1 adaptive controller. Furthermore, we introduce a windowed Dynamic Time Warping (DTW)-based target selector, which enables the nominal stabilizing controller to handle temporal misalignment for improved phase-consistent tracking. We demonstrate the efficacy of our architecture on the LASA and IROS handwriting datasets.

</details>


### [93] [Provably Safe Stein Variational Clarity-Aware Informative Planning](https://arxiv.org/abs/2511.09836)
*Kaleb Ben Naveed,Utkrisht Sahai,Anouck Girard,Dimitra Panagou*

Main category: cs.RO

TL;DR: 该研究提出了一种名为“Stein Variational Clarity-Aware Informative Planning”的框架，用于在信息衰减和存在障碍物的环境中规划自主机器人的信息收集路径。该框架通过考虑信息随时间和空间变化的动态模型（clarity），并结合基于“gatekeeper”的低级滤波机制来确保安全，从而解决了现有方法忽略可变信息衰减率和将安全视为软约束的问题。实验证明，该框架在不同衰减率和障碍物存在的情况下，都能保证安全并减少信息缺失。


<details>
  <summary>Details</summary>
Motivation: 现有机器人信息收集规划方法忽略了信息衰减率的空间变化及其在运动中的演变，并将安全视为一个软约束，这在时间和空间变化的环境中是不够的。本研究旨在解决这些挑战。

Method: 提出了一种名为“Stein Variational Clarity-Aware Informative Planning”的框架，该框架将“clarity”（一种表示信息增益和衰减的指标）动态模型嵌入到轨迹优化中，并使用基于“gatekeeper”框架的低级滤波机制来强制执行安全。该规划器通过Stein变分推理进行基于贝叶斯推理的学习，以优化信息轨迹，并通过滤波确保安全。

Result: 在具有不同衰减率和障碍物的环境的硬件实验和仿真中，该框架表现出持续的安全性，并显著减少了信息缺失。

Conclusion: Stein Variational Clarity-Aware Informative Planning框架能够有效地在信息动态变化且存在障碍物的环境中规划安全且信息丰富的机器人路径。

Abstract: Autonomous robots are increasingly deployed for information-gathering tasks in environments that vary across space and time. Planning informative and safe trajectories in such settings is challenging because information decays when regions are not revisited. Most existing planners model information as static or uniformly decaying, ignoring environments where the decay rate varies spatially; those that model non-uniform decay often overlook how it evolves along the robot's motion, and almost all treat safety as a soft penalty. In this paper, we address these challenges. We model uncertainty in the environment using clarity, a normalized representation of differential entropy from our earlier work that captures how information improves through new measurements and decays over time when regions are not revisited. Building on this, we present Stein Variational Clarity-Aware Informative Planning, a framework that embeds clarity dynamics within trajectory optimization and enforces safety through a low-level filtering mechanism based on our earlier gatekeeper framework for safety verification. The planner performs Bayesian inference-based learning via Stein variational inference, refining a distribution over informative trajectories while filtering each nominal Stein informative trajectory to ensure safety. Hardware experiments and simulations across environments with varying decay rates and obstacles demonstrate consistent safety and reduced information deficits.

</details>


### [94] [PuffyBot: An Untethered Shape Morphing Robot for Multi-environment Locomotion](https://arxiv.org/abs/2511.09885)
*Shashwat Singh,Zilin Si,Zeynep Temel*

Main category: cs.RO

TL;DR: PuffyBot是一种能够改变形态的两栖机器人，可以在陆地和水下环境中移动，通过改变体积来调整浮力，该机器人集成了一个剪刀升降机构和一个摇杆连杆机构，能够实现爬行和游泳模式之间的无缝过渡。


<details>
  <summary>Details</summary>
Motivation: 受两栖动物适应陆地和水生环境的形态和运动的启发，设计一款能够变形以在多种环境中导航的机器人。

Method: 设计一个利用剪刀升降机构作为主要结构来实现形态变化的机器人，该机构由线性致动器驱动。集成了一个摇杆连杆机构，可以调整伺服驱动的肢体，实现爬行和游泳模式之间的转换。使用热塑性聚氨酯（TPU）织物实现完全防水。

Result: 机器人可以在陆地爬行、水下地板爬行、水面游泳。通过双imodal浮力调节，可以实现下潜或上浮。实现了255.00 cm3到423.75 cm3的体积变化，可抵消3.237 N的向下的力。可进行两小时的无线操作。

Conclusion: 形态变化技术有潜力创造适合多样化环境的多功能、高能效机器人平台。

Abstract: Amphibians adapt their morphologies and motions to accommodate movement in both terrestrial and aquatic environments. Inspired by these biological features, we present PuffyBot, an untethered shape morphing robot capable of changing its body morphology to navigate multiple environments. Our robot design leverages a scissor-lift mechanism driven by a linear actuator as its primary structure to achieve shape morphing. The transformation enables a volume change from 255.00 cm3 to 423.75 cm3, modulating the buoyant force to counteract a downward force of 3.237 N due to 330 g mass of the robot. A bell-crank linkage is integrated with the scissor-lift mechanism, which adjusts the servo-actuated limbs by 90 degrees, allowing a seamless transition between crawling and swimming modes. The robot is fully waterproof, using thermoplastic polyurethane (TPU) fabric to ensure functionality in aquatic environments. The robot can operate untethered for two hours with an onboard battery of 1000 mA h. Our experimental results demonstrate multi-environment locomotion, including crawling on the land, crawling on the underwater floor, swimming on the water surface, and bimodal buoyancy adjustment to submerge underwater or resurface. These findings show the potential of shape morphing to create versatile and energy efficient robotic platforms suitable for diverse environments.

</details>


### [95] [A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation](https://arxiv.org/abs/2511.09932)
*Hanwen Wang*

Main category: cs.RO

TL;DR: The study addresses the limited generalization ability of visuomotor policies in diverse scenarios by creating a more extensively randomized dataset. This dataset, generated with minimal human input, incorporates various manipulators, grippers, and randomization factors like camera pose, lighting, and tabletop appearance, significantly improving policy generalization, especially for zero-shot sim-to-real transfer.


<details>
  <summary>Details</summary>
Motivation: Existing methods for training generalizable imitation learning policies lack diversity in their generated data, limiting their effectiveness across different scenarios. This paper aims to improve policy generalization by investigating and automating the generation of diverse training data based on scene layout factors that significantly impact performance.

Method: The study automates data generation for scene layout factors that impact visuomotor policy generalization. A diverse dataset was created with minimal human demonstration, covering 5 manipulator types, 2 gripper types, and randomizations in camera pose, lighting, tabletop texture, and table height across 6 manipulation tasks. Policies were trained on this dataset and their generalization was evaluated.

Result: The results show that all investigated scene layout factors (camera pose, lighting, tabletop texture, table height) influence policy generalization. Applying any form of randomization improves generalization, with diverse trajectories being particularly effective in bridging the visual gap. The proposed scene randomization enhanced visuomotor policy generalization for zero-shot sim-to-real transfer on a low-cost manipulator.

Conclusion: The paper concludes that extensive randomization of scene layout factors is crucial for enhancing the generalization ability of visuomotor policies. The proposed automated data generation method creates a diverse dataset that significantly improves policy performance across various scenarios and enables effective zero-shot sim-to-real transfer.

Abstract: The generalization ability of visuomotor policy is crucial, as a good policy should be deployable across diverse scenarios. Some methods can collect large amounts of trajectory augmentation data to train more generalizable imitation learning policies, aimed at handling the random placement of objects on the scene's horizontal plane. However, the data generated by these methods still lack diversity, which limits the generalization ability of the trained policy. To address this, we investigate the performance of policies trained by existing methods across different scene layout factors via automate the data generation for those factors that significantly impact generalization. We have created a more extensively randomized dataset that can be efficiently and automatically generated with only a small amount of human demonstration. The dataset covers five types of manipulators and two types of grippers, incorporating extensive randomization factors such as camera pose, lighting conditions, tabletop texture, and table height across six manipulation tasks. We found that all of these factors influence the generalization ability of the policy. Applying any form of randomization enhances policy generalization, with diverse trajectories particularly effective in bridging visual gap. Notably, we investigated on low-cost manipulator the effect of the scene randomization proposed in this work on enhancing the generalization capability of visuomotor policies for zero-shot sim-to-real transfer.

</details>


### [96] [Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2511.09958)
*Xiangyi Wei,Haotian Zhang,Xinyi Cao,Siyu Xie,Weifeng Ge,Yang Li,Changbo Wang*

Main category: cs.RO

TL;DR: Vision-Language-Action (VLA) models for robotics face limitations with vision-only inputs, especially for dynamic manipulation tasks. This paper introduces Audio-VLA, which incorporates contact audio to improve perception of interaction dynamics, overcoming vision-only constraints. A new metric, Task Completion Rate (TCR), is also proposed to evaluate dynamic processes. Audio-VLA uses pre-trained DINOv2, SigLIP, AudioCLIP, and Llama2, fine-tuned with LoRA, and a multimodal projection layer. Simulations (RLBench, LIBERO) are enhanced with audio generation, and experiments on simulations and real-world tasks show Audio-VLA outperforms vision-only models, with TCR effectively measuring dynamic process perception.


<details>
  <summary>Details</summary>
Motivation: Vision-only VLA models have fundamental limitations in perceiving interactive and dynamic manipulation processes in robotics. This paper aims to overcome these constraints by incorporating contact audio into the VLA framework.

Method: Audio-VLA, a multimodal manipulation policy, leverages contact audio alongside visual input. It uses pre-trained DINOv2 and SigLIP for visual encoding, AudioCLIP for audio encoding, and Llama2 as the language model backbone. LoRA fine-tuning is applied to these modules for cross-modal understanding. A multimodal projection layer aligns features from different modalities. Collision-based audio generation is added to RLBench and LIBERO simulations for realistic sound feedback. The Task Completion Rate (TCR) metric is introduced to evaluate dynamic process perception.

Result: Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate that Audio-VLA achieves superior performance compared to vision-only methods. The proposed TCR metric effectively quantifies the robots' capabilities in perceiving dynamic processes during manipulation.

Conclusion: Audio-VLA, by integrating contact audio, significantly enhances the perception of dynamic manipulation processes in robots, overcoming the limitations of vision-only approaches. The new TCR metric provides a more comprehensive evaluation of robotic manipulation by assessing the understanding of dynamic operational processes, not just final outcomes.

Abstract: The Vision-Language-Action models (VLA) have achieved significant advances in robotic manipulation recently. However, vision-only VLA models create fundamental limitations, particularly in perceiving interactive and manipulation dynamic processes. This paper proposes Audio-VLA, a multimodal manipulation policy that leverages contact audio to perceive contact events and dynamic process feedback. Audio-VLA overcomes the vision-only constraints of VLA models. Additionally, this paper introduces the Task Completion Rate (TCR) metric to systematically evaluate dynamic operational processes. Audio-VLA employs pre-trained DINOv2 and SigLIP as visual encoders, AudioCLIP as the audio encoder, and Llama2 as the large language model backbone. We apply LoRA fine-tuning to these pre-trained modules to achieve robust cross-modal understanding of both visual and acoustic inputs. A multimodal projection layer aligns features from different modalities into the same feature space. Moreover RLBench and LIBERO simulation environments are enhanced by adding collision-based audio generation to provide realistic sound feedback during object interactions. Since current robotic manipulation evaluations focus on final outcomes rather than providing systematic assessment of dynamic operational processes, the proposed TCR metric measures how well robots perceive dynamic processes during manipulation, creating a more comprehensive evaluation metric. Extensive experiments on LIBERO, RLBench, and two real-world tasks demonstrate Audio-VLA's superior performance over vision-only comparative methods, while the TCR metric effectively quantifies dynamic process perception capabilities.

</details>


### [97] [Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks](https://arxiv.org/abs/2511.10008)
*Xuancun Lu,Jiaxiang Chen,Shilin Xiao,Zizhi Jin,Zhangrui Chen,Hanwen Yu,Bohan Qian,Ruochen Zhou,Xiaoyu Ji,Wenyuan Xu*

Main category: cs.RO

TL;DR: 该研究首次系统地研究了视觉-语言-动作（VLA）模型在物理世界中免受传感器攻击的安全性，并提出了一种名为“Real-Sim-Real”的框架来模拟和验证这些攻击，同时开发了一种基于对抗性训练的防御方法来增强模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在物理世界中免受传感器攻击的安全性研究不足，存在安全隐患。

Method: 提出“Real-Sim-Real”框架，模拟针对摄像头和麦克风的传感器攻击，并在真实机器人系统上进行验证。开发了一种基于对抗性训练的防御方法。

Result: 大规模评估表明，VLA模型在不同任务类型和模型设计下，对传感器攻击表现出显著的脆弱性。所提出的防御方法能够提高VLA模型对传感器攻击的鲁棒性，同时保持模型性能。

Conclusion: VLA模型在实际应用中面临严峻的传感器安全威胁，需要开发标准化的鲁棒性基准和缓解策略来确保其在关键环境中的安全性。

Abstract: Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored.
  To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel ``Real-Sim-Real'' framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.

</details>


### [98] [DecARt Leg: Design and Evaluation of a Novel Humanoid Robot Leg with Decoupled Actuation for Agile Locomotion](https://arxiv.org/abs/2511.10021)
*Egor Davydenko,Andrei Volchenkov,Vladimir Gerasimov,Roman Gorbachev*

Main category: cs.RO

TL;DR: a new robotic leg called DecARt Leg is proposed for agile locomotion, featuring decoupled actuation, a forward-facing knee, and a novel ankle torque transmission system. Its performance is evaluated using a new metric, FAST, and through simulations and experiments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to design an agile robotic leg for locomotion, introducing the DecARt Leg with novel features for improved performance.

Method: The study proposes a quasi-telescopic kinematic structure with decoupled actuation, a forward-facing knee, and a multi-bar ankle torque transmission system. A new metric, FAST, is introduced for quantitative evaluation, followed by simulations and preliminary hardware experiments.

Result: The DecARt Leg's agile locomotion capabilities are evaluated numerically using the FAST metric and compared with other designs. Performance is further assessed through extensive simulations and preliminary hardware experiments.

Conclusion: The paper presents the DecARt Leg, a novel robotic leg design for agile locomotion, and evaluates its performance using simulations and experiments with a new metric, FAST.

Abstract: In this paper, we propose a novel design of an electrically actuated robotic leg, called the DecARt (Decoupled Actuation Robot) Leg, aimed at performing agile locomotion. This design incorporates several new features, such as the use of a quasi-telescopic kinematic structure with rotational motors for decoupled actuation, a near-anthropomorphic leg appearance with a forward facing knee, and a novel multi-bar system for ankle torque transmission from motors placed above the knee. To analyze the agile locomotion capabilities of the design numerically, we propose a new descriptive metric, called the `Fastest Achievable Swing Time` (FAST), and perform a quantitative evaluation of the proposed design and compare it with other designs. Then we evaluate the performance of the DecARt Leg-based robot via extensive simulation and preliminary hardware experiments.

</details>


### [99] [Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.10079)
*Yizheng Wang,Timon Rabczuk,Yinghua Liu*

Main category: cs.RO

TL;DR: 提出一种基于KAN的物理启发的机器学习方法，用于机器人关节的静摩擦建模，该方法具有高预测精度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统静摩擦模型（如Stribeck模型）通常需要预定义的函数形式，这在处理未知函数结构时存在挑战。

Method: 提出一种基于KAN的物理启发的机器学习方法，集成了样条激活函数和符号回归机制，通过修剪和属性评分实现模型简化和物理表达式提取。

Result: 在各种任务中，该方法在合成数据和真实摩擦数据上均实现了大于0.95的决定系数，并成功提取了简洁且具有物理意义的摩擦表达式。

Conclusion: 这项研究为可解释的、数据驱动的机器人摩擦建模提供了新的视角，并具有良好的工程应用前景。

Abstract: Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems. Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures. To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints. The method integrates spline activation functions with a symbolic regression mechanism, enabling model simplification and physical expression extraction through pruning and attribute scoring, while maintaining both high prediction accuracy and interpretability. We first validate the method's capability to accurately identify key parameters under known functional models, and further demonstrate its robustness and generalization ability under conditions with unknown functional structures and noisy data. Experiments conducted on both synthetic data and real friction data collected from a six-degree-of-freedom industrial manipulator show that the proposed method achieves a coefficient of determination greater than 0.95 across various tasks and successfully extracts concise and physically meaningful friction expressions. This study provides a new perspective for interpretable and data-driven robotic friction modeling with promising engineering applicability.

</details>


### [100] [Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning](https://arxiv.org/abs/2511.10087)
*Haidong Huang,Haiyue Zhu. Jiayu Song,Xixin Zhao,Yaohua Zhou,Jiayi Zhang,Yuze Zhai,Xiaocong Li*

Main category: cs.RO

TL;DR: UEPO是一个统一的生成框架，用于解决离线到在线强化学习（O2O-RL）中的行为模态覆盖和分布偏移问题。它使用多种子、动态感知扩散策略来捕捉多种模态，动态散度正则化机制来增强策略多样性，并结合基于扩散的数据增强来提高动态模型泛化能力。在D4RL基准测试中，UEPO在运动任务上比Uni-O4提高了5.9%，在灵巧操作任务上提高了12.4%。


<details>
  <summary>Details</summary>
Motivation: 解决离线到在线强化学习（O2O-RL）中有限的行为模态覆盖和分布偏移的挑战，以实现安全高效的机器人策略部署。

Method: 提出UEPO，一个统一的生成框架，借鉴了大型语言模型预训练和微调策略。具体包括：1. 多种子、动态感知扩散策略，用于高效捕捉多种行为模态；2. 动态散度正则化机制，强制执行物理上可行的策略多样性；3. 基于扩散的数据增强模块，用于增强动态模型的泛化能力。

Result: UEPO在D4RL基准测试中取得了显著成果：在运动任务上比Uni-O4提高了5.9%，在灵巧操作任务上提高了12.4%，证明了其强大的泛化能力和可扩展性。

Conclusion: UEPO通过其创新的生成框架，在解决O2O-RL的挑战方面表现出色，并在机器人策略的泛化和可扩展性方面取得了优于现有方法的成果。

Abstract: Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\% on dexterous manipulation, demonstrating strong generalization and scalability.

</details>


### [101] [Learning a Thousand Tasks in a Day](https://arxiv.org/abs/2511.10110)
*Kamil Dreczkowski,Pietro Vitiello,Vitalis Vosylius,Edward Johns*

Main category: cs.RO

TL;DR: 我们提出了一种名为MT3的新型模仿学习方法，该方法通过将轨迹分解为对齐和交互阶段，并利用基于检索的泛化，显著提高了数据效率。MT3仅需少量演示即可学会日常操作任务，并能泛化到新的物体实例，使其能够快速教会机器人大量任务。


<details>
  <summary>Details</summary>
Motivation: 目前的模仿学习方法在机器人操作任务中通常需要大量演示，限制了其效率。本研究旨在通过引入两种基本先验（轨迹分解和检索式泛化）来提高学习效率。

Method: 本研究将操纵轨迹分解为顺序的对齐和交互阶段，并利用检索式泛化。通过3450次真实世界试验，系统地研究了这种分解方法，并比较了不同设计选择的效果，以及与行为克隆的泛化和扩展趋势。

Result: 在每个任务演示次数较少（<10次）的情况下，分解方法比单阶段学习在数据效率上提高了两个数量级。检索方法在对齐和交互方面均优于行为克隆。基于此，我们开发了MT3方法，仅需单个演示即可学会日常操作任务，并能泛化到新的物体实例。MT3使机器人能够在24小时内学会1000个不同的日常任务。通过2200次额外试验，我们揭示了MT3在不同任务类别上的能力和局限性。

Conclusion: 轨迹分解和检索式泛化是提高模仿学习数据效率的有效方法。MT3通过结合这两种方法，实现了高效的机器人任务学习，显著减少了对演示数据的需求。

Abstract: Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.

</details>


### [102] [RoboBenchMart: Benchmarking Robots in Retail Environment](https://arxiv.org/abs/2511.10276)
*Konstantin Soshin,Alexander Krapukhin,Andrei Spiridonov,Denis Shepelev,Gregorii Bukhtuev,Andrey Kuznetsov,Vlad Shakhuro*

Main category: cs.RO

TL;DR: Existing robotic manipulation benchmarks are too simple. We introduce RoboBenchMart, a more challenging benchmark for dark store environments with cluttered and diverse grocery items. State-of-the-art models struggle with these tasks. We release RoboBenchMart with tools and baseline models to advance research.


<details>
  <summary>Details</summary>
Motivation: Most existing robotic manipulation benchmarks are limited to simplified tabletop scenarios, lacking the complexity needed for real-world applications like dark stores. This benchmark aims to address this limitation by introducing a more challenging and realistic setting for robotic manipulation in retail environments.

Method: Introduced RoboBenchMart, a benchmark for dark store environments. This benchmark features dense object clutter and varied spatial configurations. It includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools, and fine-tuned baseline models.

Result: Current state-of-the-art generalist models demonstrate difficulty in solving common retail tasks within the RoboBenchMart benchmark, highlighting the need for further research and development.

Conclusion: RoboBenchMart provides a more challenging and realistic benchmark for robotic manipulation in dark store environments, addressing the limitations of existing benchmarks. The release of this suite, along with baseline models and tools, is expected to drive further research and progress in near-term automation for the retail sector, as current models struggle with these complex tasks.

Abstract: Most existing robotic manipulation benchmarks focus on simplified tabletop scenarios, typically involving a stationary robotic arm interacting with various objects on a flat surface. To address this limitation, we introduce RoboBenchMart, a more challenging and realistic benchmark designed for dark store environments, where robots must perform complex manipulation tasks with diverse grocery items. This setting presents significant challenges, including dense object clutter and varied spatial configurations -- with items positioned at different heights, depths, and in close proximity. By targeting the retail domain, our benchmark addresses a setting with strong potential for near-term automation impact. We demonstrate that current state-of-the-art generalist models struggle to solve even common retail tasks. To support further research, we release the RoboBenchMart suite, which includes a procedural store layout generator, a trajectory generation pipeline, evaluation tools and fine-tuned baseline models.

</details>


### [103] [nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation](https://arxiv.org/abs/2511.10403)
*Mingxing Peng,Ruoyu Yao,Xusen Guo,Jun Ma*

Main category: cs.RO

TL;DR: nuPlan-R是一个新的闭环规划基准，通过集成基于学习的反应式多智能体模拟来解决现有基准的局限性，提高了真实性和行为多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的闭环规划基准依赖于缺乏行为多样性和真实人际交互的基于规则的反应式 Agent（如 IDM），导致交通动态过于简化。

Method: 在 nuPlan 框架中集成了基于学习的反应式多智能体模拟，用去噪扩散模型（DDPM）替代了基于规则的 IDM Agent，并引入了交互感知 Agent 选择机制。此外，还增加了两个评估指标。

Result: 与基于规则的 Agent 相比，基于学习的 Agent 表现出更真实、更多样化和更像人类的交通行为。nuPlan-R 基准能更准确地反映真实世界的交互式驾驶，并能更清晰地评估规划器性能，尤其能凸显基于学习的规划器在处理复杂动态场景中的优势。

Conclusion: nuPlan-R 建立了一个新的标准，用于公平、反应式和真实的闭环规划评估，并强调了基于学习的规划器在复杂交互场景中的优越性。该基准的代码将开源。

Abstract: Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.

</details>


### [104] [Improving dependability in robotized bolting operations](https://arxiv.org/abs/2511.10448)
*Lorenzo Pagliara,Violeta Redondo,Enrico Ferrentino,Manuel Ferre,Pasquale Chiacchio*

Main category: cs.RO

TL;DR: 提出了一种用于机器人化螺栓连接任务的控制框架，通过精确的扭矩控制、主动顺应性和多模态人机交互来提高其可靠性、自主性和故障管理能力，并在管道法兰连接任务中进行了验证，结果表明该框架能提高故障检测能力、增强操作员态势感知能力并精确执行螺栓连接操作，但也指出了单摄像头在实现全面态势感知方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人螺栓连接系统在可靠的自主性和故障管理能力方面存在不足，影响了工业装配和科研设施维护的安全性与有效性。

Method: 提出了一种机器人化螺栓连接任务的控制框架，集成了精确驱动扭矩控制、主动顺应性、多模态人机交互（包括实时可视化和自动/手动控制无缝切换）以及一个高级监督器（SV）来协调执行和管理控制模式转换，遵循监督控制（SVC）范式。

Result: 在代表性的管道法兰连接螺栓操作任务中，在多种故障条件下进行了验证。结果显示，该系统提高了故障检测能力，增强了操作员的态势感知能力，并能精确、顺应地执行螺栓连接操作。

Conclusion: 所提出的控制框架能够实现可靠的机器人化螺栓连接任务，提高了操作的安全性和效率，但需要进一步研究以克服单摄像头在态势感知方面的局限性，并可能需要结合其他传感器以实现更全面的环境感知。

Abstract: Bolting operations are critical in industrial assembly and in the maintenance of scientific facilities, requiring high precision and robustness to faults. Although robotic solutions have the potential to improve operational safety and effectiveness, current systems still lack reliable autonomy and fault management capabilities. To address this gap, we propose a control framework for dependable robotized bolting tasks and instantiate it on a specific robotic system. The system features a control architecture ensuring accurate driving torque control and active compliance throughout the entire operation, enabling safe interaction even under fault conditions. By designing a multimodal human-robot interface (HRI) providing real-time visualization of relevant system information and supporting seamless transitions between automatic and manual control, we improve operator situation awareness and fault detection capabilities. A high-level supervisor (SV) coordinates the execution and manages transitions between control modes, ensuring consistency with the supervisory control (SVC) paradigm, while preserving the human operator's authority. The system is validated in a representative bolting operation involving pipe flange joining, under several fault conditions. The results demonstrate improved fault detection capabilities, enhanced operator situational awareness, and accurate and compliant execution of the bolting operation. However, they also reveal the limitations of relying on a single camera to achieve full situational awareness.

</details>


### [105] [From Fold to Function: Dynamic Modeling and Simulation-Driven Design of Origami Mechanisms](https://arxiv.org/abs/2511.10580)
*Tianhui Han,Shashwat Singh,Sarvesh Patil,Zeynep Temel*

Main category: cs.RO

TL;DR: 提出了一个受折纸启发的机制仿真设计框架，使用MuJoCo的变形体能力来模拟折纸薄片的折叠行为和与环境的交互, 并通过一个折纸弹射器案例研究进行了优化和实验验证。


<details>
  <summary>Details</summary>
Motivation: 折纸机制在机器人和可部署系统中具有重要价值，但精确模拟其折叠行为和环境交互仍然具有挑战性。

Method: 使用MuJoCo的变形体能力，将折纸薄片表示为相互连接的可变形元素的图，并通过图形用户界面（GUI）定义折痕和驱动等约束。

Result: 能够生成物理上一致的仿真，捕捉折纸机制的几何结构以及它们与外部物体和表面的交互。通过折纸弹射器案例研究，设计参数通过协方差矩阵自适应进化策略（CMA-ES）在仿真中得到优化，并通过物理原型进行实验验证，优化后的结构提高了投掷性能。

Conclusion: 该系统能够实现快速、仿真驱动的折纸设计、优化和分析。

Abstract: Origami-inspired mechanisms can transform flat sheets into functional three-dimensional dynamic structures that are lightweight, compact, and capable of complex motion. These properties make origami increasingly valuable in robotic and deployable systems. However, accurately simulating their folding behavior and interactions with the environment remains challenging. To address this, we present a design framework for origami mechanism simulation that utilizes MuJoCo's deformable-body capabilities. In our approach, origami sheets are represented as graphs of interconnected deformable elements with user-specified constraints such as creases and actuation, defined through an intuitive graphical user interface (GUI). This framework allows users to generate physically consistent simulations that capture both the geometric structure of origami mechanisms and their interactions with external objects and surfaces. We demonstrate our method's utility through a case study on an origami catapult, where design parameters are optimized in simulation using the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and validated experimentally on physical prototypes. The optimized structure achieves improved throwing performance, illustrating how our system enables rapid, simulation-driven origami design, optimization, and analysis.

</details>


### [106] [Optimizing the flight path for a scouting Uncrewed Aerial Vehicle](https://arxiv.org/abs/2511.10598)
*Raghav Adhikari,Sachet Khatiwada,Suman Poudel*

Main category: cs.RO

TL;DR: 在灾后恢复场景中，无人机被提议用于环境侦察。提出了一种基于优化的方法，为无人机规划路径，使其在最佳高度利用传感器覆盖最大区域并收集数据，同时最小化不确定性。


<details>
  <summary>Details</summary>
Motivation: 灾后环境非结构化，给救援车辆路径规划带来挑战。

Method: 提出一种基于优化的方法，为无人机规划路径，使其在最佳高度利用传感器覆盖最大区域并收集数据，同时最小化不确定性。

Result: 无人机在非结构化灾后环境中，可以被用于环境侦察。

Conclusion: 基于优化的方法可以为无人机规划最优路径，以应对灾后非结构化环境的侦察任务。

Abstract: Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.

</details>


### [107] [Robot Crash Course: Learning Soft and Stylized Falling](https://arxiv.org/abs/2511.10635)
*Pascal Strauch,David Müller,Sammy Christen,Agon Serifi,Ruben Grandia,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: bipedal robots can perform controlled, soft falls, reducing damage and allowing user control over the end pose.


<details>
  <summary>Details</summary>
Motivation: While most research focuses on preventing falls, this paper concentrates on reducing physical damage to the robot during falls and providing users with control over the robot's end pose.

Method: Propose a robot-agnostic reward function that balances achieving a desired end pose with impact minimization and protection of critical robot parts during reinforcement learning. Introduce a simulation-based sampling strategy for initial and end poses to ensure robustness to various falling conditions and to allow specification of arbitrary end poses at inference time.

Result: Demonstrate through simulated and real-world experiments that bipedal robots can perform controlled, soft falls.

Conclusion: Controlled, soft falls are achievable for bipedal robots, offering a way to minimize damage and maintain user control during such events.

Abstract: Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.

</details>
